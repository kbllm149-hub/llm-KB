{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9a0c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    session.run(\"DROP INDEX chunk_embeddings IF EXISTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc5df9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mdriver\u001b[49m.session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m      2\u001b[39m     indexes = session.run(\u001b[33m\"\u001b[39m\u001b[33mSHOW INDEXES\u001b[39m\u001b[33m\"\u001b[39m).data()\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indexes:\n\u001b[32m      4\u001b[39m         \u001b[38;5;66;03m# 安全地列印所有欄位，避免 KeyError（不同 Neo4j 版本欄位名稱可能不同）\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "with driver.session() as session:\n",
    "    indexes = session.run(\"SHOW INDEXES\").data()\n",
    "    for idx in indexes:\n",
    "        # 安全地列印所有欄位，避免 KeyError（不同 Neo4j 版本欄位名稱可能不同）\n",
    "        try:\n",
    "            name = idx.get(\"name\")\n",
    "            typ = idx.get(\"type\") or idx.get(\"indexType\")\n",
    "            labels = idx.get(\"labelsOrTypes\") or idx.get(\"entityType\")\n",
    "            props = idx.get(\"properties\") or idx.get(\"indexedProperties\")\n",
    "            print(name, typ, labels, props)\n",
    "        except Exception:\n",
    "            print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdbb294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_33 放牧於缺乏微量元素硒的土地上或是攝食品質很差的乾草,造成剛出生仔羊無法站立。\n",
      "\n",
      "仔羊白肌病的預防方式有這些：1.母羊懷孕期要注意維生素E補充。2.出生仔羊要注意維生素E補充。3.健康的母羊才能生產優質的初乳。4.母羊有胎衣滯留的現象產生。5.仔羊可用注射維生素E的方式補充。6.成羊可以添加維生素E在精料中。\n",
      "\n",
      "山羊異食癖：仔羊異食癖是源自於礦物質磷的缺乏,磷的缺乏會導致山羊失去食慾,更有產生啃毛、吃皮及雜物等異食癖行為。\n",
      "\n",
      "山羊異食癖的預防方式有這些：1.日糧內鈣磷比率平衡。2.增加日糧中穀類比例。3.增加日糧中磷酸氫鈣的添加比例。\n",
      "\n",
      "山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺\n",
      "0_34 例。3.增加日糧中磷酸氫鈣的添加比例。\n",
      "\n",
      "山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\n",
      "\n",
      "山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素A缺乏。2.無法放牧飼養山羊者需提供青草給飼養的山羊。3.注意在飼料內添加維生素A。4.母羊懷孕期要注意維生素A補充。5.出生仔羊要注意維生素A補充。6.健康的母羊才能生產優質的初乳。\n",
      "\n",
      "山羊鼓脹(bloat)：山羊鼓脹通常是由大量採食豐富的潮濕新鮮草料或穀類引起的,如三葉草,苜蓿或豆科牧草。這些潮濕新鮮草料在瘤胃中形成微小的氣泡,導致山羊噯氣無法排出。瘤胃隨著泡沫膨脹壓迫橫隔膜,造成山羊可能因呼吸或循環衰竭而很\n",
      "0_406 之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織正常：當缺乏維生素A，則身體之呼吸道、腸道、生殖道、尿道與眼睛表皮細胞會角質化(epithelial keratinization)。3.強抵抗力：因此缺乏維生素A，容易使山羊感染疾病及寄生蟲、繁殖性能不佳。\n",
      "\n",
      "造成山羊維生素A缺乏之原因有二，主要為飼糧維生素A含量不足，尤其在乾旱季節或冬季，因青草供應受限，或餵飼山羊之乾草品質不良，例如乾草貯存期間太長及乾草貯存不當，因日曬與雨淋。製造乾草時，因天候不良，乾草\n",
      "0_434 轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變而形成夜盲症。因此在肉羊日糧中，每公斤精料至少要含3500國際單位(IU)之維生素A。\n",
      "\n",
      "維生素D：同樣是脂溶性維生素，其主要是幫助動物有效率的吸收及利用鈣與磷。肉羊缺乏維生素D會導致肉羊鈣與磷的吸收不平衡，影響肉羊骨骼發育，仔羊及成羊均會產生軟骨症及骨質疏鬆症。由於陽光中的紫外線照射動物皮膚，會使動物皮膚中之7dehydrocholesterol轉化成維生素D3。因此放牧羊隻不會缺乏維生素D，然而圈飼的羊\n"
     ]
    }
   ],
   "source": [
    "with driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.text CONTAINS \"夜盲症\"\n",
    "    RETURN c.id, c.text, c.source\n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "    for r in result:\n",
    "        print(r[\"c.id\"], r[\"c.text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45ea402",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8dc7959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(山羊)-[:每日獲得]->(微生物胺基酸質與量)  <- sample chunk: 0_9\n",
      "(山羊)-[:受到影響]->(日曬)  <- sample chunk: 0_784\n",
      "(山羊)-[:易發生]->(波爾種、安格拉種)  <- sample chunk: 0_771\n",
      "(山羊)-[:調控]->(鈣和有機磷的濃度)  <- sample chunk: 0_769\n",
      "(山羊)-[:主要原因]->(過度地損失礦物質)  <- sample chunk: 0_768\n",
      "(山羊)-[:易發生]->(上顎及下顎骨)  <- sample chunk: 0_768\n",
      "(山羊)-[:罹患]->(纖維性骨營養不良症)  <- sample chunk: 0_767\n",
      "(山羊)-[:患有]->(纖維性骨營養不良症(Osteodystrophia fibrosa))  <- sample chunk: 0_766\n",
      "(山羊)-[:死亡]->(巴斯德桿菌性肺炎)  <- sample chunk: 0_746\n",
      "(山羊)-[:infected_by]->(Mycobacterium paratuberculosis)  <- sample chunk: 0_742\n"
     ]
    }
   ],
   "source": [
    "# 9️⃣ 驗證：列出部分三元組與來源 (使用關係上的 chunks 陣列與節點 Mentions)\n",
    "with driver.session() as session:\n",
    "    result = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        RETURN h.name AS head, r.type AS relation, t.name AS tail, r.chunks AS chunks\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    "    rows = result.data()\n",
    "    if not rows:\n",
    "        print(\"尚未找到任何三元組，請先執行上一個抽取/寫入 cell。\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            sample = row.get(\"chunks\")\n",
    "            if isinstance(sample, list) and sample:\n",
    "                sample = sample[0]\n",
    "            print(f\"({row['head']})-[:{row['relation']}]->({row['tail']})  <- sample chunk: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8346a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(山羊)-[:缺乏導致]->(夜盲症)  <- sample chunk: 0_406\n",
      "(維生素A)-[:缺乏]->(山羊夜盲症)  <- sample chunk: 0_33\n",
      "(維生素A)-[:缺乏]->(夜盲症)  <- sample chunk: 0_21\n",
      "(維生素A)-[:缺乏導致]->(夜盲症)  <- sample chunk: 0_17\n",
      "(肉羊)-[:缺乏]->(夜盲症)  <- sample chunk: 0_434\n",
      "(維生素E)-[:缺乏導致]->(夜盲症)  <- sample chunk: 0_409\n",
      "(山羊夜盲症)-[:源自於]->(維生素A缺乏)  <- sample chunk: 0_34\n",
      "(缺乏維生素A)-[:導致]->(夜盲症)  <- sample chunk: 0_406\n",
      "(維他命A)-[:缺乏]->(夜盲症)  <- sample chunk: 0_617\n"
     ]
    }
   ],
   "source": [
    "# 9️⃣ 驗證：列出部分三元組與來源 (使用關係上的 chunks 陣列與節點 Mentions)\n",
    "\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        WHERE h.name CONTAINS '夜盲症' AND r.chunks <> 'None'\n",
    "        OR t.name CONTAINS '夜盲症' AND r.chunks <> 'None'\n",
    "        OR r.chunks CONTAINS '夜盲症' AND r.chunks <> 'None'\n",
    "        RETURN h.name AS head, r.type AS relation, t.name AS tail, r.chunks AS chunks\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    "    rows = result.data()\n",
    "    if not rows:\n",
    "        print(\"尚未找到任何三元組，請先執行上一個抽取/寫入 cell。\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            sample = row.get(\"chunks\")\n",
    "            if isinstance(sample, list) and sample:\n",
    "                sample = sample[0]\n",
    "            print(f\"({row['head']})-[:{row['relation']}]->({row['tail']})  <- sample chunk: {sample}\")\n",
    "            #print(f\"({row['head']})-[:{row['relation']}]->({row['tail']}) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af122f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aint\\Desktop\\LLM+KB\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已存在本地向量模型\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector index 'chunk_embeddings' 已存在\n",
      "📄 載入檔案共 799 個 chunk\n",
      "✅ 已寫入 799 個 chunk 到 Neo4j\n",
      "✅ 已寫入 799 個 chunk 到 Neo4j\n",
      "✅ 已寫入 7255 條 (Entity)-[RELATION]->(Entity) 關係，並建立來源 MENTIONS 與 r.chunks\n",
      "🔎 三元組: (山羊)-[:每日獲得]->(微生物胺基酸質與量)  <- sample chunk: 0_9\n",
      "🔎 三元組: (山羊)-[:受到影響]->(日曬)  <- sample chunk: 0_784\n",
      "🔎 三元組: (山羊)-[:易發生]->(波爾種、安格拉種)  <- sample chunk: 0_771\n",
      "🔎 三元組: (山羊)-[:調控]->(鈣和有機磷的濃度)  <- sample chunk: 0_769\n",
      "🔎 三元組: (山羊)-[:主要原因]->(過度地損失礦物質)  <- sample chunk: 0_768\n",
      "🔎 三元組: (山羊)-[:易發生]->(上顎及下顎骨)  <- sample chunk: 0_768\n",
      "🔎 三元組: (山羊)-[:罹患]->(纖維性骨營養不良症)  <- sample chunk: 0_767\n",
      "🔎 三元組: (山羊)-[:患有]->(纖維性骨營養不良症(Osteodystrophia fibrosa))  <- sample chunk: 0_766\n",
      "🔎 三元組: (山羊)-[:死亡]->(巴斯德桿菌性肺炎)  <- sample chunk: 0_746\n",
      "🔎 三元組: (山羊)-[:infected_by]->(Mycobacterium paratuberculosis)  <- sample chunk: 0_742\n",
      "✅ 已寫入 7255 條 (Entity)-[RELATION]->(Entity) 關係，並建立來源 MENTIONS 與 r.chunks\n",
      "🔎 三元組: (山羊)-[:每日獲得]->(微生物胺基酸質與量)  <- sample chunk: 0_9\n",
      "🔎 三元組: (山羊)-[:受到影響]->(日曬)  <- sample chunk: 0_784\n",
      "🔎 三元組: (山羊)-[:易發生]->(波爾種、安格拉種)  <- sample chunk: 0_771\n",
      "🔎 三元組: (山羊)-[:調控]->(鈣和有機磷的濃度)  <- sample chunk: 0_769\n",
      "🔎 三元組: (山羊)-[:主要原因]->(過度地損失礦物質)  <- sample chunk: 0_768\n",
      "🔎 三元組: (山羊)-[:易發生]->(上顎及下顎骨)  <- sample chunk: 0_768\n",
      "🔎 三元組: (山羊)-[:罹患]->(纖維性骨營養不良症)  <- sample chunk: 0_767\n",
      "🔎 三元組: (山羊)-[:患有]->(纖維性骨營養不良症(Osteodystrophia fibrosa))  <- sample chunk: 0_766\n",
      "🔎 三元組: (山羊)-[:死亡]->(巴斯德桿菌性肺炎)  <- sample chunk: 0_746\n",
      "🔎 三元組: (山羊)-[:infected_by]->(Mycobacterium paratuberculosis)  <- sample chunk: 0_742\n",
      "\n",
      "❓ 問題: 山羊缺乏維生素A會導致視力出現什麼問題\n",
      "🟩 回答: 根據文章，山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。\n",
      "⏱️ Inference latency: 872.6 ms\n",
      "\n",
      "❓ 問題: 山羊缺乏維生素A會導致視力出現什麼問題\n",
      "🟩 回答: 根據文章，山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。\n",
      "⏱️ Inference latency: 872.6 ms\n",
      "🚀 LLM throughput: N/A (Ollama 未提供 eval 統計)\n",
      "🟨 檢索結果為空，改用向量索引 fallback：\n",
      "- 生素群,以滿足山羊生理需要。\n",
      "\n",
      "維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的NRC,建議維生素A在山羊每公斤日糧中至少含3500國際單位(IU)以上。而2007年版的NRC則將山羊維生素A需要量改以視網醇當量(retinol equivalent,RE)為估算單位。\n",
      "\n",
      "維生素D:維生素D在山羊的鈣、磷吸收上扮演著非常重要的角色,缺乏維生素D會導致山羊鈣磷吸收不平衡、影響山羊骨骼發育。放牧羊隻會因太陽照射其皮膚而自行合成維生素D,故不會缺乏維生素D。然而圈飼羊隻則因無法長時間接觸太陽光與 (source=read.txt) score=0.8932\n",
      "- 之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織正常：當缺乏維生素A，則身體之呼吸道、腸道、生殖道、尿道與眼睛表皮細胞會角質化(epithelial keratinization)。3.強抵抗力：因此缺乏維生素A，容易使山羊感染疾病及寄生蟲、繁殖性能不佳。\n",
      "\n",
      "造成山羊維生素A缺乏之原因有二，主要為飼糧維生素A含量不足，尤其在乾旱季節或冬季，因青草供應受限，或餵飼山羊之乾草品質不良，例如乾草貯存期間太長及乾草貯存不當，因日曬與雨淋。製造乾草時，因天候不良，乾草 (source=read.txt) score=0.8606\n",
      "- 轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變而形成夜盲症。因此在肉羊日糧中，每公斤精料至少要含3500國際單位(IU)之維生素A。\n",
      "\n",
      "維生素D：同樣是脂溶性維生素，其主要是幫助動物有效率的吸收及利用鈣與磷。肉羊缺乏維生素D會導致肉羊鈣與磷的吸收不平衡，影響肉羊骨骼發育，仔羊及成羊均會產生軟骨症及骨質疏鬆症。由於陽光中的紫外線照射動物皮膚，會使動物皮膚中之7dehydrocholesterol轉化成維生素D3。因此放牧羊隻不會缺乏維生素D，然而圈飼的羊 (source=read.txt) score=0.8489\n",
      "- 痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨之下痢、劇渴(polydipsia)、多尿、呼吸困難與直腸溫度低下。硒過量也會引起蹄之疼痛及脫落。\n",
      "\n",
      "維生素A：山羊之芻料不含有維生素A，不過含有維生素A之先驅物，即胡蘿蔔素。大約1g的B-胡蘿蔔素相當於400IU的維生素A。動物吸收維生素A之主要部位在小腸前段，身體貯藏維生素A之器官為肝臟，約佔75-90%之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsi (source=read.txt) score=0.8323\n",
      "- 例。3.增加日糧中磷酸氫鈣的添加比例。\n",
      "\n",
      "山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\n",
      "\n",
      "山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素A缺乏。2.無法放牧飼養山羊者需提供青草給飼養的山羊。3.注意在飼料內添加維生素A。4.母羊懷孕期要注意維生素A補充。5.出生仔羊要注意維生素A補充。6.健康的母羊才能生產優質的初乳。\n",
      "\n",
      "山羊鼓脹(bloat)：山羊鼓脹通常是由大量採食豐富的潮濕新鮮草料或穀類引起的,如三葉草,苜蓿或豆科牧草。這些潮濕新鮮草料在瘤胃中形成微小的氣泡,導致山羊噯氣無法排出。瘤胃隨著泡沫膨脹壓迫橫隔膜,造成山羊可能因呼吸或循環衰竭而很 (source=read.txt) score=0.8274\n",
      "🚀 LLM throughput: N/A (Ollama 未提供 eval 統計)\n",
      "🟨 檢索結果為空，改用向量索引 fallback：\n",
      "- 生素群,以滿足山羊生理需要。\n",
      "\n",
      "維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的NRC,建議維生素A在山羊每公斤日糧中至少含3500國際單位(IU)以上。而2007年版的NRC則將山羊維生素A需要量改以視網醇當量(retinol equivalent,RE)為估算單位。\n",
      "\n",
      "維生素D:維生素D在山羊的鈣、磷吸收上扮演著非常重要的角色,缺乏維生素D會導致山羊鈣磷吸收不平衡、影響山羊骨骼發育。放牧羊隻會因太陽照射其皮膚而自行合成維生素D,故不會缺乏維生素D。然而圈飼羊隻則因無法長時間接觸太陽光與 (source=read.txt) score=0.8932\n",
      "- 之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織正常：當缺乏維生素A，則身體之呼吸道、腸道、生殖道、尿道與眼睛表皮細胞會角質化(epithelial keratinization)。3.強抵抗力：因此缺乏維生素A，容易使山羊感染疾病及寄生蟲、繁殖性能不佳。\n",
      "\n",
      "造成山羊維生素A缺乏之原因有二，主要為飼糧維生素A含量不足，尤其在乾旱季節或冬季，因青草供應受限，或餵飼山羊之乾草品質不良，例如乾草貯存期間太長及乾草貯存不當，因日曬與雨淋。製造乾草時，因天候不良，乾草 (source=read.txt) score=0.8606\n",
      "- 轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變而形成夜盲症。因此在肉羊日糧中，每公斤精料至少要含3500國際單位(IU)之維生素A。\n",
      "\n",
      "維生素D：同樣是脂溶性維生素，其主要是幫助動物有效率的吸收及利用鈣與磷。肉羊缺乏維生素D會導致肉羊鈣與磷的吸收不平衡，影響肉羊骨骼發育，仔羊及成羊均會產生軟骨症及骨質疏鬆症。由於陽光中的紫外線照射動物皮膚，會使動物皮膚中之7dehydrocholesterol轉化成維生素D3。因此放牧羊隻不會缺乏維生素D，然而圈飼的羊 (source=read.txt) score=0.8489\n",
      "- 痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨之下痢、劇渴(polydipsia)、多尿、呼吸困難與直腸溫度低下。硒過量也會引起蹄之疼痛及脫落。\n",
      "\n",
      "維生素A：山羊之芻料不含有維生素A，不過含有維生素A之先驅物，即胡蘿蔔素。大約1g的B-胡蘿蔔素相當於400IU的維生素A。動物吸收維生素A之主要部位在小腸前段，身體貯藏維生素A之器官為肝臟，約佔75-90%之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsi (source=read.txt) score=0.8323\n",
      "- 例。3.增加日糧中磷酸氫鈣的添加比例。\n",
      "\n",
      "山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\n",
      "\n",
      "山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素A缺乏。2.無法放牧飼養山羊者需提供青草給飼養的山羊。3.注意在飼料內添加維生素A。4.母羊懷孕期要注意維生素A補充。5.出生仔羊要注意維生素A補充。6.健康的母羊才能生產優質的初乳。\n",
      "\n",
      "山羊鼓脹(bloat)：山羊鼓脹通常是由大量採食豐富的潮濕新鮮草料或穀類引起的,如三葉草,苜蓿或豆科牧草。這些潮濕新鮮草料在瘤胃中形成微小的氣泡,導致山羊噯氣無法排出。瘤胃隨著泡沫膨脹壓迫橫隔膜,造成山羊可能因呼吸或循環衰竭而很 (source=read.txt) score=0.8274\n"
     ]
    }
   ],
   "source": [
    "# 一次執行完整流程：建立 Chunk、向量索引、寫入向量、抽取三元組入圖譜、建立檢索與 QA\n",
    "# 建議在重啟 Kernel 後，直接只執行本 Cell。\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# neo4j-graphrag 相關（確保套件已安裝）\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "import ollama\n",
    "\n",
    "# -----------------------------\n",
    "# 參數設定\n",
    "# -----------------------------\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "\n",
    "DATA_FOLDER = r\"C:\\\\Users\\\\aint\\\\Desktop\\\\LLM+KB\\\\neo4j-graphrag-python\\\\data\"\n",
    "EMBED_MODEL_REPO = \"GanymedeNil/text2vec-large-chinese\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "TOP_K = 5\n",
    "\n",
    "# -----------------------------\n",
    "# 連線 Neo4j\n",
    "# -----------------------------\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "\n",
    "# -----------------------------\n",
    "# 工具函式\n",
    "# -----------------------------\n",
    "def chunk_text(text: str, max_length: int = 300, overlap: int = 50) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_length\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_length - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def ensure_vector_index(driver, name: str, label: str, embedding_property: str, dimensions: int, similarity_fn: str = \"cosine\"):\n",
    "    with driver.session() as session:\n",
    "        existing = session.run(\"SHOW INDEXES\").data()\n",
    "        if any(idx.get(\"name\") == name for idx in existing):\n",
    "            print(f\"✅ Vector index '{name}' 已存在\")\n",
    "            return\n",
    "        cypher = f\"\"\"\n",
    "        CREATE VECTOR INDEX {name}\n",
    "        FOR (n:{label}) ON (n.{embedding_property})\n",
    "        OPTIONS {{\n",
    "            indexConfig: {{\n",
    "                `vector.dimensions`: {dimensions},\n",
    "                `vector.similarity_function`: '{similarity_fn}'\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        session.run(cypher)\n",
    "        print(f\"🚀 Vector index '{name}' 建立完成 (dim={dimensions}, sim={similarity_fn})\")\n",
    "\n",
    "\n",
    "def load_txt_files(folder: str) -> List[Dict[str, str]]:\n",
    "    docs = []\n",
    "    for i, fname in enumerate(sorted(os.listdir(folder))):\n",
    "        if fname.lower().endswith(\".txt\"):\n",
    "            path = os.path.join(folder, fname)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            chunks = chunk_text(text)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                docs.append({\n",
    "                    \"id\": f\"{i}_{j}\",\n",
    "                    \"text\": chunk,\n",
    "                    \"source\": fname,\n",
    "                })\n",
    "    return docs\n",
    "\n",
    "\n",
    "def ingest_documents(docs: List[Dict[str, str]], embedder: SentenceTransformerEmbeddings, driver) -> None:\n",
    "    with driver.session() as session:\n",
    "        for doc in docs:\n",
    "            emb = embedder.embed_query(doc[\"text\"])\n",
    "            if hasattr(emb, \"tolist\"):\n",
    "                emb = emb.tolist()\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (c:Chunk {id:$id})\n",
    "                SET c.text = $text,\n",
    "                    c.source = $source,\n",
    "                    c.embedding = $embedding\n",
    "                \"\"\",\n",
    "                id=doc[\"id\"],\n",
    "                text=doc[\"text\"],\n",
    "                source=doc[\"source\"],\n",
    "                embedding=emb,\n",
    "            )\n",
    "    print(f\"✅ 已寫入 {len(docs)} 個 chunk 到 Neo4j\")\n",
    "\n",
    "\n",
    "# ---- 三元組抽取與寫入 ----\n",
    "TRIPLE_PROMPT_TMPL = \"\"\"\n",
    "你是一個資訊抽取器。請從下方文字中抽取知識三元組，輸出純 JSON 陣列，不要加解說文字。\n",
    "每個元素包含 head, relation, tail 三個欄位，例如：\n",
    "[{{\"head\":\"山羊\",\"relation\":\"缺乏\",\"tail\":\"維生素A\"}}]\n",
    "\n",
    "文字：\n",
    "{chunk}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _coerce_triple_list(obj) -> List[Dict[str, str]]:\n",
    "    out = []\n",
    "    if isinstance(obj, list):\n",
    "        for it in obj:\n",
    "            if isinstance(it, dict):\n",
    "                h, r, t = it.get(\"head\"), it.get(\"relation\"), it.get(\"tail\")\n",
    "                if all(isinstance(x, str) and x.strip() for x in (h, r, t)):\n",
    "                    out.append({\"head\": h.strip(), \"relation\": r.strip(), \"tail\": t.strip()})\n",
    "            elif isinstance(it, (list, tuple)) and len(it) == 3:\n",
    "                h, r, t = it\n",
    "                if all(isinstance(x, str) and x.strip() for x in (h, r, t)):\n",
    "                    out.append({\"head\": str(h).strip(), \"relation\": str(r).strip(), \"tail\": str(t).strip()})\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_triples_from_text(raw: str) -> List[Dict[str, str]]:\n",
    "    try:\n",
    "        return _coerce_triple_list(json.loads(raw))\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\[[\\s\\S]*\\]\", raw)\n",
    "        if m:\n",
    "            try:\n",
    "                return _coerce_triple_list(json.loads(m.group(0)))\n",
    "            except Exception:\n",
    "                return []\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_triples_from_chunk(chunk_text: str, model: str = OLLAMA_MODEL) -> List[Dict[str, str]]:\n",
    "    prompt = TRIPLE_PROMPT_TMPL.format(chunk=chunk_text)\n",
    "    resp = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0}\n",
    "    )\n",
    "    content = resp.get(\"message\", {}).get(\"content\", \"\")\n",
    "    return parse_triples_from_text(content)\n",
    "\n",
    "\n",
    "def ingest_triples_with_provenance(triples: List[Dict[str, str]], chunk_id: str, driver) -> int:\n",
    "    if not triples:\n",
    "        return 0\n",
    "    with driver.session() as session:\n",
    "        for t in triples:\n",
    "            head, rel, tail = t[\"head\"], t[\"relation\"], t[\"tail\"]\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (h:Entity {name:$head})\n",
    "                MERGE (t:Entity {name:$tail})\n",
    "                MERGE (h)-[r:RELATION {type:$rel}]->(t)\n",
    "                WITH h, t, r, $cid AS cid\n",
    "                SET r.chunks = CASE\n",
    "                  WHEN r.chunks IS NULL THEN [cid]\n",
    "                  WHEN NOT cid IN r.chunks THEN r.chunks + cid\n",
    "                  ELSE r.chunks\n",
    "                END\n",
    "                WITH h, t, r, cid\n",
    "                MATCH (c:Chunk {id:cid})\n",
    "                MERGE (c)-[:MENTIONS]->(h)\n",
    "                MERGE (c)-[:MENTIONS]->(t)\n",
    "                \"\"\",\n",
    "                head=head,\n",
    "                rel=rel,\n",
    "                tail=tail,\n",
    "                cid=chunk_id,\n",
    "            )\n",
    "    return len(triples)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) 準備 Embedding 模型與索引\n",
    "# -----------------------------\n",
    "if not os.path.isdir(EMBED_MODEL_DIR) or not os.listdir(EMBED_MODEL_DIR):\n",
    "    print(\"⏬ 下載中文向量模型……\")\n",
    "    snapshot_download(repo_id=EMBED_MODEL_REPO, local_dir=EMBED_MODEL_DIR)\n",
    "else:\n",
    "    print(\"✅ 已存在本地向量模型\")\n",
    "\n",
    "embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "embedding_dim = embedder.model.get_sentence_embedding_dimension()\n",
    "\n",
    "ensure_vector_index(driver, name=\"chunk_embeddings\", label=\"Chunk\", embedding_property=\"embedding\", dimensions=embedding_dim, similarity_fn=\"cosine\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) 載入 TXT 並切 Chunk、寫入向量\n",
    "# -----------------------------\n",
    "docs = load_txt_files(DATA_FOLDER)\n",
    "print(f\"📄 載入檔案共 {len(docs)} 個 chunk\")\n",
    "ingest_documents(docs, embedder, driver)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 從每個 Chunk 抽取三元組並寫入知識圖譜\n",
    "# -----------------------------\n",
    "rel_count = 0\n",
    "for doc in docs:\n",
    "    triples = extract_triples_from_chunk(doc[\"text\"], model=OLLAMA_MODEL)\n",
    "    rel_count += ingest_triples_with_provenance(triples, doc[\"id\"], driver)\n",
    "print(f\"✅ 已寫入 {rel_count} 條 (Entity)-[RELATION]->(Entity) 關係，並建立來源 MENTIONS 與 r.chunks\")\n",
    "\n",
    "# 簡單檢視幾條三元組\n",
    "with driver.session() as session:\n",
    "    rows = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        RETURN h.name AS head, r.type AS relation, t.name AS tail, r.chunks AS chunks\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    ").data()\n",
    "    for row in rows:\n",
    "        sample = row.get(\"chunks\")\n",
    "        if isinstance(sample, list) and sample:\n",
    "            sample = sample[0]\n",
    "        print(f\"🔎 三元組: ({row['head']})-[:{row['relation']}]->({row['tail']})  <- sample chunk: {sample}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 建立檢索與 QA（VectorRetriever + Ollama LLM）\n",
    "# -----------------------------\n",
    "vector_retriever = VectorRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"chunk_embeddings\",\n",
    "    embedder=embedder,\n",
    "    return_properties=[\"text\", \"source\"],\n",
    ")\n",
    "llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "rag = GraphRAG(llm=llm, retriever=vector_retriever)\n",
    "\n",
    "\n",
    "# 直接向量查詢 fallback（當 rag 返回中沒有 retriever_result 時）\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings as _STE\n",
    "\n",
    "def vector_query_fallback(query_text: str, top_k: int = TOP_K):\n",
    "    q_emb = embedder.embed_query(query_text)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    with driver.session() as session:\n",
    "        rows = session.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes('chunk_embeddings', $k, $qemb)\n",
    "            YIELD node, score\n",
    "            RETURN node.text AS text, node.source AS source, score\n",
    "            LIMIT $k\n",
    "            \"\"\",\n",
    "            k=top_k,\n",
    "            qemb=q_emb,\n",
    "        ).data()\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _measure_tok_s(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"使用 Ollama 的 eval 統計估算 tok/s；若不可得則嘗試以 eval_count/eval_duration 估算，仍不可得則回傳 -1。\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        # 1) 直接取 tokens_per_second（若存在）\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or eval_info.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        # 2) 以 eval_count / eval_duration(秒) 推估（Ollama 一般回傳 ns）\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "def qa(query: str, top_k: int = TOP_K):\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag.search(query_text=query, retriever_config={\"top_k\": top_k})\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "    print(\"\\n❓ 問題:\", query)\n",
    "    print(\"🟩 回答:\", getattr(resp, \"answer\", None))\n",
    "    print(f\"⏱️ Inference latency: {latency_ms:.1f} ms\")\n",
    "    # 嘗試用最簡 prompt 估計 tok/s（不影響實際回答）\n",
    "    ctx_nodes = getattr(resp, \"retriever_result\", None) or []\n",
    "    ctx_text = \"\\n\\n\".join([n.get(\"text\", \"\") for n in ctx_nodes[:min(len(ctx_nodes), top_k)]])\n",
    "    probe_prompt = f\"請根據以下內容簡短回答：\\n\\n{ctx_text}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "    tps = _measure_tok_s(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"🚀 LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"🚀 LLM throughput: N/A (Ollama 未提供 eval 統計)\")\n",
    "    nodes = getattr(resp, \"retriever_result\", None)\n",
    "    if nodes:\n",
    "        print(\"🟦 相關內容 (前幾筆):\")\n",
    "        for node in nodes[:min(len(nodes), top_k)]:\n",
    "            print(\"-\", node.get(\"text\", \"\"), f\"(source={node.get('source')})\")\n",
    "    else:\n",
    "        print(\"🟨 檢索結果為空，改用向量索引 fallback：\")\n",
    "        rows = vector_query_fallback(query, top_k)\n",
    "        if rows:\n",
    "            for r in rows:\n",
    "                print(\"-\", r.get(\"text\", \"\"), f\"(source={r.get('source')}) score={r.get('score'):.4f}\")\n",
    "        else:\n",
    "            print(\"（查無相關內容）\")\n",
    "\n",
    "# Demo 問題（可自行開關，避免重複輸出）\n",
    "RUN_DEMOS = True\n",
    "if RUN_DEMOS:\n",
    "    qa(\"山羊缺乏維生素A會導致視力出現什麼問題\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec96f55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m RUN_DEMOS = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RUN_DEMOS:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mqa\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m山羊缺乏維生素A會導致視力出現什麼問題？\u001b[39m\u001b[33m\"\u001b[39m, top_k=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "RUN_DEMOS = True\n",
    "if RUN_DEMOS:\n",
    "    qa(\"山羊缺乏維生素A會導致視力出現什麼問題？\", top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415bb64",
   "metadata": {},
   "source": [
    "> 提醒：為避免重複輸出與重複寫入，建議先 Kernel → Restart，然後只執行最下方「一次執行完整流程」的 Cell。上面的範例程式已改為需手動把 `RUN_DEMOS = True` 才會執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29bdf63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "問題: 山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\n",
      "回答: According to the provided context, 山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。\n",
      "Inference latency: 4151.0 ms\n",
      "LLM throughput: 85.02 tok/s (Ollama eval)\n",
      "檢索結果為空，改用向量索引 fallback：\n",
      "- 生素群,以滿足山羊生理需要。\n",
      "\n",
      "維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的NRC,建議維生素A在山羊每公斤日糧中至少含3500國際單位(IU)以上。而2007年版的NRC則將山羊維生素A需要量改以視網醇當量(retinol equivalent,RE)為估算單位。\n",
      "\n",
      "維生素D:維生素D在山羊的鈣、磷吸收上扮演著非常重要的角色,缺乏維生素D會導致山羊鈣磷吸收不平衡、影響山羊骨骼發育。放牧羊隻會因太陽照射其皮膚而自行合成維生素D,故不會缺乏維生素D。然而圈飼羊隻則因無法長時間接觸太陽光與 (source=read.txt) score=0.8935\n",
      "- 之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織正常：當缺乏維生素A，則身體之呼吸道、腸道、生殖道、尿道與眼睛表皮細胞會角質化(epithelial keratinization)。3.強抵抗力：因此缺乏維生素A，容易使山羊感染疾病及寄生蟲、繁殖性能不佳。\n",
      "\n",
      "造成山羊維生素A缺乏之原因有二，主要為飼糧維生素A含量不足，尤其在乾旱季節或冬季，因青草供應受限，或餵飼山羊之乾草品質不良，例如乾草貯存期間太長及乾草貯存不當，因日曬與雨淋。製造乾草時，因天候不良，乾草 (source=read.txt) score=0.8598\n",
      "- 轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變而形成夜盲症。因此在肉羊日糧中，每公斤精料至少要含3500國際單位(IU)之維生素A。\n",
      "\n",
      "維生素D：同樣是脂溶性維生素，其主要是幫助動物有效率的吸收及利用鈣與磷。肉羊缺乏維生素D會導致肉羊鈣與磷的吸收不平衡，影響肉羊骨骼發育，仔羊及成羊均會產生軟骨症及骨質疏鬆症。由於陽光中的紫外線照射動物皮膚，會使動物皮膚中之7dehydrocholesterol轉化成維生素D3。因此放牧羊隻不會缺乏維生素D，然而圈飼的羊 (source=read.txt) score=0.8475\n",
      "- 痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨之下痢、劇渴(polydipsia)、多尿、呼吸困難與直腸溫度低下。硒過量也會引起蹄之疼痛及脫落。\n",
      "\n",
      "維生素A：山羊之芻料不含有維生素A，不過含有維生素A之先驅物，即胡蘿蔔素。大約1g的B-胡蘿蔔素相當於400IU的維生素A。動物吸收維生素A之主要部位在小腸前段，身體貯藏維生素A之器官為肝臟，約佔75-90%之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsi (source=read.txt) score=0.8347\n",
      "- 例。3.增加日糧中磷酸氫鈣的添加比例。\n",
      "\n",
      "山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\n",
      "\n",
      "山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素A缺乏。2.無法放牧飼養山羊者需提供青草給飼養的山羊。3.注意在飼料內添加維生素A。4.母羊懷孕期要注意維生素A補充。5.出生仔羊要注意維生素A補充。6.健康的母羊才能生產優質的初乳。\n",
      "\n",
      "山羊鼓脹(bloat)：山羊鼓脹通常是由大量採食豐富的潮濕新鮮草料或穀類引起的,如三葉草,苜蓿或豆科牧草。這些潮濕新鮮草料在瘤胃中形成微小的氣泡,導致山羊噯氣無法排出。瘤胃隨著泡沫膨脹壓迫橫隔膜,造成山羊可能因呼吸或循環衰竭而很 (source=read.txt) score=0.8322\n"
     ]
    }
   ],
   "source": [
    "# ⚡ QA-only：只建立檢索與QA，不重新寫入或抽取\n",
    "import os\n",
    "import time\n",
    "import ollama\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "# 與完整流程保持一致的設定\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "INDEX_NAME = \"chunk_embeddings\"\n",
    "TOP_K = 5\n",
    "\n",
    "# 連線 Neo4j\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "\n",
    "# 載入本地 Embedding 模型（不下載、不寫入）\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# 建立檢索與 LLM（不重建索引）\n",
    "_vector_retriever = VectorRetriever(\n",
    "    driver=_driver,\n",
    "    index_name=INDEX_NAME,\n",
    "    embedder=_embedder,\n",
    "    return_properties=[\"text\", \"source\"],\n",
    ")\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag = GraphRAG(llm=_llm, retriever=_vector_retriever)\n",
    "\n",
    "# 向量查詢 fallback（若 retriever_result 為空）\n",
    "def _vector_query_fallback(query_text: str, top_k: int = TOP_K):\n",
    "    q_emb = _embedder.embed_query(query_text)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    with _driver.session() as session:\n",
    "        rows = session.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes($index, $k, $qemb)\n",
    "            YIELD node, score\n",
    "            RETURN node.text AS text, node.source AS source, score\n",
    "            LIMIT $k\n",
    "            \"\"\",\n",
    "            index=INDEX_NAME,\n",
    "            k=top_k,\n",
    "            qemb=q_emb,\n",
    "        ).data()\n",
    "    return rows\n",
    "\n",
    "def _measure_tok_s(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        # 1) 直接從 eval 拿 tokens_per_second\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or eval_info.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        # 2) 以 eval_count / eval_duration 推估（eval_duration 通常為奈秒）\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "# 封裝成可重用的 QA 函式\n",
    "def qa_only(query: str, top_k: int = TOP_K, temperature: float | None = None):\n",
    "    # 根據 temperature 動態建立 LLM 與 RAG（不影響預設 _rag）\n",
    "    rag_to_use = _rag\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_temp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(llm=_llm_temp, retriever=_vector_retriever)\n",
    "        except Exception as e:\n",
    "            print(f\"[QA-only] 建立帶 temperature 的 LLM 失敗，改用預設：{e}\")\n",
    "            rag_to_use = _rag\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config={\"top_k\": top_k})\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "    print(\"\\n問題:\", query)\n",
    "    print(\"回答:\", getattr(resp, \"answer\", None))\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    nodes = getattr(resp, \"retriever_result\", None)\n",
    "    # 嘗試用最簡 prompt 估計 tok/s（不影響實際回答）\n",
    "    ctx_text = \"\\n\\n\".join([n.get(\"text\", \"\") for n in (nodes or [])[:min(len(nodes or []), top_k)]])\n",
    "    probe_prompt = f\"請根據以下內容簡短回答：\\n\\n{ctx_text}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "    tps = _measure_tok_s(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (無 eval 統計)\")\n",
    "    if nodes:\n",
    "        print(\"相關內容 (前幾筆):\")\n",
    "        for node in nodes[:min(len(nodes), top_k)]:\n",
    "            print(\"-\", node.get(\"text\", \"\"), f\"(source={node.get('source')})\")\n",
    "    else:\n",
    "        print(\"檢索結果為空，改用向量索引 fallback：\")\n",
    "        rows = _vector_query_fallback(query, top_k)\n",
    "        if rows:\n",
    "            for r in rows:\n",
    "                try:\n",
    "                    score = r.get(\"score\")\n",
    "                    score_s = f\" score={score:.4f}\" if isinstance(score, (int, float)) else \"\"\n",
    "                except Exception:\n",
    "                    score_s = \"\"\n",
    "                print(\"-\", r.get(\"text\", \"\"), f\"(source={r.get('source')})\" + score_s)\n",
    "        else:\n",
    "            print(\"（查無相關內容）\")\n",
    "\n",
    "# 範例（預設不自動執行）\n",
    "RUN_QA_ONLY_DEMO = True\n",
    "if RUN_QA_ONLY_DEMO:\n",
    "    qa_only(\"山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\", top_k=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b9ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "問題: 山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\n",
      "回答: According to the provided context, mountain goats deficient in vitamin A may lead to:\n",
      "\n",
      "* 夜盲症 (night blindness) due to a lack of vision\n",
      "* 角質化 (keratinization) of epithelial cells in the respiratory tract, intestines, reproductive organs, and skin\n",
      "* 弱抵抗力 (weak immune system), making them more susceptible to diseases and parasites\n",
      "\n",
      "Additionally, severe vitamin A deficiency can cause 山羊夜盲症 (mountain goat night blindness), which is characterized by a lack of vision.\n",
      "Inference latency: 7035.8 ms\n",
      "LLM throughput: 88.86 tok/s (Ollama eval)\n",
      "檢索結果為空，改用向量索引 fallback：\n",
      "- 生素群,以滿足山羊生理需要。\n",
      "\n",
      "維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的NRC,建議維生素A在山羊每公斤日糧中至少含3500國際單位(IU)以上。而2007年版的NRC則將山羊維生素A需要量改以視網醇當量(retinol equivalent,RE)為估算單位。\n",
      "\n",
      "維生素D:維生素D在山羊的鈣、磷吸收上扮演著非常重要的角色,缺乏維生素D會導致山羊鈣磷吸收不平衡、影響山羊骨骼發育。放牧羊隻會因太陽照射其皮膚而自行合成維生素D,故不會缺乏維生素D。然而圈飼羊隻則因無法長時間接觸太陽光與 (source=read.txt) score=0.8935\n",
      "- 之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織正常：當缺乏維生素A，則身體之呼吸道、腸道、生殖道、尿道與眼睛表皮細胞會角質化(epithelial keratinization)。3.強抵抗力：因此缺乏維生素A，容易使山羊感染疾病及寄生蟲、繁殖性能不佳。\n",
      "\n",
      "造成山羊維生素A缺乏之原因有二，主要為飼糧維生素A含量不足，尤其在乾旱季節或冬季，因青草供應受限，或餵飼山羊之乾草品質不良，例如乾草貯存期間太長及乾草貯存不當，因日曬與雨淋。製造乾草時，因天候不良，乾草 (source=read.txt) score=0.8598\n",
      "- 轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變而形成夜盲症。因此在肉羊日糧中，每公斤精料至少要含3500國際單位(IU)之維生素A。\n",
      "\n",
      "維生素D：同樣是脂溶性維生素，其主要是幫助動物有效率的吸收及利用鈣與磷。肉羊缺乏維生素D會導致肉羊鈣與磷的吸收不平衡，影響肉羊骨骼發育，仔羊及成羊均會產生軟骨症及骨質疏鬆症。由於陽光中的紫外線照射動物皮膚，會使動物皮膚中之7dehydrocholesterol轉化成維生素D3。因此放牧羊隻不會缺乏維生素D，然而圈飼的羊 (source=read.txt) score=0.8475\n",
      "- 痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨之下痢、劇渴(polydipsia)、多尿、呼吸困難與直腸溫度低下。硒過量也會引起蹄之疼痛及脫落。\n",
      "\n",
      "維生素A：山羊之芻料不含有維生素A，不過含有維生素A之先驅物，即胡蘿蔔素。大約1g的B-胡蘿蔔素相當於400IU的維生素A。動物吸收維生素A之主要部位在小腸前段，身體貯藏維生素A之器官為肝臟，約佔75-90%之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsi (source=read.txt) score=0.8347\n",
      "- 例。3.增加日糧中磷酸氫鈣的添加比例。\n",
      "\n",
      "山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\n",
      "\n",
      "山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素A缺乏。2.無法放牧飼養山羊者需提供青草給飼養的山羊。3.注意在飼料內添加維生素A。4.母羊懷孕期要注意維生素A補充。5.出生仔羊要注意維生素A補充。6.健康的母羊才能生產優質的初乳。\n",
      "\n",
      "山羊鼓脹(bloat)：山羊鼓脹通常是由大量採食豐富的潮濕新鮮草料或穀類引起的,如三葉草,苜蓿或豆科牧草。這些潮濕新鮮草料在瘤胃中形成微小的氣泡,導致山羊噯氣無法排出。瘤胃隨著泡沫膨脹壓迫橫隔膜,造成山羊可能因呼吸或循環衰竭而很 (source=read.txt) score=0.8322\n",
      "LLM throughput: 88.86 tok/s (Ollama eval)\n",
      "檢索結果為空，改用向量索引 fallback：\n",
      "- 生素群,以滿足山羊生理需要。\n",
      "\n",
      "維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的NRC,建議維生素A在山羊每公斤日糧中至少含3500國際單位(IU)以上。而2007年版的NRC則將山羊維生素A需要量改以視網醇當量(retinol equivalent,RE)為估算單位。\n",
      "\n",
      "維生素D:維生素D在山羊的鈣、磷吸收上扮演著非常重要的角色,缺乏維生素D會導致山羊鈣磷吸收不平衡、影響山羊骨骼發育。放牧羊隻會因太陽照射其皮膚而自行合成維生素D,故不會缺乏維生素D。然而圈飼羊隻則因無法長時間接觸太陽光與 (source=read.txt) score=0.8935\n",
      "- 之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織正常：當缺乏維生素A，則身體之呼吸道、腸道、生殖道、尿道與眼睛表皮細胞會角質化(epithelial keratinization)。3.強抵抗力：因此缺乏維生素A，容易使山羊感染疾病及寄生蟲、繁殖性能不佳。\n",
      "\n",
      "造成山羊維生素A缺乏之原因有二，主要為飼糧維生素A含量不足，尤其在乾旱季節或冬季，因青草供應受限，或餵飼山羊之乾草品質不良，例如乾草貯存期間太長及乾草貯存不當，因日曬與雨淋。製造乾草時，因天候不良，乾草 (source=read.txt) score=0.8598\n",
      "- 轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變而形成夜盲症。因此在肉羊日糧中，每公斤精料至少要含3500國際單位(IU)之維生素A。\n",
      "\n",
      "維生素D：同樣是脂溶性維生素，其主要是幫助動物有效率的吸收及利用鈣與磷。肉羊缺乏維生素D會導致肉羊鈣與磷的吸收不平衡，影響肉羊骨骼發育，仔羊及成羊均會產生軟骨症及骨質疏鬆症。由於陽光中的紫外線照射動物皮膚，會使動物皮膚中之7dehydrocholesterol轉化成維生素D3。因此放牧羊隻不會缺乏維生素D，然而圈飼的羊 (source=read.txt) score=0.8475\n",
      "- 痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨之下痢、劇渴(polydipsia)、多尿、呼吸困難與直腸溫度低下。硒過量也會引起蹄之疼痛及脫落。\n",
      "\n",
      "維生素A：山羊之芻料不含有維生素A，不過含有維生素A之先驅物，即胡蘿蔔素。大約1g的B-胡蘿蔔素相當於400IU的維生素A。動物吸收維生素A之主要部位在小腸前段，身體貯藏維生素A之器官為肝臟，約佔75-90%之總貯存量。\n",
      "\n",
      "維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsi (source=read.txt) score=0.8347\n",
      "- 例。3.增加日糧中磷酸氫鈣的添加比例。\n",
      "\n",
      "山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\n",
      "\n",
      "山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素A缺乏。2.無法放牧飼養山羊者需提供青草給飼養的山羊。3.注意在飼料內添加維生素A。4.母羊懷孕期要注意維生素A補充。5.出生仔羊要注意維生素A補充。6.健康的母羊才能生產優質的初乳。\n",
      "\n",
      "山羊鼓脹(bloat)：山羊鼓脹通常是由大量採食豐富的潮濕新鮮草料或穀類引起的,如三葉草,苜蓿或豆科牧草。這些潮濕新鮮草料在瘤胃中形成微小的氣泡,導致山羊噯氣無法排出。瘤胃隨著泡沫膨脹壓迫橫隔膜,造成山羊可能因呼吸或循環衰竭而很 (source=read.txt) score=0.8322\n"
     ]
    }
   ],
   "source": [
    "RUN_QA_ONLY_DEMO = True\n",
    "if RUN_QA_ONLY_DEMO:\n",
    "    qa_only(\"山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\", top_k=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a24a7",
   "metadata": {},
   "source": [
    "### 如何使用 QA-only 模式\n",
    "\n",
    "- 不想重跑整個流程時，直接執行上一個「QA-only」程式碼 Cell。\n",
    "- 該 Cell 只連線 Neo4j、載入本地向量模型、建立檢索與 LLM，不會重建索引或重寫資料。\n",
    "- 呼叫方式：\n",
    "  - `qa_only(\"你的問題\", top_k=5)`\n",
    "- 如需快速示範，把 `RUN_QA_ONLY_DEMO = True` 後再次執行該 Cell。\n",
    "\n",
    "#### 額外輸出說明（Latency 與 tok/s）\n",
    "- ⏱️ Inference latency: 包含檢索與 LLM 生成的端到端時間（毫秒）。\n",
    "- 🚀 LLM throughput (tok/s): 透過一次極簡 `ollama.chat` 取得 Ollama 的 eval 統計（若版本/模型不支援則顯示 N/A）。這不會影響實際回答，只作為參考效能指標。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a082f",
   "metadata": {},
   "source": [
    "## ✅ Schema hardening and indexes (optional)\n",
    "\n",
    "這個區塊會：\n",
    "- 新增唯一性限制：`Chunk.id`、`Entity.name`\n",
    "- 建立 Fulltext 索引：`chunk_text_fts` 於 `Chunk(text)`，支援混合檢索\n",
    "\n",
    "執行一次即可（之後會自動跳過已存在的索引/限制）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dab9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema and fulltext index ensured ✅\n"
     ]
    }
   ],
   "source": [
    "# 建立唯一性限制與 Fulltext 索引（若不存在）\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "_uri = \"bolt://localhost:7687\"\n",
    "_user = \"neo4j\"\n",
    "_pw = \"neo4jgoat\"\n",
    "\n",
    "_driver = GraphDatabase.driver(_uri, auth=(_user, _pw))\n",
    "with _driver.session() as session:\n",
    "    # constraints\n",
    "    session.run(\"CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\n",
    "    session.run(\"CREATE CONSTRAINT entity_name_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE\")\n",
    "    # fulltext\n",
    "    session.run(\n",
    "        \"\"\"\n",
    "        CREATE FULLTEXT INDEX chunk_text_fts IF NOT EXISTS\n",
    "        FOR (c:Chunk) ON EACH [c.text]\n",
    "        \"\"\"\n",
    "    )\n",
    "print(\"Schema and fulltext index ensured ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b63fa",
   "metadata": {},
   "source": [
    "## 🔎 Hybrid retrieval and graph expansion (optional)\n",
    "\n",
    "這個區塊示範：\n",
    "- 使用 `HybridRetriever` 同時結合向量索引與 Fulltext 索引（可選線性加權）\n",
    "- 以檢索到的 Chunk 擴展到其 `MENTIONS` 的 `Entity` 與 `RELATION` 三元組，組合更豐富的上下文\n",
    "\n",
    "你可以與現有 `VectorRetriever + GraphRAG` 併存，互不影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd2b2f",
   "metadata": {},
   "source": [
    "# HybridRetriever + Graph expansion QA\n",
    "from neo4j_graphrag.retrievers.hybrid import HybridRetriever, HybridSearchRanker\n",
    "from neo4j_graphrag.retrievers import VectorRetriever as _VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_VECTOR = \"chunk_embeddings\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# Ensure the fulltext index exists and is online (idempotent, with procedure fallback)\n",
    "def _ensure_fulltext_index(driver, index_name: str, label: str, prop: str = \"text\", strict: bool = False) -> bool:\n",
    "    \"\"\"Try to ensure a fulltext index exists. Returns True if exists/created, else False.\n",
    "    If strict=True, raises when cannot be ensured.\n",
    "    \"\"\"\n",
    "    def _exists(sess) -> bool:\n",
    "        try:\n",
    "            rows = sess.run(\"SHOW INDEXES\").data()\n",
    "            if any(str(r.get(\"name\")) == index_name and (r.get(\"type\") == \"FULLTEXT\" or r.get(\"indexType\") == \"FULLTEXT\") for r in rows):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback check via procedure\n",
    "        try:\n",
    "            r = sess.run(\"CALL db.index.fulltext.list() YIELD name WHERE name = $name RETURN name\", name=index_name).single()\n",
    "            if r:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    with driver.session() as session:\n",
    "        if _exists(session):\n",
    "            return True\n",
    "        created = False\n",
    "        # Try schema DDL (Neo4j 5.x)\n",
    "        try:\n",
    "            cypher = f\"\"\"\n",
    "            CREATE FULLTEXT INDEX {index_name} IF NOT EXISTS\n",
    "            FOR (n:{label}) ON EACH [n.{prop}]\n",
    "            \"\"\"\n",
    "            session.run(cypher)\n",
    "            created = True\n",
    "        except Exception:\n",
    "            created = False\n",
    "        # Fallback to procedure (Neo4j 4.x/compat)\n",
    "        if not created:\n",
    "            try:\n",
    "                session.run(\n",
    "                    \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                    name=index_name,\n",
    "                    labels=[label],\n",
    "                    props=[prop],\n",
    "                )\n",
    "                created = True\n",
    "            except Exception:\n",
    "                created = False\n",
    "        # Wait until online (best-effort)\n",
    "        try:\n",
    "            session.run(\"CALL db.awaitIndexes()\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        ok = _exists(session)\n",
    "        if not ok and strict:\n",
    "            raise RuntimeError(f\"無法建立 Fulltext 索引: {index_name}\")\n",
    "        return ok\n",
    "\n",
    "_USING_VECTOR_ONLY = False\n",
    "_has_fts = _ensure_fulltext_index(_driver, INDEX_FTS, label=\"Chunk\", prop=\"text\", strict=False)\n",
    "\n",
    "# 1) 準備檢索器：若 Fulltext 不可用，則退化成向量-only 檢索\n",
    "if _has_fts:\n",
    "    _hybrid = HybridRetriever(\n",
    "        driver=_driver,\n",
    "        vector_index_name=INDEX_VECTOR,\n",
    "        fulltext_index_name=INDEX_FTS,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ Fulltext 索引不可用或無法建立，將使用向量-only 檢索做為替代。\")\n",
    "    _USING_VECTOR_ONLY = True\n",
    "    _hybrid = _VectorRetriever(\n",
    "        driver=_driver,\n",
    "        index_name=INDEX_VECTOR,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag_hybrid = GraphRAG(retriever=_hybrid, llm=_llm)\n",
    "\n",
    "\n",
    "def _measure_tok_s_hybrid(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"Estimate tok/s from Ollama eval stats; return -1.0 if unavailable.\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or r.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "def qa_hybrid(query: str, top_k: int = 5, ranker: str = \"naive\", alpha: float | None = None, temperature: float | None = None):\n",
    "    \"\"\"Hybrid QA with optional temperature to control LLM variability.\"\"\"\n",
    "    # choose LLM per call if temperature is provided\n",
    "    rag_to_use = _rag_hybrid\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_tmp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(retriever=_hybrid, llm=_llm_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"[Hybrid] 建立帶 temperature 的 LLM 失敗，改用預設：{e}\")\n",
    "            rag_to_use = _rag_hybrid\n",
    "\n",
    "    if _USING_VECTOR_ONLY:\n",
    "        print(\"[Hybrid] 提示：Fulltext 不可用，現以向量-only 模式執行。\")\n",
    "    cfg = {\"top_k\": top_k}\n",
    "    if not _USING_VECTOR_ONLY and ranker == \"linear\":\n",
    "        cfg[\"ranker\"] = HybridSearchRanker.LINEAR\n",
    "        cfg[\"alpha\"] = alpha if alpha is not None else 0.6\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config=cfg, return_context=True)\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[Hybrid] 問題:\", query)\n",
    "    print(\"[Hybrid] 回答:\", resp.answer)\n",
    "\n",
    "    # Print latency and estimate tok/s via a small probe\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    items = getattr(resp, \"retriever_result\", None)\n",
    "    # Build a small context snippet for throughput probe\n",
    "    ctx_texts = []\n",
    "    if items:\n",
    "        try:\n",
    "            # Hybrid items container\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None)\n",
    "                if not content and isinstance(getattr(it, \"metadata\", None), dict):\n",
    "                    content = getattr(it, \"metadata\", {}).get(\"text\")\n",
    "                if content:\n",
    "                    ctx_texts.append(str(content))\n",
    "        except Exception:\n",
    "            # Vector-only list of dicts\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    ctx_texts.append(str(n.get(\"text\", \"\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "    ctx_blob = \"\\n\\n\".join(ctx_texts)\n",
    "    probe_prompt = f\"請根據以下內容簡短回答：\\n\\n{ctx_blob}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (無 eval 統計)\")\n",
    "\n",
    "    if items:\n",
    "        print(\"[Hybrid] 檢索結果摘要:\")\n",
    "        # 支援 HybridRetriever 的 items 容器或 VectorRetriever 的 list 節點\n",
    "        try:\n",
    "            # Hybrid path\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None) or it.get(\"text\", \"\")\n",
    "                meta = getattr(it, \"metadata\", None) or it.get(\"source\", \"\")\n",
    "                print(\"-\", str(content)[:120].replace(\"\\n\", \" \"), str(meta)[:80])\n",
    "        except Exception:\n",
    "            # Vector-only path (list of dicts)\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    print(\"-\", str(n.get(\"text\", \"\"))[:120].replace(\"\\n\", \" \"), f\"(source={n.get('source')})\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# 2) 從檢索到的 Chunk 擴展圖（MENTIONS 與 RELATION）\n",
    "\n",
    "def expand_context_from_chunks(query: str, top_k: int = 5) -> str:\n",
    "    q_emb = _embedder.embed_query(query)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    cypher = \"\"\"\n",
    "    CALL db.index.vector.queryNodes($index, $k, $qemb)\n",
    "    YIELD node, score\n",
    "    WITH collect(node)[..$k] AS chunks\n",
    "    // 擴展相關實體與三元組\n",
    "    UNWIND chunks AS c\n",
    "    OPTIONAL MATCH (c)-[:MENTIONS]->(e:Entity)\n",
    "    OPTIONAL MATCH (e)<-[r:RELATION]->(t:Entity)\n",
    "    WITH chunks, collect(DISTINCT e.name) AS ents,\n",
    "         collect(DISTINCT {h:e.name, rel:r.type, t:t.name, sample: head(coalesce(r.chunks, []))}) AS triples\n",
    "    RETURN [c IN chunks | c.text][..$k] AS chunk_texts, ents[..50] AS entities, triples[..100] AS triples\n",
    "    \"\"\"\n",
    "    with _driver.session() as session:\n",
    "        row = session.run(cypher, index=INDEX_VECTOR, k=top_k, qemb=q_emb).single()\n",
    "    if not row:\n",
    "        return \"\"\n",
    "    chunks = row.get(\"chunk_texts\") or []\n",
    "    ents = row.get(\"entities\") or []\n",
    "    triples = row.get(\"triples\") or []\n",
    "    triple_lines = [f\"({t.get('h')})-[:{t.get('rel')}]->({t.get('t')})\" for t in triples if t]\n",
    "    context = (\n",
    "        \"[Chunks]\\n\" + \"\\n\\n\".join(chunks) +\n",
    "        \"\\n\\n[Entities]\\n\" + \", \".join(ents) +\n",
    "        \"\\n\\n[Triples]\\n\" + \"\\n\".join(triple_lines)\n",
    "    )\n",
    "    return context\n",
    "\n",
    "\n",
    "def qa_graph(query: str, top_k: int = 5, temperature: float | None = None):\n",
    "    \"\"\"Graph expansion QA with metrics and adjustable temperature.\n",
    "\n",
    "    Args:\n",
    "        query: 問題\n",
    "        top_k: 檢索擴展的 chunk 數量\n",
    "        temperature: 若提供，將以該值建立臨時 LLM 以控制隨機性（0 更保守，1 更發散）。\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    context = expand_context_from_chunks(query, top_k=top_k)\n",
    "    if not context:\n",
    "        print(\"[GraphExpand] 無檢索結果\")\n",
    "        return\n",
    "\n",
    "    prompt = f\"請根據提供的內容回答問題，條列且含依據。\\n\\n{context}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "\n",
    "    # 選擇 LLM（支援 per-call 調整 temperature）\n",
    "    llm_obj = _llm\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            llm_obj = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[GraphExpand] 建立帶 temperature 的 LLM 失敗，改用預設：{e}\")\n",
    "            llm_obj = _llm\n",
    "\n",
    "    ans = llm_obj.invoke(prompt).content\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[GraphExpand] 問題:\", query)\n",
    "    print(\"[GraphExpand] 回答:\\n\", ans)\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "\n",
    "    # 以相同 context 構造極簡 probe 估算 tok/s\n",
    "    probe_prompt = f\"請根據以下內容簡短回答：\\n\\n{context}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (無 eval 統計)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04cb292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aint\\Desktop\\LLM+KB\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Fulltext 索引不可用或無法建立，將使用向量-only 檢索做為替代。\n"
     ]
    }
   ],
   "source": [
    "# HybridRetriever + Graph expansion QA\n",
    "from neo4j_graphrag.retrievers.hybrid import HybridRetriever, HybridSearchRanker\n",
    "from neo4j_graphrag.retrievers import VectorRetriever as _VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_VECTOR = \"chunk_embeddings\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# Ensure the fulltext index exists and is online (idempotent, with procedure fallback)\n",
    "def _ensure_fulltext_index(driver, index_name: str, label: str, prop: str = \"text\", strict: bool = False) -> bool:\n",
    "    \"\"\"Try to ensure a fulltext index exists. Returns True if exists/created, else False.\n",
    "    If strict=True, raises when cannot be ensured.\n",
    "    \"\"\"\n",
    "    def _exists(sess) -> bool:\n",
    "        try:\n",
    "            rows = sess.run(\"SHOW INDEXES\").data()\n",
    "            if any(str(r.get(\"name\")) == index_name and (r.get(\"type\") == \"FULLTEXT\" or r.get(\"indexType\") == \"FULLTEXT\") for r in rows):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback check via procedure\n",
    "        try:\n",
    "            r = sess.run(\"CALL db.index.fulltext.list() YIELD name WHERE name = $name RETURN name\", name=index_name).single()\n",
    "            if r:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    with driver.session() as session:\n",
    "        if _exists(session):\n",
    "            return True\n",
    "        created = False\n",
    "        # Try schema DDL (Neo4j 5.x)\n",
    "        try:\n",
    "            cypher = f\"\"\"\n",
    "            CREATE FULLTEXT INDEX {index_name} IF NOT EXISTS\n",
    "            FOR (n:{label}) ON EACH [n.{prop}]\n",
    "            \"\"\"\n",
    "            session.run(cypher)\n",
    "            created = True\n",
    "        except Exception:\n",
    "            created = False\n",
    "        # Fallback to procedure (Neo4j 4.x/compat)\n",
    "        if not created:\n",
    "            try:\n",
    "                session.run(\n",
    "                    \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                    name=index_name,\n",
    "                    labels=[label],\n",
    "                    props=[prop],\n",
    "                )\n",
    "                created = True\n",
    "            except Exception:\n",
    "                created = False\n",
    "        # Wait until online (best-effort)\n",
    "        try:\n",
    "            session.run(\"CALL db.awaitIndexes()\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        ok = _exists(session)\n",
    "        if not ok and strict:\n",
    "            raise RuntimeError(f\"無法建立 Fulltext 索引: {index_name}\")\n",
    "        return ok\n",
    "\n",
    "_USING_VECTOR_ONLY = False\n",
    "_has_fts = _ensure_fulltext_index(_driver, INDEX_FTS, label=\"Chunk\", prop=\"text\", strict=False)\n",
    "\n",
    "# 1) 準備檢索器：若 Fulltext 不可用，則退化成向量-only 檢索\n",
    "if _has_fts:\n",
    "    _hybrid = HybridRetriever(\n",
    "        driver=_driver,\n",
    "        vector_index_name=INDEX_VECTOR,\n",
    "        fulltext_index_name=INDEX_FTS,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ Fulltext 索引不可用或無法建立，將使用向量-only 檢索做為替代。\")\n",
    "    _USING_VECTOR_ONLY = True\n",
    "    _hybrid = _VectorRetriever(\n",
    "        driver=_driver,\n",
    "        index_name=INDEX_VECTOR,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag_hybrid = GraphRAG(retriever=_hybrid, llm=_llm)\n",
    "\n",
    "\n",
    "def _measure_tok_s_hybrid(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"Estimate tok/s from Ollama eval stats; return -1.0 if unavailable.\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or r.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "def qa_hybrid(query: str, top_k: int = 5, ranker: str = \"naive\", alpha: float | None = None, temperature: float | None = None):\n",
    "    \"\"\"Hybrid QA with optional temperature to control LLM variability.\"\"\"\n",
    "    # choose LLM per call if temperature is provided\n",
    "    rag_to_use = _rag_hybrid\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_tmp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(retriever=_hybrid, llm=_llm_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"[Hybrid] 建立帶 temperature 的 LLM 失敗，改用預設：{e}\")\n",
    "            rag_to_use = _rag_hybrid\n",
    "\n",
    "    if _USING_VECTOR_ONLY:\n",
    "        print(\"[Hybrid] 提示：Fulltext 不可用，現以向量-only 模式執行。\")\n",
    "    cfg = {\"top_k\": top_k}\n",
    "    if not _USING_VECTOR_ONLY and ranker == \"linear\":\n",
    "        cfg[\"ranker\"] = HybridSearchRanker.LINEAR\n",
    "        cfg[\"alpha\"] = alpha if alpha is not None else 0.6\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config=cfg, return_context=True)\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[Hybrid] 問題:\", query)\n",
    "    print(\"[Hybrid] 回答:\", resp.answer)\n",
    "\n",
    "    # Print latency and estimate tok/s via a small probe\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    items = getattr(resp, \"retriever_result\", None)\n",
    "    # Build a small context snippet for throughput probe\n",
    "    ctx_texts = []\n",
    "    if items:\n",
    "        try:\n",
    "            # Hybrid items container\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None)\n",
    "                if not content and isinstance(getattr(it, \"metadata\", None), dict):\n",
    "                    content = getattr(it, \"metadata\", {}).get(\"text\")\n",
    "                if content:\n",
    "                    ctx_texts.append(str(content))\n",
    "        except Exception:\n",
    "            # Vector-only list of dicts\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    ctx_texts.append(str(n.get(\"text\", \"\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "    ctx_blob = \"\\n\\n\".join(ctx_texts)\n",
    "    probe_prompt = f\"請根據以下內容簡短回答：\\n\\n{ctx_blob}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (無 eval 統計)\")\n",
    "\n",
    "    if items:\n",
    "        print(\"[Hybrid] 檢索結果摘要:\")\n",
    "        # 支援 HybridRetriever 的 items 容器或 VectorRetriever 的 list 節點\n",
    "        try:\n",
    "            # Hybrid path\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None) or it.get(\"text\", \"\")\n",
    "                meta = getattr(it, \"metadata\", None) or it.get(\"source\", \"\")\n",
    "                print(\"-\", str(content)[:120].replace(\"\\n\", \" \"), str(meta)[:80])\n",
    "        except Exception:\n",
    "            # Vector-only path (list of dicts)\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    print(\"-\", str(n.get(\"text\", \"\"))[:120].replace(\"\\n\", \" \"), f\"(source={n.get('source')})\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# 2) 從檢索到的 Chunk 擴展圖（MENTIONS 與 RELATION）\n",
    "\n",
    "def expand_context_from_chunks(query: str, top_k: int = 5) -> str:\n",
    "    q_emb = _embedder.embed_query(query)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    cypher = \"\"\"\n",
    "    CALL db.index.vector.queryNodes($index, $k, $qemb)\n",
    "    YIELD node, score\n",
    "    WITH collect(node)[..$k] AS chunks\n",
    "    // 擴展相關實體與三元組\n",
    "    UNWIND chunks AS c\n",
    "    OPTIONAL MATCH (c)-[:MENTIONS]->(e:Entity)\n",
    "    OPTIONAL MATCH (e)<-[r:RELATION]->(t:Entity)\n",
    "    WITH chunks, collect(DISTINCT e.name) AS ents,\n",
    "         collect(DISTINCT {h:e.name, rel:r.type, t:t.name, sample: head(coalesce(r.chunks, []))}) AS triples\n",
    "    RETURN [c IN chunks | c.text][..$k] AS chunk_texts, ents[..50] AS entities, triples[..100] AS triples\n",
    "    \"\"\"\n",
    "    with _driver.session() as session:\n",
    "        row = session.run(cypher, index=INDEX_VECTOR, k=top_k, qemb=q_emb).single()\n",
    "    if not row:\n",
    "        return \"\"\n",
    "    chunks = row.get(\"chunk_texts\") or []\n",
    "    ents = row.get(\"entities\") or []\n",
    "    triples = row.get(\"triples\") or []\n",
    "    triple_lines = [f\"({t.get('h')})-[:{t.get('rel')}]->({t.get('t')})\" for t in triples if t]\n",
    "    context = (\n",
    "        \"[Chunks]\\n\" + \"\\n\\n\".join(chunks) +\n",
    "        \"\\n\\n[Entities]\\n\" + \", \".join(ents) +\n",
    "        \"\\n\\n[Triples]\\n\" + \"\\n\".join(triple_lines)\n",
    "    )\n",
    "    return context\n",
    "\n",
    "\n",
    "def qa_graph(query: str, top_k: int = 5, temperature: float | None = None):\n",
    "    \"\"\"Graph expansion QA with metrics and adjustable temperature.\n",
    "\n",
    "    Args:\n",
    "        query: 問題\n",
    "        top_k: 檢索擴展的 chunk 數量\n",
    "        temperature: 若提供，將以該值建立臨時 LLM 以控制隨機性（0 更保守，1 更發散）。\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    context = expand_context_from_chunks(query, top_k=top_k)\n",
    "    if not context:\n",
    "        print(\"[GraphExpand] 無檢索結果\")\n",
    "        return\n",
    "\n",
    "    prompt = f\"請根據提供的內容回答問題，條列且含依據。\\n\\n{context}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "\n",
    "    # 選擇 LLM（支援 per-call 調整 temperature）\n",
    "    llm_obj = _llm\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            llm_obj = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[GraphExpand] 建立帶 temperature 的 LLM 失敗，改用預設：{e}\")\n",
    "            llm_obj = _llm\n",
    "\n",
    "    ans = llm_obj.invoke(prompt).content\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[GraphExpand] 問題:\", query)\n",
    "    print(\"[GraphExpand] 回答:\\n\", ans)\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "\n",
    "    # 以相同 context 構造極簡 probe 估算 tok/s\n",
    "    probe_prompt = f\"請根據以下內容簡短回答：\\n\\n{context}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (無 eval 統計)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdc0dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Fulltext 索引不可用或無法建立，將使用向量-only 檢索做為替代。\n"
     ]
    }
   ],
   "source": [
    "# HybridRetriever + Graph expansion QA\n",
    "from neo4j_graphrag.retrievers.hybrid import HybridRetriever, HybridSearchRanker\n",
    "from neo4j_graphrag.retrievers import VectorRetriever as _VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_VECTOR = \"chunk_embeddings\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# Ensure the fulltext index exists and is online (idempotent, with procedure fallback)\n",
    "def _ensure_fulltext_index(driver, index_name: str, label: str, prop: str = \"text\", strict: bool = False) -> bool:\n",
    "    \"\"\"Try to ensure a fulltext index exists. Returns True if exists/created, else False.\n",
    "    If strict=True, raises when cannot be ensured.\n",
    "    \"\"\"\n",
    "    def _exists(sess) -> bool:\n",
    "        try:\n",
    "            rows = sess.run(\"SHOW INDEXES\").data()\n",
    "            if any(str(r.get(\"name\")) == index_name and (r.get(\"type\") == \"FULLTEXT\" or r.get(\"indexType\") == \"FULLTEXT\") for r in rows):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback check via procedure\n",
    "        try:\n",
    "            r = sess.run(\"CALL db.index.fulltext.list() YIELD name WHERE name = $name RETURN name\", name=index_name).single()\n",
    "            if r:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    with driver.session() as session:\n",
    "        if _exists(session):\n",
    "            return True\n",
    "        created = False\n",
    "        # Try schema DDL (Neo4j 5.x)\n",
    "        try:\n",
    "            cypher = f\"\"\"\n",
    "            CREATE FULLTEXT INDEX {index_name} IF NOT EXISTS\n",
    "            FOR (n:{label}) ON EACH [n.{prop}]\n",
    "            \"\"\"\n",
    "            session.run(cypher)\n",
    "            created = True\n",
    "        except Exception:\n",
    "            created = False\n",
    "        # Fallback to procedure (Neo4j 4.x/compat)\n",
    "        if not created:\n",
    "            try:\n",
    "                session.run(\n",
    "                    \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                    name=index_name,\n",
    "                    labels=[label],\n",
    "                    props=[prop],\n",
    "                )\n",
    "                created = True\n",
    "            except Exception:\n",
    "                created = False\n",
    "        # Wait until online (best-effort)\n",
    "        try:\n",
    "            session.run(\"CALL db.awaitIndexes()\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        ok = _exists(session)\n",
    "        if not ok and strict:\n",
    "            raise RuntimeError(f\"無法建立 Fulltext 索引: {index_name}\")\n",
    "        return ok\n",
    "\n",
    "_USING_VECTOR_ONLY = False\n",
    "_has_fts = _ensure_fulltext_index(_driver, INDEX_FTS, label=\"Chunk\", prop=\"text\", strict=False)\n",
    "\n",
    "# 1) 準備檢索器：若 Fulltext 不可用，則退化成向量-only 檢索\n",
    "if _has_fts:\n",
    "    _hybrid = HybridRetriever(\n",
    "        driver=_driver,\n",
    "        vector_index_name=INDEX_VECTOR,\n",
    "        fulltext_index_name=INDEX_FTS,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ Fulltext 索引不可用或無法建立，將使用向量-only 檢索做為替代。\")\n",
    "    _USING_VECTOR_ONLY = True\n",
    "    _hybrid = _VectorRetriever(\n",
    "        driver=_driver,\n",
    "        index_name=INDEX_VECTOR,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag_hybrid = GraphRAG(retriever=_hybrid, llm=_llm)\n",
    "\n",
    "\n",
    "def _measure_tok_s_hybrid(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"Estimate tok/s from Ollama eval stats; return -1.0 if unavailable.\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or r.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "def qa_hybrid(query: str, top_k: int = 5, ranker: str = \"naive\", alpha: float | None = None, temperature: float | None = None):\n",
    "    \"\"\"Hybrid QA with optional temperature to control LLM variability.\"\"\"\n",
    "    # choose LLM per call if temperature is provided\n",
    "    rag_to_use = _rag_hybrid\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_tmp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(retriever=_hybrid, llm=_llm_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"[Hybrid] 建立帶 temperature 的 LLM 失敗，改用預設：{e}\")\n",
    "            rag_to_use = _rag_hybrid\n",
    "\n",
    "    if _USING_VECTOR_ONLY:\n",
    "        print(\"[Hybrid] 提示：Fulltext 不可用，現以向量-only 模式執行。\")\n",
    "    cfg = {\"top_k\": top_k}\n",
    "    if not _USING_VECTOR_ONLY and ranker == \"linear\":\n",
    "        cfg[\"ranker\"] = HybridSearchRanker.LINEAR\n",
    "        cfg[\"alpha\"] = alpha if alpha is not None else 0.6\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config=cfg, return_context=True)\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[Hybrid] 問題:\", query)\n",
    "    print(\"[Hybrid] 回答:\", resp.answer)\n",
    "\n",
    "    # Print latency and estimate tok/s via a small probe\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    items = getattr(resp, \"retriever_result\", None)\n",
    "    # Build a small context snippet for throughput probe\n",
    "    ctx_texts = []\n",
    "    if items:\n",
    "        try:\n",
    "            # Hybrid items container\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None)\n",
    "                if not content and isinstance(getattr(it, \"metadata\", None), dict):\n",
    "                    content = getattr(it, \"metadata\", {}).get(\"text\")\n",
    "                if content:\n",
    "                    ctx_texts.append(str(content))\n",
    "        except Exception:\n",
    "            # Vector-only list of dicts\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    ctx_texts.append(str(n.get(\"text\", \"\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "    ctx_blob = \"\\n\\n\".join(ctx_texts)\n",
    "    probe_prompt = f\"請根據以下內容簡短回答：\\n\\n{ctx_blob}\\n\\n問題：{query}\\n請用繁體中文回答。\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (無 eval 統計)\")\n",
    "\n",
    "    if items:\n",
    "        print(\"[Hybrid] 檢索結果摘要:\")\n",
    "        # 支援 HybridRetriever 的 items 容器或 VectorRetriever 的 list 節點\n",
    "        try:\n",
    "            # Hybrid path\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None) or it.get(\"text\", \"\")\n",
    "                meta = getattr(it, \"metadata\", None) or it.get(\"source\", \"\")\n",
    "                print(\"-\", str(content)[:120].replace(\"\\n\", \" \"), str(meta)[:80])\n",
    "        except Exception:\n",
    "            # Vector-only path (list of dicts)\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    print(\"-\", str(n.get(\"text\", \"\"))[:120].replace(\"\\n\", \" \"), f\"(source={n.get('source')})\")\n",
    "            except Exception:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d166b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SHOW INDEXES ==\n",
      "{'id': 3, 'name': 'chunk_embeddings', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'VECTOR', 'entityType': 'NODE', 'labelsOrTypes': ['Chunk'], 'properties': ['embedding'], 'indexProvider': 'vector-2.0', 'owningConstraint': None, 'lastRead': neo4j.time.DateTime(2025, 9, 22, 11, 32, 17, 818000000, tzinfo=<UTC>), 'readCount': 48}\n",
      "{'id': 4, 'name': 'chunk_fulltext', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'FULLTEXT', 'entityType': 'NODE', 'labelsOrTypes': ['Chunk'], 'properties': ['text'], 'indexProvider': 'fulltext-1.0', 'owningConstraint': None, 'lastRead': neo4j.time.DateTime(2025, 9, 11, 9, 9, 19, 80000000, tzinfo=<UTC>), 'readCount': 10}\n",
      "{'id': 5, 'name': 'chunk_id_unique', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'RANGE', 'entityType': 'NODE', 'labelsOrTypes': ['Chunk'], 'properties': ['id'], 'indexProvider': 'range-1.0', 'owningConstraint': 'chunk_id_unique', 'lastRead': None, 'readCount': 0}\n",
      "{'id': 7, 'name': 'entity_name_unique', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'RANGE', 'entityType': 'NODE', 'labelsOrTypes': ['Entity'], 'properties': ['name'], 'indexProvider': 'range-1.0', 'owningConstraint': 'entity_name_unique', 'lastRead': None, 'readCount': 0}\n",
      "{'id': 1, 'name': 'index_343aff4e', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'LOOKUP', 'entityType': 'NODE', 'labelsOrTypes': None, 'properties': None, 'indexProvider': 'token-lookup-1.0', 'owningConstraint': None, 'lastRead': neo4j.time.DateTime(2025, 9, 17, 7, 20, 33, 517000000, tzinfo=<UTC>), 'readCount': 216167}\n",
      "{'id': 2, 'name': 'index_f7700477', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'LOOKUP', 'entityType': 'RELATIONSHIP', 'labelsOrTypes': None, 'properties': None, 'indexProvider': 'token-lookup-1.0', 'owningConstraint': None, 'lastRead': None, 'readCount': 0}\n",
      "\n",
      "== Fulltext list ==\n",
      "fulltext list error: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `db.index.fulltext.list` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n",
      "DDL 建立/確保 Fulltext 索引完成\n",
      "Fulltext 仍不可用: chunk_text_fts\n"
     ]
    }
   ],
   "source": [
    "# 診斷與（重新）建立 Fulltext 索引（可選）\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "\n",
    "_driver_diag = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "with _driver_diag.session() as s:\n",
    "    print(\"== SHOW INDEXES ==\")\n",
    "    try:\n",
    "        for r in s.run(\"SHOW INDEXES\").data():\n",
    "            print(r)\n",
    "    except Exception as e:\n",
    "        print(\"show indexes error:\", e)\n",
    "\n",
    "    print(\"\\n== Fulltext list ==\")\n",
    "    try:\n",
    "        for r in s.run(\"CALL db.index.fulltext.list()\").data():\n",
    "            print(r)\n",
    "    except Exception as e:\n",
    "        print(\"fulltext list error:\", e)\n",
    "\n",
    "    # 嘗試以 DDL 建立（Neo4j 5.x），失敗則以 procedure 建立\n",
    "    try:\n",
    "        s.run(f\"\"\"\n",
    "        CREATE FULLTEXT INDEX {INDEX_FTS} IF NOT EXISTS\n",
    "        FOR (c:Chunk) ON EACH [c.text]\n",
    "        \"\"\")\n",
    "        print(\"DDL 建立/確保 Fulltext 索引完成\")\n",
    "    except Exception as e:\n",
    "        print(\"DDL 建立失敗，嘗試 procedure：\", e)\n",
    "        try:\n",
    "            s.run(\n",
    "                \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                name=INDEX_FTS,\n",
    "                labels=[\"Chunk\"],\n",
    "                props=[\"text\"],\n",
    "            )\n",
    "            print(\"procedure 建立 Fulltext 索引完成\")\n",
    "        except Exception as e2:\n",
    "            print(\"procedure 亦失敗：\", e2)\n",
    "\n",
    "    # 等待索引 online（最佳努力）\n",
    "    try:\n",
    "        s.run(\"CALL db.awaitIndexes()\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    ok = False\n",
    "    try:\n",
    "        ok = s.run(\n",
    "            \"CALL db.index.fulltext.list() YIELD name WHERE name = $n RETURN count(*) AS c\",\n",
    "            n=INDEX_FTS,\n",
    "        ).single().get(\"c\", 0) > 0\n",
    "    except Exception:\n",
    "        ok = False\n",
    "\n",
    "print(\"Fulltext 可用:\" if ok else \"Fulltext 仍不可用:\", INDEX_FTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7234054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hybrid] 提示：Fulltext 不可用，現以向量-only 模式執行。\n",
      "\n",
      "[Hybrid] 問題: 山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\n",
      "[Hybrid] 回答: 根據提供的文本，山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。\n",
      "Inference latency: 1560.6 ms\n",
      "\n",
      "[Hybrid] 問題: 山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\n",
      "[Hybrid] 回答: 根據提供的文本，山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。\n",
      "Inference latency: 1560.6 ms\n",
      "LLM throughput: 78.18 tok/s (Ollama eval)\n",
      "[Hybrid] 檢索結果摘要:\n",
      "- {'id': '0_17', 'text': '生素群,以滿足山羊生理需要。\\n\\n維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的N {'score': 0.8934812545776367, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': '之總貯存量。\\n\\n維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織 {'score': 0.8598113059997559, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_434', 'text': '轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變 {'score': 0.8474764823913574, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': '痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨 {'score': 0.8346552848815918, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': '例。3.增加日糧中磷酸氫鈣的添加比例。\\n\\n山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\\n\\n山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素 {'score': 0.832240104675293, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n",
      "LLM throughput: 78.18 tok/s (Ollama eval)\n",
      "[Hybrid] 檢索結果摘要:\n",
      "- {'id': '0_17', 'text': '生素群,以滿足山羊生理需要。\\n\\n維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的N {'score': 0.8934812545776367, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': '之總貯存量。\\n\\n維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織 {'score': 0.8598113059997559, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_434', 'text': '轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變 {'score': 0.8474764823913574, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': '痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨 {'score': 0.8346552848815918, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': '例。3.增加日糧中磷酸氫鈣的添加比例。\\n\\n山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\\n\\n山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素 {'score': 0.832240104675293, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n"
     ]
    }
   ],
   "source": [
    "RUN_QA_hybrid = True\n",
    "if RUN_QA_hybrid:\n",
    "    qa_hybrid(\"山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\", top_k=5, ranker=\"linear\", alpha=0.6, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed67d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GraphExpand] 問題: 山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\n",
      "[GraphExpand] 回答:\n",
      " 根據提供的內容，山羊缺乏維生素A會導致視力出現以下問題：\n",
      "\n",
      "* 夜盲症：山羊缺乏維生素A會導致夜盲症，山羊在黑暗中無法正常地發現物體。\n",
      "* 视力下降：維生素A是眼睛視紫質(rhodopsin)色素形成與再生的必要物質，因此山羊缺乏維生素A會導致視力下降。\n",
      "\n",
      "這兩個問題都是由於山羊的維生素A水平不足所引起的，並且如果缺乏維生素A太久，可能會導致瞎眼。\n",
      "Inference latency: 4473.8 ms\n",
      "LLM throughput: 76.81 tok/s (Ollama eval)\n"
     ]
    }
   ],
   "source": [
    "RUN_QA_graph_DEMO = True\n",
    "if RUN_QA_graph_DEMO:\n",
    "    qa_graph(\"山羊缺乏維生素A會導致視力出現什麼問題？繁體中文回答\", top_k=5,temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b077f8",
   "metadata": {},
   "source": [
    "## 🔍 利用圖譜關係的進階檢索模式\n",
    "\n",
    "相較於 QA-only 的純向量檢索，以下兩種模式能充分利用已建立的知識圖譜關係：\n",
    "\n",
    "### 1️⃣ Graph Expansion 模式 (推薦)\n",
    "**最佳選擇**：直接利用實體關係和三元組進行推理\n",
    "\n",
    "**優勢**：\n",
    "- 🧠 **智能推理**：通過 `MENTIONS` 關係找到相關實體\n",
    "- 🔗 **關係擴展**：利用 `RELATION` 三元組豐富上下文\n",
    "- 📊 **結構化資訊**：提供 [Chunks] + [Entities] + [Triples] 完整脈絡\n",
    "- 🎯 **精準回答**：基於知識圖譜的邏輯推理\n",
    "\n",
    "**使用方式**：\n",
    "```python\n",
    "# 基礎用法\n",
    "qa_graph(\"你的問題\", top_k=5)\n",
    "\n",
    "# 控制創意度（temperature）\n",
    "qa_graph(\"你的問題\", top_k=5, temperature=0.0)  # 保守回答\n",
    "qa_graph(\"你的問題\", top_k=5, temperature=0.8)  # 創意回答\n",
    "```\n",
    "\n",
    "### 2️⃣ Hybrid 模式\n",
    "**平衡選擇**：結合向量檢索和全文檢索\n",
    "\n",
    "**優勢**：\n",
    "- 🔍 **雙重檢索**：向量語意 + 關鍵字匹配\n",
    "- ⚡ **高效能**：比純圖遍歷更快\n",
    "- 🎛️ **可調權重**：控制向量vs全文的比重\n",
    "\n",
    "**使用方式**：\n",
    "```python\n",
    "# 基礎混合檢索\n",
    "qa_hybrid(\"你的問題\", top_k=5)\n",
    "\n",
    "# 線性加權檢索（推薦）\n",
    "qa_hybrid(\"你的問題\", top_k=5, ranker=\"linear\", alpha=0.6)\n",
    "\n",
    "# 控制創意度\n",
    "qa_hybrid(\"你的問題\", top_k=5, temperature=0.7)\n",
    "```\n",
    "\n",
    "### 🆚 三種模式比較\n",
    "\n",
    "| 特性 | QA-only | Hybrid | Graph Expansion |\n",
    "|------|---------|--------|-----------------|\n",
    "| **檢索方式** | 純向量 | 向量+全文 | 向量+圖譜關係 |\n",
    "| **使用圖譜** | ❌ | ❌ | ✅ |\n",
    "| **推理能力** | 基礎 | 中等 | 高級 |\n",
    "| **回答品質** | 良好 | 很好 | 優秀 |\n",
    "| **執行速度** | 最快 | 快 | 中等 |\n",
    "| **適用場景** | 簡單問答 | 關鍵字+語意 | 複雜推理 |\n",
    "\n",
    "### 💡 建議使用策略\n",
    "\n",
    "1. **複雜推理問題** → 使用 `qa_graph()`\n",
    "2. **需要精確匹配** → 使用 `qa_hybrid()` \n",
    "3. **快速簡單查詢** → 使用 `qa_only()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a91c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 🔍 進階檢索模式比較示範 ===\n",
      "\n",
      "🚀 1. Graph Expansion 模式 (利用圖譜關係)\n",
      "==================================================\n",
      "\n",
      "[GraphExpand] 問題: 山羊缺乏維生素A會導致視力出現什麼問題？\n",
      "[GraphExpand] 回答:\n",
      " 根據提供的內容，山羊缺乏維生素A會導致夜盲症（夜盲症）。維生素A是眼睛視紫質(rhodopsin)色素形成與再生的必要物質，因此缺乏維生素A會導致山羊失去視力。\n",
      "Inference latency: 6731.4 ms\n",
      "\n",
      "[GraphExpand] 問題: 山羊缺乏維生素A會導致視力出現什麼問題？\n",
      "[GraphExpand] 回答:\n",
      " 根據提供的內容，山羊缺乏維生素A會導致夜盲症（夜盲症）。維生素A是眼睛視紫質(rhodopsin)色素形成與再生的必要物質，因此缺乏維生素A會導致山羊失去視力。\n",
      "Inference latency: 6731.4 ms\n",
      "LLM throughput: 79.82 tok/s (Ollama eval)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🔍 2. Hybrid 模式 (向量 + 全文檢索)\n",
      "==================================================\n",
      "LLM throughput: 79.82 tok/s (Ollama eval)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🔍 2. Hybrid 模式 (向量 + 全文檢索)\n",
      "==================================================\n",
      "[Hybrid] 提示：Fulltext 不可用，現以向量-only 模式執行。\n",
      "[Hybrid] 提示：Fulltext 不可用，現以向量-only 模式執行。\n",
      "\n",
      "[Hybrid] 問題: 山羊缺乏維生素A會導致視力出現什麼問題？\n",
      "[Hybrid] 回答: According to the provided context, mountain goats lacking vitamin A will experience night blindness, which can lead to severe consequences such as complete loss of vision.\n",
      "Inference latency: 3100.1 ms\n",
      "\n",
      "[Hybrid] 問題: 山羊缺乏維生素A會導致視力出現什麼問題？\n",
      "[Hybrid] 回答: According to the provided context, mountain goats lacking vitamin A will experience night blindness, which can lead to severe consequences such as complete loss of vision.\n",
      "Inference latency: 3100.1 ms\n",
      "LLM throughput: 84.10 tok/s (Ollama eval)\n",
      "[Hybrid] 檢索結果摘要:\n",
      "- {'id': '0_17', 'text': '生素群,以滿足山羊生理需要。\\n\\n維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的N {'score': 0.8930339813232422, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': '之總貯存量。\\n\\n維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織 {'score': 0.858832836151123, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n",
      "- {'id': '0_434', 'text': '轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變 {'score': 0.8501629829406738, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': '痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨 {'score': 0.8311724662780762, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': '例。3.增加日糧中磷酸氫鈣的添加比例。\\n\\n山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\\n\\n山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素 {'score': 0.8266477584838867, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "\n",
      "================================================================================\n",
      "\n",
      "💡 比較結果：\n",
      "- Graph Expansion 提供更豐富的實體關係和三元組資訊\n",
      "- Hybrid 模式結合語意和關鍵字匹配，檢索更全面\n",
      "- 兩種模式都比 QA-only 提供更深入的上下文理解\n",
      "LLM throughput: 84.10 tok/s (Ollama eval)\n",
      "[Hybrid] 檢索結果摘要:\n",
      "- {'id': '0_17', 'text': '生素群,以滿足山羊生理需要。\\n\\n維生素A:維生素A的缺乏會導致山羊食慾減少,種公羊精液品質降低、缺乏性慾,母羊則發情不易、生殖效率降低,嚴重缺乏亦會造成山羊夜盲之病變。因此1981年版的N {'score': 0.8930339813232422, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': '之總貯存量。\\n\\n維生素A之功能包括如下：1.維持正常之視力：因維生素A是眼睛視紫質(rhodopsin)色素形成與再生所必需的物質。所以山羊缺乏維生素A會導致夜盲症，嚴重甚至瞎眼。2.織 {'score': 0.858832836151123, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n",
      "- {'id': '0_434', 'text': '轉換成視覺醇。維生素A在細胞與組織的生長發育上扮演重要的角色。肉羊缺乏維生素A會導致其食慾減少，種公羊會缺乏性慾且精液品質低落。種母羊則會發情不易、生殖效率差，更嚴重的缺乏會造成肉羊眼睛病變 {'score': 0.8501629829406738, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': '痺，尤其後肢僵硬而不能提起。硒由乳汁排出而貯存在皮毛。山羊與其他家畜一樣很容易發生中毒，一般認為山羊硒中毒之劑量與綿羊者相同為3ppm。山羊中毒之症狀為喪失食慾、失重、精神沮喪、先便秘而後隨 {'score': 0.8311724662780762, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': '例。3.增加日糧中磷酸氫鈣的添加比例。\\n\\n山羊夜盲症：山羊夜盲症源自於維生素A缺乏,日糧中維生素A缺乏會導致山羊失去視力。\\n\\n山羊夜盲症的預防方式有這些：1.注意以放牧飼養山羊的維生素 {'score': 0.8266477584838867, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "\n",
      "================================================================================\n",
      "\n",
      "💡 比較結果：\n",
      "- Graph Expansion 提供更豐富的實體關係和三元組資訊\n",
      "- Hybrid 模式結合語意和關鍵字匹配，檢索更全面\n",
      "- 兩種模式都比 QA-only 提供更深入的上下文理解\n"
     ]
    }
   ],
   "source": [
    "# 🧪 進階檢索模式示範與比較\n",
    "print(\"=== 🔍 進階檢索模式比較示範 ===\\n\")\n",
    "\n",
    "# 測試問題\n",
    "test_query = \"山羊缺乏維生素A會導致視力出現什麼問題？\"\n",
    "\n",
    "print(\"🚀 1. Graph Expansion 模式 (利用圖譜關係)\")\n",
    "print(\"=\" * 50)\n",
    "qa_graph(test_query, top_k=5, temperature=0.3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"🔍 2. Hybrid 模式 (向量 + 全文檢索)\")\n",
    "print(\"=\" * 50)\n",
    "qa_hybrid(test_query, top_k=5, ranker=\"linear\", alpha=0.6, temperature=0.3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "print(\"💡 比較結果：\")\n",
    "print(\"- Graph Expansion 提供更豐富的實體關係和三元組資訊\")\n",
    "print(\"- Hybrid 模式結合語意和關鍵字匹配，檢索更全面\")\n",
    "print(\"- 兩種模式都比 QA-only 提供更深入的上下文理解\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcbee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎛️ 參數調優示範\n",
    "print(\"=== 🎛️ 不同參數設定的效果比較 ===\\n\")\n",
    "\n",
    "test_query = \"山羊缺乏維生素A會導致什麼健康問題？\"\n",
    "\n",
    "print(\"🧊 低溫度 (temperature=0.0) - 保守回答\")\n",
    "print(\"-\" * 40)\n",
    "qa_graph(test_query, top_k=3, temperature=0.0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"🔥 高溫度 (temperature=0.9) - 創意回答\")\n",
    "print(\"-\" * 40)\n",
    "qa_graph(test_query, top_k=3, temperature=0.9)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"⚖️ Hybrid 模式權重調整示範\")\n",
    "print(\"-\" * 40)\n",
    "print(\"📊 Alpha=0.3 (偏重全文檢索)\")\n",
    "qa_hybrid(test_query, top_k=3, ranker=\"linear\", alpha=0.3, temperature=0.5)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "print(\"📊 Alpha=0.8 (偏重向量檢索)\")\n",
    "qa_hybrid(test_query, top_k=3, ranker=\"linear\", alpha=0.8, temperature=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM+KB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
