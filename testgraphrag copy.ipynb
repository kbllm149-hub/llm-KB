{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9a0c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    session.run(\"DROP INDEX chunk_embeddings IF EXISTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc5df9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mdriver\u001b[49m.session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m      2\u001b[39m     indexes = session.run(\u001b[33m\"\u001b[39m\u001b[33mSHOW INDEXES\u001b[39m\u001b[33m\"\u001b[39m).data()\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indexes:\n\u001b[32m      4\u001b[39m         \u001b[38;5;66;03m# å®‰å…¨åœ°åˆ—å°æ‰€æœ‰æ¬„ä½ï¼Œé¿å… KeyErrorï¼ˆä¸åŒ Neo4j ç‰ˆæœ¬æ¬„ä½åç¨±å¯èƒ½ä¸åŒï¼‰\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "with driver.session() as session:\n",
    "    indexes = session.run(\"SHOW INDEXES\").data()\n",
    "    for idx in indexes:\n",
    "        # å®‰å…¨åœ°åˆ—å°æ‰€æœ‰æ¬„ä½ï¼Œé¿å… KeyErrorï¼ˆä¸åŒ Neo4j ç‰ˆæœ¬æ¬„ä½åç¨±å¯èƒ½ä¸åŒï¼‰\n",
    "        try:\n",
    "            name = idx.get(\"name\")\n",
    "            typ = idx.get(\"type\") or idx.get(\"indexType\")\n",
    "            labels = idx.get(\"labelsOrTypes\") or idx.get(\"entityType\")\n",
    "            props = idx.get(\"properties\") or idx.get(\"indexedProperties\")\n",
    "            print(name, typ, labels, props)\n",
    "        except Exception:\n",
    "            print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdbb294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_33 æ”¾ç‰§æ–¼ç¼ºä¹å¾®é‡å…ƒç´ ç¡’çš„åœŸåœ°ä¸Šæˆ–æ˜¯æ”é£Ÿå“è³ªå¾ˆå·®çš„ä¹¾è‰,é€ æˆå‰›å‡ºç”Ÿä»”ç¾Šç„¡æ³•ç«™ç«‹ã€‚\n",
      "\n",
      "ä»”ç¾Šç™½è‚Œç—…çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ¯ç¾Šæ‡·å­•æœŸè¦æ³¨æ„ç¶­ç”Ÿç´ Eè£œå……ã€‚2.å‡ºç”Ÿä»”ç¾Šè¦æ³¨æ„ç¶­ç”Ÿç´ Eè£œå……ã€‚3.å¥åº·çš„æ¯ç¾Šæ‰èƒ½ç”Ÿç”¢å„ªè³ªçš„åˆä¹³ã€‚4.æ¯ç¾Šæœ‰èƒè¡£æ»¯ç•™çš„ç¾è±¡ç”¢ç”Ÿã€‚5.ä»”ç¾Šå¯ç”¨æ³¨å°„ç¶­ç”Ÿç´ Eçš„æ–¹å¼è£œå……ã€‚6.æˆç¾Šå¯ä»¥æ·»åŠ ç¶­ç”Ÿç´ Eåœ¨ç²¾æ–™ä¸­ã€‚\n",
      "\n",
      "å±±ç¾Šç•°é£Ÿç™–ï¼šä»”ç¾Šç•°é£Ÿç™–æ˜¯æºè‡ªæ–¼ç¤¦ç‰©è³ªç£·çš„ç¼ºä¹,ç£·çš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»é£Ÿæ…¾,æ›´æœ‰ç”¢ç”Ÿå•ƒæ¯›ã€åƒçš®åŠé›œç‰©ç­‰ç•°é£Ÿç™–è¡Œç‚ºã€‚\n",
      "\n",
      "å±±ç¾Šç•°é£Ÿç™–çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ—¥ç³§å…§éˆ£ç£·æ¯”ç‡å¹³è¡¡ã€‚2.å¢åŠ æ—¥ç³§ä¸­ç©€é¡æ¯”ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼º\n",
      "0_34 ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´ Aç¼ºä¹ã€‚2.ç„¡æ³•æ”¾ç‰§é£¼é¤Šå±±ç¾Šè€…éœ€æä¾›é’è‰çµ¦é£¼é¤Šçš„å±±ç¾Šã€‚3.æ³¨æ„åœ¨é£¼æ–™å…§æ·»åŠ ç¶­ç”Ÿç´ Aã€‚4.æ¯ç¾Šæ‡·å­•æœŸè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚5.å‡ºç”Ÿä»”ç¾Šè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚6.å¥åº·çš„æ¯ç¾Šæ‰èƒ½ç”Ÿç”¢å„ªè³ªçš„åˆä¹³ã€‚\n",
      "\n",
      "å±±ç¾Šé¼“è„¹(bloat)ï¼šå±±ç¾Šé¼“è„¹é€šå¸¸æ˜¯ç”±å¤§é‡æ¡é£Ÿè±å¯Œçš„æ½®æ¿•æ–°é®®è‰æ–™æˆ–ç©€é¡å¼•èµ·çš„,å¦‚ä¸‰è‘‰è‰,è‹œè“¿æˆ–è±†ç§‘ç‰§è‰ã€‚é€™äº›æ½®æ¿•æ–°é®®è‰æ–™åœ¨ç˜¤èƒƒä¸­å½¢æˆå¾®å°çš„æ°£æ³¡,å°è‡´å±±ç¾Šå™¯æ°£ç„¡æ³•æ’å‡ºã€‚ç˜¤èƒƒéš¨è‘—æ³¡æ²«è†¨è„¹å£“è¿«æ©«éš”è†œ,é€ æˆå±±ç¾Šå¯èƒ½å› å‘¼å¸æˆ–å¾ªç’°è¡°ç«­è€Œå¾ˆ\n",
      "0_406 ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹”æ­£å¸¸ï¼šç•¶ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå‰‡èº«é«”ä¹‹å‘¼å¸é“ã€è…¸é“ã€ç”Ÿæ®–é“ã€å°¿é“èˆ‡çœ¼ç›è¡¨çš®ç´°èƒæœƒè§’è³ªåŒ–(epithelial keratinization)ã€‚3.å¼·æŠµæŠ—åŠ›ï¼šå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå®¹æ˜“ä½¿å±±ç¾Šæ„ŸæŸ“ç–¾ç—…åŠå¯„ç”ŸèŸ²ã€ç¹æ®–æ€§èƒ½ä¸ä½³ã€‚\n",
      "\n",
      "é€ æˆå±±ç¾Šç¶­ç”Ÿç´ Aç¼ºä¹ä¹‹åŸå› æœ‰äºŒï¼Œä¸»è¦ç‚ºé£¼ç³§ç¶­ç”Ÿç´ Aå«é‡ä¸è¶³ï¼Œå°¤å…¶åœ¨ä¹¾æ—±å­£ç¯€æˆ–å†¬å­£ï¼Œå› é’è‰ä¾›æ‡‰å—é™ï¼Œæˆ–é¤µé£¼å±±ç¾Šä¹‹ä¹¾è‰å“è³ªä¸è‰¯ï¼Œä¾‹å¦‚ä¹¾è‰è²¯å­˜æœŸé–“å¤ªé•·åŠä¹¾è‰è²¯å­˜ä¸ç•¶ï¼Œå› æ—¥æ›¬èˆ‡é›¨æ·‹ã€‚è£½é€ ä¹¾è‰æ™‚ï¼Œå› å¤©å€™ä¸è‰¯ï¼Œä¹¾è‰\n",
      "0_434 è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Šè€Œå½¢æˆå¤œç›²ç—‡ã€‚å› æ­¤åœ¨è‚‰ç¾Šæ—¥ç³§ä¸­ï¼Œæ¯å…¬æ–¤ç²¾æ–™è‡³å°‘è¦å«3500åœ‹éš›å–®ä½(IU)ä¹‹ç¶­ç”Ÿç´ Aã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Dï¼šåŒæ¨£æ˜¯è„‚æº¶æ€§ç¶­ç”Ÿç´ ï¼Œå…¶ä¸»è¦æ˜¯å¹«åŠ©å‹•ç‰©æœ‰æ•ˆç‡çš„å¸æ”¶åŠåˆ©ç”¨éˆ£èˆ‡ç£·ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´è‚‰ç¾Šéˆ£èˆ‡ç£·çš„å¸æ”¶ä¸å¹³è¡¡ï¼Œå½±éŸ¿è‚‰ç¾Šéª¨éª¼ç™¼è‚²ï¼Œä»”ç¾ŠåŠæˆç¾Šå‡æœƒç”¢ç”Ÿè»Ÿéª¨ç—‡åŠéª¨è³ªç–é¬†ç—‡ã€‚ç”±æ–¼é™½å…‰ä¸­çš„ç´«å¤–ç·šç…§å°„å‹•ç‰©çš®è†šï¼Œæœƒä½¿å‹•ç‰©çš®è†šä¸­ä¹‹7dehydrocholesterolè½‰åŒ–æˆç¶­ç”Ÿç´ D3ã€‚å› æ­¤æ”¾ç‰§ç¾Šéš»ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dï¼Œç„¶è€Œåœˆé£¼çš„ç¾Š\n"
     ]
    }
   ],
   "source": [
    "with driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.text CONTAINS \"å¤œç›²ç—‡\"\n",
    "    RETURN c.id, c.text, c.source\n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "    for r in result:\n",
    "        print(r[\"c.id\"], r[\"c.text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45ea402",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8dc7959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(å±±ç¾Š)-[:æ¯æ—¥ç²å¾—]->(å¾®ç”Ÿç‰©èƒºåŸºé…¸è³ªèˆ‡é‡)  <- sample chunk: 0_9\n",
      "(å±±ç¾Š)-[:å—åˆ°å½±éŸ¿]->(æ—¥æ›¬)  <- sample chunk: 0_784\n",
      "(å±±ç¾Š)-[:æ˜“ç™¼ç”Ÿ]->(æ³¢çˆ¾ç¨®ã€å®‰æ ¼æ‹‰ç¨®)  <- sample chunk: 0_771\n",
      "(å±±ç¾Š)-[:èª¿æ§]->(éˆ£å’Œæœ‰æ©Ÿç£·çš„æ¿ƒåº¦)  <- sample chunk: 0_769\n",
      "(å±±ç¾Š)-[:ä¸»è¦åŸå› ]->(éåº¦åœ°æå¤±ç¤¦ç‰©è³ª)  <- sample chunk: 0_768\n",
      "(å±±ç¾Š)-[:æ˜“ç™¼ç”Ÿ]->(ä¸Šé¡åŠä¸‹é¡éª¨)  <- sample chunk: 0_768\n",
      "(å±±ç¾Š)-[:ç½¹æ‚£]->(çº–ç¶­æ€§éª¨ç‡Ÿé¤Šä¸è‰¯ç—‡)  <- sample chunk: 0_767\n",
      "(å±±ç¾Š)-[:æ‚£æœ‰]->(çº–ç¶­æ€§éª¨ç‡Ÿé¤Šä¸è‰¯ç—‡(Osteodystrophia fibrosa))  <- sample chunk: 0_766\n",
      "(å±±ç¾Š)-[:æ­»äº¡]->(å·´æ–¯å¾·æ¡¿èŒæ€§è‚ºç‚)  <- sample chunk: 0_746\n",
      "(å±±ç¾Š)-[:infected_by]->(Mycobacterium paratuberculosis)  <- sample chunk: 0_742\n"
     ]
    }
   ],
   "source": [
    "# 9ï¸âƒ£ é©—è­‰ï¼šåˆ—å‡ºéƒ¨åˆ†ä¸‰å…ƒçµ„èˆ‡ä¾†æº (ä½¿ç”¨é—œä¿‚ä¸Šçš„ chunks é™£åˆ—èˆ‡ç¯€é» Mentions)\n",
    "with driver.session() as session:\n",
    "    result = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        RETURN h.name AS head, r.type AS relation, t.name AS tail, r.chunks AS chunks\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    "    rows = result.data()\n",
    "    if not rows:\n",
    "        print(\"å°šæœªæ‰¾åˆ°ä»»ä½•ä¸‰å…ƒçµ„ï¼Œè«‹å…ˆåŸ·è¡Œä¸Šä¸€å€‹æŠ½å–/å¯«å…¥ cellã€‚\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            sample = row.get(\"chunks\")\n",
    "            if isinstance(sample, list) and sample:\n",
    "                sample = sample[0]\n",
    "            print(f\"({row['head']})-[:{row['relation']}]->({row['tail']})  <- sample chunk: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8346a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(å±±ç¾Š)-[:ç¼ºä¹å°è‡´]->(å¤œç›²ç—‡)  <- sample chunk: 0_406\n",
      "(ç¶­ç”Ÿç´ A)-[:ç¼ºä¹]->(å±±ç¾Šå¤œç›²ç—‡)  <- sample chunk: 0_33\n",
      "(ç¶­ç”Ÿç´ A)-[:ç¼ºä¹]->(å¤œç›²ç—‡)  <- sample chunk: 0_21\n",
      "(ç¶­ç”Ÿç´ A)-[:ç¼ºä¹å°è‡´]->(å¤œç›²ç—‡)  <- sample chunk: 0_17\n",
      "(è‚‰ç¾Š)-[:ç¼ºä¹]->(å¤œç›²ç—‡)  <- sample chunk: 0_434\n",
      "(ç¶­ç”Ÿç´ E)-[:ç¼ºä¹å°è‡´]->(å¤œç›²ç—‡)  <- sample chunk: 0_409\n",
      "(å±±ç¾Šå¤œç›²ç—‡)-[:æºè‡ªæ–¼]->(ç¶­ç”Ÿç´ Aç¼ºä¹)  <- sample chunk: 0_34\n",
      "(ç¼ºä¹ç¶­ç”Ÿç´ A)-[:å°è‡´]->(å¤œç›²ç—‡)  <- sample chunk: 0_406\n",
      "(ç¶­ä»–å‘½A)-[:ç¼ºä¹]->(å¤œç›²ç—‡)  <- sample chunk: 0_617\n"
     ]
    }
   ],
   "source": [
    "# 9ï¸âƒ£ é©—è­‰ï¼šåˆ—å‡ºéƒ¨åˆ†ä¸‰å…ƒçµ„èˆ‡ä¾†æº (ä½¿ç”¨é—œä¿‚ä¸Šçš„ chunks é™£åˆ—èˆ‡ç¯€é» Mentions)\n",
    "\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        WHERE h.name CONTAINS 'å¤œç›²ç—‡' AND r.chunks <> 'None'\n",
    "        OR t.name CONTAINS 'å¤œç›²ç—‡' AND r.chunks <> 'None'\n",
    "        OR r.chunks CONTAINS 'å¤œç›²ç—‡' AND r.chunks <> 'None'\n",
    "        RETURN h.name AS head, r.type AS relation, t.name AS tail, r.chunks AS chunks\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    "    rows = result.data()\n",
    "    if not rows:\n",
    "        print(\"å°šæœªæ‰¾åˆ°ä»»ä½•ä¸‰å…ƒçµ„ï¼Œè«‹å…ˆåŸ·è¡Œä¸Šä¸€å€‹æŠ½å–/å¯«å…¥ cellã€‚\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            sample = row.get(\"chunks\")\n",
    "            if isinstance(sample, list) and sample:\n",
    "                sample = sample[0]\n",
    "            print(f\"({row['head']})-[:{row['relation']}]->({row['tail']})  <- sample chunk: {sample}\")\n",
    "            #print(f\"({row['head']})-[:{row['relation']}]->({row['tail']}) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af122f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aint\\Desktop\\LLM+KB\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å­˜åœ¨æœ¬åœ°å‘é‡æ¨¡å‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector index 'chunk_embeddings' å·²å­˜åœ¨\n",
      "ğŸ“„ è¼‰å…¥æª”æ¡ˆå…± 799 å€‹ chunk\n",
      "âœ… å·²å¯«å…¥ 799 å€‹ chunk åˆ° Neo4j\n",
      "âœ… å·²å¯«å…¥ 799 å€‹ chunk åˆ° Neo4j\n",
      "âœ… å·²å¯«å…¥ 7255 æ¢ (Entity)-[RELATION]->(Entity) é—œä¿‚ï¼Œä¸¦å»ºç«‹ä¾†æº MENTIONS èˆ‡ r.chunks\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ¯æ—¥ç²å¾—]->(å¾®ç”Ÿç‰©èƒºåŸºé…¸è³ªèˆ‡é‡)  <- sample chunk: 0_9\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:å—åˆ°å½±éŸ¿]->(æ—¥æ›¬)  <- sample chunk: 0_784\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ˜“ç™¼ç”Ÿ]->(æ³¢çˆ¾ç¨®ã€å®‰æ ¼æ‹‰ç¨®)  <- sample chunk: 0_771\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:èª¿æ§]->(éˆ£å’Œæœ‰æ©Ÿç£·çš„æ¿ƒåº¦)  <- sample chunk: 0_769\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:ä¸»è¦åŸå› ]->(éåº¦åœ°æå¤±ç¤¦ç‰©è³ª)  <- sample chunk: 0_768\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ˜“ç™¼ç”Ÿ]->(ä¸Šé¡åŠä¸‹é¡éª¨)  <- sample chunk: 0_768\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:ç½¹æ‚£]->(çº–ç¶­æ€§éª¨ç‡Ÿé¤Šä¸è‰¯ç—‡)  <- sample chunk: 0_767\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ‚£æœ‰]->(çº–ç¶­æ€§éª¨ç‡Ÿé¤Šä¸è‰¯ç—‡(Osteodystrophia fibrosa))  <- sample chunk: 0_766\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ­»äº¡]->(å·´æ–¯å¾·æ¡¿èŒæ€§è‚ºç‚)  <- sample chunk: 0_746\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:infected_by]->(Mycobacterium paratuberculosis)  <- sample chunk: 0_742\n",
      "âœ… å·²å¯«å…¥ 7255 æ¢ (Entity)-[RELATION]->(Entity) é—œä¿‚ï¼Œä¸¦å»ºç«‹ä¾†æº MENTIONS èˆ‡ r.chunks\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ¯æ—¥ç²å¾—]->(å¾®ç”Ÿç‰©èƒºåŸºé…¸è³ªèˆ‡é‡)  <- sample chunk: 0_9\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:å—åˆ°å½±éŸ¿]->(æ—¥æ›¬)  <- sample chunk: 0_784\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ˜“ç™¼ç”Ÿ]->(æ³¢çˆ¾ç¨®ã€å®‰æ ¼æ‹‰ç¨®)  <- sample chunk: 0_771\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:èª¿æ§]->(éˆ£å’Œæœ‰æ©Ÿç£·çš„æ¿ƒåº¦)  <- sample chunk: 0_769\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:ä¸»è¦åŸå› ]->(éåº¦åœ°æå¤±ç¤¦ç‰©è³ª)  <- sample chunk: 0_768\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ˜“ç™¼ç”Ÿ]->(ä¸Šé¡åŠä¸‹é¡éª¨)  <- sample chunk: 0_768\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:ç½¹æ‚£]->(çº–ç¶­æ€§éª¨ç‡Ÿé¤Šä¸è‰¯ç—‡)  <- sample chunk: 0_767\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ‚£æœ‰]->(çº–ç¶­æ€§éª¨ç‡Ÿé¤Šä¸è‰¯ç—‡(Osteodystrophia fibrosa))  <- sample chunk: 0_766\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:æ­»äº¡]->(å·´æ–¯å¾·æ¡¿èŒæ€§è‚ºç‚)  <- sample chunk: 0_746\n",
      "ğŸ” ä¸‰å…ƒçµ„: (å±±ç¾Š)-[:infected_by]->(Mycobacterium paratuberculosis)  <- sample chunk: 0_742\n",
      "\n",
      "â“ å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œ\n",
      "ğŸŸ© å›ç­”: æ ¹æ“šæ–‡ç« ï¼Œå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚\n",
      "â±ï¸ Inference latency: 872.6 ms\n",
      "\n",
      "â“ å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œ\n",
      "ğŸŸ© å›ç­”: æ ¹æ“šæ–‡ç« ï¼Œå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚\n",
      "â±ï¸ Inference latency: 872.6 ms\n",
      "ğŸš€ LLM throughput: N/A (Ollama æœªæä¾› eval çµ±è¨ˆ)\n",
      "ğŸŸ¨ æª¢ç´¢çµæœç‚ºç©ºï¼Œæ”¹ç”¨å‘é‡ç´¢å¼• fallbackï¼š\n",
      "- ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„NRC,å»ºè­°ç¶­ç”Ÿç´ Aåœ¨å±±ç¾Šæ¯å…¬æ–¤æ—¥ç³§ä¸­è‡³å°‘å«3500åœ‹éš›å–®ä½(IU)ä»¥ä¸Šã€‚è€Œ2007å¹´ç‰ˆçš„NRCå‰‡å°‡å±±ç¾Šç¶­ç”Ÿç´ Aéœ€è¦é‡æ”¹ä»¥è¦–ç¶²é†‡ç•¶é‡(retinol equivalent,RE)ç‚ºä¼°ç®—å–®ä½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ D:ç¶­ç”Ÿç´ Dåœ¨å±±ç¾Šçš„éˆ£ã€ç£·å¸æ”¶ä¸Šæ‰®æ¼”è‘—éå¸¸é‡è¦çš„è§’è‰²,ç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´å±±ç¾Šéˆ£ç£·å¸æ”¶ä¸å¹³è¡¡ã€å½±éŸ¿å±±ç¾Šéª¨éª¼ç™¼è‚²ã€‚æ”¾ç‰§ç¾Šéš»æœƒå› å¤ªé™½ç…§å°„å…¶çš®è†šè€Œè‡ªè¡Œåˆæˆç¶­ç”Ÿç´ D,æ•…ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dã€‚ç„¶è€Œåœˆé£¼ç¾Šéš»å‰‡å› ç„¡æ³•é•·æ™‚é–“æ¥è§¸å¤ªé™½å…‰èˆ‡ (source=read.txt) score=0.8932\n",
      "- ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹”æ­£å¸¸ï¼šç•¶ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå‰‡èº«é«”ä¹‹å‘¼å¸é“ã€è…¸é“ã€ç”Ÿæ®–é“ã€å°¿é“èˆ‡çœ¼ç›è¡¨çš®ç´°èƒæœƒè§’è³ªåŒ–(epithelial keratinization)ã€‚3.å¼·æŠµæŠ—åŠ›ï¼šå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå®¹æ˜“ä½¿å±±ç¾Šæ„ŸæŸ“ç–¾ç—…åŠå¯„ç”ŸèŸ²ã€ç¹æ®–æ€§èƒ½ä¸ä½³ã€‚\n",
      "\n",
      "é€ æˆå±±ç¾Šç¶­ç”Ÿç´ Aç¼ºä¹ä¹‹åŸå› æœ‰äºŒï¼Œä¸»è¦ç‚ºé£¼ç³§ç¶­ç”Ÿç´ Aå«é‡ä¸è¶³ï¼Œå°¤å…¶åœ¨ä¹¾æ—±å­£ç¯€æˆ–å†¬å­£ï¼Œå› é’è‰ä¾›æ‡‰å—é™ï¼Œæˆ–é¤µé£¼å±±ç¾Šä¹‹ä¹¾è‰å“è³ªä¸è‰¯ï¼Œä¾‹å¦‚ä¹¾è‰è²¯å­˜æœŸé–“å¤ªé•·åŠä¹¾è‰è²¯å­˜ä¸ç•¶ï¼Œå› æ—¥æ›¬èˆ‡é›¨æ·‹ã€‚è£½é€ ä¹¾è‰æ™‚ï¼Œå› å¤©å€™ä¸è‰¯ï¼Œä¹¾è‰ (source=read.txt) score=0.8606\n",
      "- è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Šè€Œå½¢æˆå¤œç›²ç—‡ã€‚å› æ­¤åœ¨è‚‰ç¾Šæ—¥ç³§ä¸­ï¼Œæ¯å…¬æ–¤ç²¾æ–™è‡³å°‘è¦å«3500åœ‹éš›å–®ä½(IU)ä¹‹ç¶­ç”Ÿç´ Aã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Dï¼šåŒæ¨£æ˜¯è„‚æº¶æ€§ç¶­ç”Ÿç´ ï¼Œå…¶ä¸»è¦æ˜¯å¹«åŠ©å‹•ç‰©æœ‰æ•ˆç‡çš„å¸æ”¶åŠåˆ©ç”¨éˆ£èˆ‡ç£·ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´è‚‰ç¾Šéˆ£èˆ‡ç£·çš„å¸æ”¶ä¸å¹³è¡¡ï¼Œå½±éŸ¿è‚‰ç¾Šéª¨éª¼ç™¼è‚²ï¼Œä»”ç¾ŠåŠæˆç¾Šå‡æœƒç”¢ç”Ÿè»Ÿéª¨ç—‡åŠéª¨è³ªç–é¬†ç—‡ã€‚ç”±æ–¼é™½å…‰ä¸­çš„ç´«å¤–ç·šç…§å°„å‹•ç‰©çš®è†šï¼Œæœƒä½¿å‹•ç‰©çš®è†šä¸­ä¹‹7dehydrocholesterolè½‰åŒ–æˆç¶­ç”Ÿç´ D3ã€‚å› æ­¤æ”¾ç‰§ç¾Šéš»ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dï¼Œç„¶è€Œåœˆé£¼çš„ç¾Š (source=read.txt) score=0.8489\n",
      "- ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ä¹‹ä¸‹ç—¢ã€åŠ‡æ¸´(polydipsia)ã€å¤šå°¿ã€å‘¼å¸å›°é›£èˆ‡ç›´è…¸æº«åº¦ä½ä¸‹ã€‚ç¡’éé‡ä¹Ÿæœƒå¼•èµ·è¹„ä¹‹ç–¼ç—›åŠè„«è½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aï¼šå±±ç¾Šä¹‹èŠ»æ–™ä¸å«æœ‰ç¶­ç”Ÿç´ Aï¼Œä¸éå«æœ‰ç¶­ç”Ÿç´ Aä¹‹å…ˆé©…ç‰©ï¼Œå³èƒ¡è˜¿è””ç´ ã€‚å¤§ç´„1gçš„B-èƒ¡è˜¿è””ç´ ç›¸ç•¶æ–¼400IUçš„ç¶­ç”Ÿç´ Aã€‚å‹•ç‰©å¸æ”¶ç¶­ç”Ÿç´ Aä¹‹ä¸»è¦éƒ¨ä½åœ¨å°è…¸å‰æ®µï¼Œèº«é«”è²¯è—ç¶­ç”Ÿç´ Aä¹‹å™¨å®˜ç‚ºè‚è‡Ÿï¼Œç´„ä½”75-90%ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsi (source=read.txt) score=0.8323\n",
      "- ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´ Aç¼ºä¹ã€‚2.ç„¡æ³•æ”¾ç‰§é£¼é¤Šå±±ç¾Šè€…éœ€æä¾›é’è‰çµ¦é£¼é¤Šçš„å±±ç¾Šã€‚3.æ³¨æ„åœ¨é£¼æ–™å…§æ·»åŠ ç¶­ç”Ÿç´ Aã€‚4.æ¯ç¾Šæ‡·å­•æœŸè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚5.å‡ºç”Ÿä»”ç¾Šè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚6.å¥åº·çš„æ¯ç¾Šæ‰èƒ½ç”Ÿç”¢å„ªè³ªçš„åˆä¹³ã€‚\n",
      "\n",
      "å±±ç¾Šé¼“è„¹(bloat)ï¼šå±±ç¾Šé¼“è„¹é€šå¸¸æ˜¯ç”±å¤§é‡æ¡é£Ÿè±å¯Œçš„æ½®æ¿•æ–°é®®è‰æ–™æˆ–ç©€é¡å¼•èµ·çš„,å¦‚ä¸‰è‘‰è‰,è‹œè“¿æˆ–è±†ç§‘ç‰§è‰ã€‚é€™äº›æ½®æ¿•æ–°é®®è‰æ–™åœ¨ç˜¤èƒƒä¸­å½¢æˆå¾®å°çš„æ°£æ³¡,å°è‡´å±±ç¾Šå™¯æ°£ç„¡æ³•æ’å‡ºã€‚ç˜¤èƒƒéš¨è‘—æ³¡æ²«è†¨è„¹å£“è¿«æ©«éš”è†œ,é€ æˆå±±ç¾Šå¯èƒ½å› å‘¼å¸æˆ–å¾ªç’°è¡°ç«­è€Œå¾ˆ (source=read.txt) score=0.8274\n",
      "ğŸš€ LLM throughput: N/A (Ollama æœªæä¾› eval çµ±è¨ˆ)\n",
      "ğŸŸ¨ æª¢ç´¢çµæœç‚ºç©ºï¼Œæ”¹ç”¨å‘é‡ç´¢å¼• fallbackï¼š\n",
      "- ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„NRC,å»ºè­°ç¶­ç”Ÿç´ Aåœ¨å±±ç¾Šæ¯å…¬æ–¤æ—¥ç³§ä¸­è‡³å°‘å«3500åœ‹éš›å–®ä½(IU)ä»¥ä¸Šã€‚è€Œ2007å¹´ç‰ˆçš„NRCå‰‡å°‡å±±ç¾Šç¶­ç”Ÿç´ Aéœ€è¦é‡æ”¹ä»¥è¦–ç¶²é†‡ç•¶é‡(retinol equivalent,RE)ç‚ºä¼°ç®—å–®ä½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ D:ç¶­ç”Ÿç´ Dåœ¨å±±ç¾Šçš„éˆ£ã€ç£·å¸æ”¶ä¸Šæ‰®æ¼”è‘—éå¸¸é‡è¦çš„è§’è‰²,ç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´å±±ç¾Šéˆ£ç£·å¸æ”¶ä¸å¹³è¡¡ã€å½±éŸ¿å±±ç¾Šéª¨éª¼ç™¼è‚²ã€‚æ”¾ç‰§ç¾Šéš»æœƒå› å¤ªé™½ç…§å°„å…¶çš®è†šè€Œè‡ªè¡Œåˆæˆç¶­ç”Ÿç´ D,æ•…ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dã€‚ç„¶è€Œåœˆé£¼ç¾Šéš»å‰‡å› ç„¡æ³•é•·æ™‚é–“æ¥è§¸å¤ªé™½å…‰èˆ‡ (source=read.txt) score=0.8932\n",
      "- ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹”æ­£å¸¸ï¼šç•¶ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå‰‡èº«é«”ä¹‹å‘¼å¸é“ã€è…¸é“ã€ç”Ÿæ®–é“ã€å°¿é“èˆ‡çœ¼ç›è¡¨çš®ç´°èƒæœƒè§’è³ªåŒ–(epithelial keratinization)ã€‚3.å¼·æŠµæŠ—åŠ›ï¼šå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå®¹æ˜“ä½¿å±±ç¾Šæ„ŸæŸ“ç–¾ç—…åŠå¯„ç”ŸèŸ²ã€ç¹æ®–æ€§èƒ½ä¸ä½³ã€‚\n",
      "\n",
      "é€ æˆå±±ç¾Šç¶­ç”Ÿç´ Aç¼ºä¹ä¹‹åŸå› æœ‰äºŒï¼Œä¸»è¦ç‚ºé£¼ç³§ç¶­ç”Ÿç´ Aå«é‡ä¸è¶³ï¼Œå°¤å…¶åœ¨ä¹¾æ—±å­£ç¯€æˆ–å†¬å­£ï¼Œå› é’è‰ä¾›æ‡‰å—é™ï¼Œæˆ–é¤µé£¼å±±ç¾Šä¹‹ä¹¾è‰å“è³ªä¸è‰¯ï¼Œä¾‹å¦‚ä¹¾è‰è²¯å­˜æœŸé–“å¤ªé•·åŠä¹¾è‰è²¯å­˜ä¸ç•¶ï¼Œå› æ—¥æ›¬èˆ‡é›¨æ·‹ã€‚è£½é€ ä¹¾è‰æ™‚ï¼Œå› å¤©å€™ä¸è‰¯ï¼Œä¹¾è‰ (source=read.txt) score=0.8606\n",
      "- è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Šè€Œå½¢æˆå¤œç›²ç—‡ã€‚å› æ­¤åœ¨è‚‰ç¾Šæ—¥ç³§ä¸­ï¼Œæ¯å…¬æ–¤ç²¾æ–™è‡³å°‘è¦å«3500åœ‹éš›å–®ä½(IU)ä¹‹ç¶­ç”Ÿç´ Aã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Dï¼šåŒæ¨£æ˜¯è„‚æº¶æ€§ç¶­ç”Ÿç´ ï¼Œå…¶ä¸»è¦æ˜¯å¹«åŠ©å‹•ç‰©æœ‰æ•ˆç‡çš„å¸æ”¶åŠåˆ©ç”¨éˆ£èˆ‡ç£·ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´è‚‰ç¾Šéˆ£èˆ‡ç£·çš„å¸æ”¶ä¸å¹³è¡¡ï¼Œå½±éŸ¿è‚‰ç¾Šéª¨éª¼ç™¼è‚²ï¼Œä»”ç¾ŠåŠæˆç¾Šå‡æœƒç”¢ç”Ÿè»Ÿéª¨ç—‡åŠéª¨è³ªç–é¬†ç—‡ã€‚ç”±æ–¼é™½å…‰ä¸­çš„ç´«å¤–ç·šç…§å°„å‹•ç‰©çš®è†šï¼Œæœƒä½¿å‹•ç‰©çš®è†šä¸­ä¹‹7dehydrocholesterolè½‰åŒ–æˆç¶­ç”Ÿç´ D3ã€‚å› æ­¤æ”¾ç‰§ç¾Šéš»ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dï¼Œç„¶è€Œåœˆé£¼çš„ç¾Š (source=read.txt) score=0.8489\n",
      "- ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ä¹‹ä¸‹ç—¢ã€åŠ‡æ¸´(polydipsia)ã€å¤šå°¿ã€å‘¼å¸å›°é›£èˆ‡ç›´è…¸æº«åº¦ä½ä¸‹ã€‚ç¡’éé‡ä¹Ÿæœƒå¼•èµ·è¹„ä¹‹ç–¼ç—›åŠè„«è½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aï¼šå±±ç¾Šä¹‹èŠ»æ–™ä¸å«æœ‰ç¶­ç”Ÿç´ Aï¼Œä¸éå«æœ‰ç¶­ç”Ÿç´ Aä¹‹å…ˆé©…ç‰©ï¼Œå³èƒ¡è˜¿è””ç´ ã€‚å¤§ç´„1gçš„B-èƒ¡è˜¿è””ç´ ç›¸ç•¶æ–¼400IUçš„ç¶­ç”Ÿç´ Aã€‚å‹•ç‰©å¸æ”¶ç¶­ç”Ÿç´ Aä¹‹ä¸»è¦éƒ¨ä½åœ¨å°è…¸å‰æ®µï¼Œèº«é«”è²¯è—ç¶­ç”Ÿç´ Aä¹‹å™¨å®˜ç‚ºè‚è‡Ÿï¼Œç´„ä½”75-90%ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsi (source=read.txt) score=0.8323\n",
      "- ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´ Aç¼ºä¹ã€‚2.ç„¡æ³•æ”¾ç‰§é£¼é¤Šå±±ç¾Šè€…éœ€æä¾›é’è‰çµ¦é£¼é¤Šçš„å±±ç¾Šã€‚3.æ³¨æ„åœ¨é£¼æ–™å…§æ·»åŠ ç¶­ç”Ÿç´ Aã€‚4.æ¯ç¾Šæ‡·å­•æœŸè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚5.å‡ºç”Ÿä»”ç¾Šè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚6.å¥åº·çš„æ¯ç¾Šæ‰èƒ½ç”Ÿç”¢å„ªè³ªçš„åˆä¹³ã€‚\n",
      "\n",
      "å±±ç¾Šé¼“è„¹(bloat)ï¼šå±±ç¾Šé¼“è„¹é€šå¸¸æ˜¯ç”±å¤§é‡æ¡é£Ÿè±å¯Œçš„æ½®æ¿•æ–°é®®è‰æ–™æˆ–ç©€é¡å¼•èµ·çš„,å¦‚ä¸‰è‘‰è‰,è‹œè“¿æˆ–è±†ç§‘ç‰§è‰ã€‚é€™äº›æ½®æ¿•æ–°é®®è‰æ–™åœ¨ç˜¤èƒƒä¸­å½¢æˆå¾®å°çš„æ°£æ³¡,å°è‡´å±±ç¾Šå™¯æ°£ç„¡æ³•æ’å‡ºã€‚ç˜¤èƒƒéš¨è‘—æ³¡æ²«è†¨è„¹å£“è¿«æ©«éš”è†œ,é€ æˆå±±ç¾Šå¯èƒ½å› å‘¼å¸æˆ–å¾ªç’°è¡°ç«­è€Œå¾ˆ (source=read.txt) score=0.8274\n"
     ]
    }
   ],
   "source": [
    "# ä¸€æ¬¡åŸ·è¡Œå®Œæ•´æµç¨‹ï¼šå»ºç«‹ Chunkã€å‘é‡ç´¢å¼•ã€å¯«å…¥å‘é‡ã€æŠ½å–ä¸‰å…ƒçµ„å…¥åœ–è­œã€å»ºç«‹æª¢ç´¢èˆ‡ QA\n",
    "# å»ºè­°åœ¨é‡å•Ÿ Kernel å¾Œï¼Œç›´æ¥åªåŸ·è¡Œæœ¬ Cellã€‚\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# neo4j-graphrag ç›¸é—œï¼ˆç¢ºä¿å¥—ä»¶å·²å®‰è£ï¼‰\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "import ollama\n",
    "\n",
    "# -----------------------------\n",
    "# åƒæ•¸è¨­å®š\n",
    "# -----------------------------\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "\n",
    "DATA_FOLDER = r\"C:\\\\Users\\\\aint\\\\Desktop\\\\LLM+KB\\\\neo4j-graphrag-python\\\\data\"\n",
    "EMBED_MODEL_REPO = \"GanymedeNil/text2vec-large-chinese\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "TOP_K = 5\n",
    "\n",
    "# -----------------------------\n",
    "# é€£ç·š Neo4j\n",
    "# -----------------------------\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "\n",
    "# -----------------------------\n",
    "# å·¥å…·å‡½å¼\n",
    "# -----------------------------\n",
    "def chunk_text(text: str, max_length: int = 300, overlap: int = 50) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_length\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_length - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def ensure_vector_index(driver, name: str, label: str, embedding_property: str, dimensions: int, similarity_fn: str = \"cosine\"):\n",
    "    with driver.session() as session:\n",
    "        existing = session.run(\"SHOW INDEXES\").data()\n",
    "        if any(idx.get(\"name\") == name for idx in existing):\n",
    "            print(f\"âœ… Vector index '{name}' å·²å­˜åœ¨\")\n",
    "            return\n",
    "        cypher = f\"\"\"\n",
    "        CREATE VECTOR INDEX {name}\n",
    "        FOR (n:{label}) ON (n.{embedding_property})\n",
    "        OPTIONS {{\n",
    "            indexConfig: {{\n",
    "                `vector.dimensions`: {dimensions},\n",
    "                `vector.similarity_function`: '{similarity_fn}'\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        session.run(cypher)\n",
    "        print(f\"ğŸš€ Vector index '{name}' å»ºç«‹å®Œæˆ (dim={dimensions}, sim={similarity_fn})\")\n",
    "\n",
    "\n",
    "def load_txt_files(folder: str) -> List[Dict[str, str]]:\n",
    "    docs = []\n",
    "    for i, fname in enumerate(sorted(os.listdir(folder))):\n",
    "        if fname.lower().endswith(\".txt\"):\n",
    "            path = os.path.join(folder, fname)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            chunks = chunk_text(text)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                docs.append({\n",
    "                    \"id\": f\"{i}_{j}\",\n",
    "                    \"text\": chunk,\n",
    "                    \"source\": fname,\n",
    "                })\n",
    "    return docs\n",
    "\n",
    "\n",
    "def ingest_documents(docs: List[Dict[str, str]], embedder: SentenceTransformerEmbeddings, driver) -> None:\n",
    "    with driver.session() as session:\n",
    "        for doc in docs:\n",
    "            emb = embedder.embed_query(doc[\"text\"])\n",
    "            if hasattr(emb, \"tolist\"):\n",
    "                emb = emb.tolist()\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (c:Chunk {id:$id})\n",
    "                SET c.text = $text,\n",
    "                    c.source = $source,\n",
    "                    c.embedding = $embedding\n",
    "                \"\"\",\n",
    "                id=doc[\"id\"],\n",
    "                text=doc[\"text\"],\n",
    "                source=doc[\"source\"],\n",
    "                embedding=emb,\n",
    "            )\n",
    "    print(f\"âœ… å·²å¯«å…¥ {len(docs)} å€‹ chunk åˆ° Neo4j\")\n",
    "\n",
    "\n",
    "# ---- ä¸‰å…ƒçµ„æŠ½å–èˆ‡å¯«å…¥ ----\n",
    "TRIPLE_PROMPT_TMPL = \"\"\"\n",
    "ä½ æ˜¯ä¸€å€‹è³‡è¨ŠæŠ½å–å™¨ã€‚è«‹å¾ä¸‹æ–¹æ–‡å­—ä¸­æŠ½å–çŸ¥è­˜ä¸‰å…ƒçµ„ï¼Œè¼¸å‡ºç´” JSON é™£åˆ—ï¼Œä¸è¦åŠ è§£èªªæ–‡å­—ã€‚\n",
    "æ¯å€‹å…ƒç´ åŒ…å« head, relation, tail ä¸‰å€‹æ¬„ä½ï¼Œä¾‹å¦‚ï¼š\n",
    "[{{\"head\":\"å±±ç¾Š\",\"relation\":\"ç¼ºä¹\",\"tail\":\"ç¶­ç”Ÿç´ A\"}}]\n",
    "\n",
    "æ–‡å­—ï¼š\n",
    "{chunk}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _coerce_triple_list(obj) -> List[Dict[str, str]]:\n",
    "    out = []\n",
    "    if isinstance(obj, list):\n",
    "        for it in obj:\n",
    "            if isinstance(it, dict):\n",
    "                h, r, t = it.get(\"head\"), it.get(\"relation\"), it.get(\"tail\")\n",
    "                if all(isinstance(x, str) and x.strip() for x in (h, r, t)):\n",
    "                    out.append({\"head\": h.strip(), \"relation\": r.strip(), \"tail\": t.strip()})\n",
    "            elif isinstance(it, (list, tuple)) and len(it) == 3:\n",
    "                h, r, t = it\n",
    "                if all(isinstance(x, str) and x.strip() for x in (h, r, t)):\n",
    "                    out.append({\"head\": str(h).strip(), \"relation\": str(r).strip(), \"tail\": str(t).strip()})\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_triples_from_text(raw: str) -> List[Dict[str, str]]:\n",
    "    try:\n",
    "        return _coerce_triple_list(json.loads(raw))\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\[[\\s\\S]*\\]\", raw)\n",
    "        if m:\n",
    "            try:\n",
    "                return _coerce_triple_list(json.loads(m.group(0)))\n",
    "            except Exception:\n",
    "                return []\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_triples_from_chunk(chunk_text: str, model: str = OLLAMA_MODEL) -> List[Dict[str, str]]:\n",
    "    prompt = TRIPLE_PROMPT_TMPL.format(chunk=chunk_text)\n",
    "    resp = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0}\n",
    "    )\n",
    "    content = resp.get(\"message\", {}).get(\"content\", \"\")\n",
    "    return parse_triples_from_text(content)\n",
    "\n",
    "\n",
    "def ingest_triples_with_provenance(triples: List[Dict[str, str]], chunk_id: str, driver) -> int:\n",
    "    if not triples:\n",
    "        return 0\n",
    "    with driver.session() as session:\n",
    "        for t in triples:\n",
    "            head, rel, tail = t[\"head\"], t[\"relation\"], t[\"tail\"]\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (h:Entity {name:$head})\n",
    "                MERGE (t:Entity {name:$tail})\n",
    "                MERGE (h)-[r:RELATION {type:$rel}]->(t)\n",
    "                WITH h, t, r, $cid AS cid\n",
    "                SET r.chunks = CASE\n",
    "                  WHEN r.chunks IS NULL THEN [cid]\n",
    "                  WHEN NOT cid IN r.chunks THEN r.chunks + cid\n",
    "                  ELSE r.chunks\n",
    "                END\n",
    "                WITH h, t, r, cid\n",
    "                MATCH (c:Chunk {id:cid})\n",
    "                MERGE (c)-[:MENTIONS]->(h)\n",
    "                MERGE (c)-[:MENTIONS]->(t)\n",
    "                \"\"\",\n",
    "                head=head,\n",
    "                rel=rel,\n",
    "                tail=tail,\n",
    "                cid=chunk_id,\n",
    "            )\n",
    "    return len(triples)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) æº–å‚™ Embedding æ¨¡å‹èˆ‡ç´¢å¼•\n",
    "# -----------------------------\n",
    "if not os.path.isdir(EMBED_MODEL_DIR) or not os.listdir(EMBED_MODEL_DIR):\n",
    "    print(\"â¬ ä¸‹è¼‰ä¸­æ–‡å‘é‡æ¨¡å‹â€¦â€¦\")\n",
    "    snapshot_download(repo_id=EMBED_MODEL_REPO, local_dir=EMBED_MODEL_DIR)\n",
    "else:\n",
    "    print(\"âœ… å·²å­˜åœ¨æœ¬åœ°å‘é‡æ¨¡å‹\")\n",
    "\n",
    "embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "embedding_dim = embedder.model.get_sentence_embedding_dimension()\n",
    "\n",
    "ensure_vector_index(driver, name=\"chunk_embeddings\", label=\"Chunk\", embedding_property=\"embedding\", dimensions=embedding_dim, similarity_fn=\"cosine\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) è¼‰å…¥ TXT ä¸¦åˆ‡ Chunkã€å¯«å…¥å‘é‡\n",
    "# -----------------------------\n",
    "docs = load_txt_files(DATA_FOLDER)\n",
    "print(f\"ğŸ“„ è¼‰å…¥æª”æ¡ˆå…± {len(docs)} å€‹ chunk\")\n",
    "ingest_documents(docs, embedder, driver)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) å¾æ¯å€‹ Chunk æŠ½å–ä¸‰å…ƒçµ„ä¸¦å¯«å…¥çŸ¥è­˜åœ–è­œ\n",
    "# -----------------------------\n",
    "rel_count = 0\n",
    "for doc in docs:\n",
    "    triples = extract_triples_from_chunk(doc[\"text\"], model=OLLAMA_MODEL)\n",
    "    rel_count += ingest_triples_with_provenance(triples, doc[\"id\"], driver)\n",
    "print(f\"âœ… å·²å¯«å…¥ {rel_count} æ¢ (Entity)-[RELATION]->(Entity) é—œä¿‚ï¼Œä¸¦å»ºç«‹ä¾†æº MENTIONS èˆ‡ r.chunks\")\n",
    "\n",
    "# ç°¡å–®æª¢è¦–å¹¾æ¢ä¸‰å…ƒçµ„\n",
    "with driver.session() as session:\n",
    "    rows = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        RETURN h.name AS head, r.type AS relation, t.name AS tail, r.chunks AS chunks\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    ").data()\n",
    "    for row in rows:\n",
    "        sample = row.get(\"chunks\")\n",
    "        if isinstance(sample, list) and sample:\n",
    "            sample = sample[0]\n",
    "        print(f\"ğŸ” ä¸‰å…ƒçµ„: ({row['head']})-[:{row['relation']}]->({row['tail']})  <- sample chunk: {sample}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) å»ºç«‹æª¢ç´¢èˆ‡ QAï¼ˆVectorRetriever + Ollama LLMï¼‰\n",
    "# -----------------------------\n",
    "vector_retriever = VectorRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"chunk_embeddings\",\n",
    "    embedder=embedder,\n",
    "    return_properties=[\"text\", \"source\"],\n",
    ")\n",
    "llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "rag = GraphRAG(llm=llm, retriever=vector_retriever)\n",
    "\n",
    "\n",
    "# ç›´æ¥å‘é‡æŸ¥è©¢ fallbackï¼ˆç•¶ rag è¿”å›ä¸­æ²’æœ‰ retriever_result æ™‚ï¼‰\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings as _STE\n",
    "\n",
    "def vector_query_fallback(query_text: str, top_k: int = TOP_K):\n",
    "    q_emb = embedder.embed_query(query_text)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    with driver.session() as session:\n",
    "        rows = session.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes('chunk_embeddings', $k, $qemb)\n",
    "            YIELD node, score\n",
    "            RETURN node.text AS text, node.source AS source, score\n",
    "            LIMIT $k\n",
    "            \"\"\",\n",
    "            k=top_k,\n",
    "            qemb=q_emb,\n",
    "        ).data()\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _measure_tok_s(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"ä½¿ç”¨ Ollama çš„ eval çµ±è¨ˆä¼°ç®— tok/sï¼›è‹¥ä¸å¯å¾—å‰‡å˜—è©¦ä»¥ eval_count/eval_duration ä¼°ç®—ï¼Œä»ä¸å¯å¾—å‰‡å›å‚³ -1ã€‚\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        # 1) ç›´æ¥å– tokens_per_secondï¼ˆè‹¥å­˜åœ¨ï¼‰\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or eval_info.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        # 2) ä»¥ eval_count / eval_duration(ç§’) æ¨ä¼°ï¼ˆOllama ä¸€èˆ¬å›å‚³ nsï¼‰\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "def qa(query: str, top_k: int = TOP_K):\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag.search(query_text=query, retriever_config={\"top_k\": top_k})\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "    print(\"\\nâ“ å•é¡Œ:\", query)\n",
    "    print(\"ğŸŸ© å›ç­”:\", getattr(resp, \"answer\", None))\n",
    "    print(f\"â±ï¸ Inference latency: {latency_ms:.1f} ms\")\n",
    "    # å˜—è©¦ç”¨æœ€ç°¡ prompt ä¼°è¨ˆ tok/sï¼ˆä¸å½±éŸ¿å¯¦éš›å›ç­”ï¼‰\n",
    "    ctx_nodes = getattr(resp, \"retriever_result\", None) or []\n",
    "    ctx_text = \"\\n\\n\".join([n.get(\"text\", \"\") for n in ctx_nodes[:min(len(ctx_nodes), top_k)]])\n",
    "    probe_prompt = f\"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç°¡çŸ­å›ç­”ï¼š\\n\\n{ctx_text}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "    tps = _measure_tok_s(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"ğŸš€ LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"ğŸš€ LLM throughput: N/A (Ollama æœªæä¾› eval çµ±è¨ˆ)\")\n",
    "    nodes = getattr(resp, \"retriever_result\", None)\n",
    "    if nodes:\n",
    "        print(\"ğŸŸ¦ ç›¸é—œå…§å®¹ (å‰å¹¾ç­†):\")\n",
    "        for node in nodes[:min(len(nodes), top_k)]:\n",
    "            print(\"-\", node.get(\"text\", \"\"), f\"(source={node.get('source')})\")\n",
    "    else:\n",
    "        print(\"ğŸŸ¨ æª¢ç´¢çµæœç‚ºç©ºï¼Œæ”¹ç”¨å‘é‡ç´¢å¼• fallbackï¼š\")\n",
    "        rows = vector_query_fallback(query, top_k)\n",
    "        if rows:\n",
    "            for r in rows:\n",
    "                print(\"-\", r.get(\"text\", \"\"), f\"(source={r.get('source')}) score={r.get('score'):.4f}\")\n",
    "        else:\n",
    "            print(\"ï¼ˆæŸ¥ç„¡ç›¸é—œå…§å®¹ï¼‰\")\n",
    "\n",
    "# Demo å•é¡Œï¼ˆå¯è‡ªè¡Œé–‹é—œï¼Œé¿å…é‡è¤‡è¼¸å‡ºï¼‰\n",
    "RUN_DEMOS = True\n",
    "if RUN_DEMOS:\n",
    "    qa(\"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œ\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec96f55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m RUN_DEMOS = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RUN_DEMOS:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mqa\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33må±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿ\u001b[39m\u001b[33m\"\u001b[39m, top_k=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "RUN_DEMOS = True\n",
    "if RUN_DEMOS:\n",
    "    qa(\"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿ\", top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415bb64",
   "metadata": {},
   "source": [
    "> æé†’ï¼šç‚ºé¿å…é‡è¤‡è¼¸å‡ºèˆ‡é‡è¤‡å¯«å…¥ï¼Œå»ºè­°å…ˆ Kernel â†’ Restartï¼Œç„¶å¾ŒåªåŸ·è¡Œæœ€ä¸‹æ–¹ã€Œä¸€æ¬¡åŸ·è¡Œå®Œæ•´æµç¨‹ã€çš„ Cellã€‚ä¸Šé¢çš„ç¯„ä¾‹ç¨‹å¼å·²æ”¹ç‚ºéœ€æ‰‹å‹•æŠŠ `RUN_DEMOS = True` æ‰æœƒåŸ·è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29bdf63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\n",
      "å›ç­”: According to the provided context, å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚\n",
      "Inference latency: 4151.0 ms\n",
      "LLM throughput: 85.02 tok/s (Ollama eval)\n",
      "æª¢ç´¢çµæœç‚ºç©ºï¼Œæ”¹ç”¨å‘é‡ç´¢å¼• fallbackï¼š\n",
      "- ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„NRC,å»ºè­°ç¶­ç”Ÿç´ Aåœ¨å±±ç¾Šæ¯å…¬æ–¤æ—¥ç³§ä¸­è‡³å°‘å«3500åœ‹éš›å–®ä½(IU)ä»¥ä¸Šã€‚è€Œ2007å¹´ç‰ˆçš„NRCå‰‡å°‡å±±ç¾Šç¶­ç”Ÿç´ Aéœ€è¦é‡æ”¹ä»¥è¦–ç¶²é†‡ç•¶é‡(retinol equivalent,RE)ç‚ºä¼°ç®—å–®ä½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ D:ç¶­ç”Ÿç´ Dåœ¨å±±ç¾Šçš„éˆ£ã€ç£·å¸æ”¶ä¸Šæ‰®æ¼”è‘—éå¸¸é‡è¦çš„è§’è‰²,ç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´å±±ç¾Šéˆ£ç£·å¸æ”¶ä¸å¹³è¡¡ã€å½±éŸ¿å±±ç¾Šéª¨éª¼ç™¼è‚²ã€‚æ”¾ç‰§ç¾Šéš»æœƒå› å¤ªé™½ç…§å°„å…¶çš®è†šè€Œè‡ªè¡Œåˆæˆç¶­ç”Ÿç´ D,æ•…ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dã€‚ç„¶è€Œåœˆé£¼ç¾Šéš»å‰‡å› ç„¡æ³•é•·æ™‚é–“æ¥è§¸å¤ªé™½å…‰èˆ‡ (source=read.txt) score=0.8935\n",
      "- ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹”æ­£å¸¸ï¼šç•¶ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå‰‡èº«é«”ä¹‹å‘¼å¸é“ã€è…¸é“ã€ç”Ÿæ®–é“ã€å°¿é“èˆ‡çœ¼ç›è¡¨çš®ç´°èƒæœƒè§’è³ªåŒ–(epithelial keratinization)ã€‚3.å¼·æŠµæŠ—åŠ›ï¼šå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå®¹æ˜“ä½¿å±±ç¾Šæ„ŸæŸ“ç–¾ç—…åŠå¯„ç”ŸèŸ²ã€ç¹æ®–æ€§èƒ½ä¸ä½³ã€‚\n",
      "\n",
      "é€ æˆå±±ç¾Šç¶­ç”Ÿç´ Aç¼ºä¹ä¹‹åŸå› æœ‰äºŒï¼Œä¸»è¦ç‚ºé£¼ç³§ç¶­ç”Ÿç´ Aå«é‡ä¸è¶³ï¼Œå°¤å…¶åœ¨ä¹¾æ—±å­£ç¯€æˆ–å†¬å­£ï¼Œå› é’è‰ä¾›æ‡‰å—é™ï¼Œæˆ–é¤µé£¼å±±ç¾Šä¹‹ä¹¾è‰å“è³ªä¸è‰¯ï¼Œä¾‹å¦‚ä¹¾è‰è²¯å­˜æœŸé–“å¤ªé•·åŠä¹¾è‰è²¯å­˜ä¸ç•¶ï¼Œå› æ—¥æ›¬èˆ‡é›¨æ·‹ã€‚è£½é€ ä¹¾è‰æ™‚ï¼Œå› å¤©å€™ä¸è‰¯ï¼Œä¹¾è‰ (source=read.txt) score=0.8598\n",
      "- è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Šè€Œå½¢æˆå¤œç›²ç—‡ã€‚å› æ­¤åœ¨è‚‰ç¾Šæ—¥ç³§ä¸­ï¼Œæ¯å…¬æ–¤ç²¾æ–™è‡³å°‘è¦å«3500åœ‹éš›å–®ä½(IU)ä¹‹ç¶­ç”Ÿç´ Aã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Dï¼šåŒæ¨£æ˜¯è„‚æº¶æ€§ç¶­ç”Ÿç´ ï¼Œå…¶ä¸»è¦æ˜¯å¹«åŠ©å‹•ç‰©æœ‰æ•ˆç‡çš„å¸æ”¶åŠåˆ©ç”¨éˆ£èˆ‡ç£·ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´è‚‰ç¾Šéˆ£èˆ‡ç£·çš„å¸æ”¶ä¸å¹³è¡¡ï¼Œå½±éŸ¿è‚‰ç¾Šéª¨éª¼ç™¼è‚²ï¼Œä»”ç¾ŠåŠæˆç¾Šå‡æœƒç”¢ç”Ÿè»Ÿéª¨ç—‡åŠéª¨è³ªç–é¬†ç—‡ã€‚ç”±æ–¼é™½å…‰ä¸­çš„ç´«å¤–ç·šç…§å°„å‹•ç‰©çš®è†šï¼Œæœƒä½¿å‹•ç‰©çš®è†šä¸­ä¹‹7dehydrocholesterolè½‰åŒ–æˆç¶­ç”Ÿç´ D3ã€‚å› æ­¤æ”¾ç‰§ç¾Šéš»ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dï¼Œç„¶è€Œåœˆé£¼çš„ç¾Š (source=read.txt) score=0.8475\n",
      "- ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ä¹‹ä¸‹ç—¢ã€åŠ‡æ¸´(polydipsia)ã€å¤šå°¿ã€å‘¼å¸å›°é›£èˆ‡ç›´è…¸æº«åº¦ä½ä¸‹ã€‚ç¡’éé‡ä¹Ÿæœƒå¼•èµ·è¹„ä¹‹ç–¼ç—›åŠè„«è½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aï¼šå±±ç¾Šä¹‹èŠ»æ–™ä¸å«æœ‰ç¶­ç”Ÿç´ Aï¼Œä¸éå«æœ‰ç¶­ç”Ÿç´ Aä¹‹å…ˆé©…ç‰©ï¼Œå³èƒ¡è˜¿è””ç´ ã€‚å¤§ç´„1gçš„B-èƒ¡è˜¿è””ç´ ç›¸ç•¶æ–¼400IUçš„ç¶­ç”Ÿç´ Aã€‚å‹•ç‰©å¸æ”¶ç¶­ç”Ÿç´ Aä¹‹ä¸»è¦éƒ¨ä½åœ¨å°è…¸å‰æ®µï¼Œèº«é«”è²¯è—ç¶­ç”Ÿç´ Aä¹‹å™¨å®˜ç‚ºè‚è‡Ÿï¼Œç´„ä½”75-90%ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsi (source=read.txt) score=0.8347\n",
      "- ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´ Aç¼ºä¹ã€‚2.ç„¡æ³•æ”¾ç‰§é£¼é¤Šå±±ç¾Šè€…éœ€æä¾›é’è‰çµ¦é£¼é¤Šçš„å±±ç¾Šã€‚3.æ³¨æ„åœ¨é£¼æ–™å…§æ·»åŠ ç¶­ç”Ÿç´ Aã€‚4.æ¯ç¾Šæ‡·å­•æœŸè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚5.å‡ºç”Ÿä»”ç¾Šè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚6.å¥åº·çš„æ¯ç¾Šæ‰èƒ½ç”Ÿç”¢å„ªè³ªçš„åˆä¹³ã€‚\n",
      "\n",
      "å±±ç¾Šé¼“è„¹(bloat)ï¼šå±±ç¾Šé¼“è„¹é€šå¸¸æ˜¯ç”±å¤§é‡æ¡é£Ÿè±å¯Œçš„æ½®æ¿•æ–°é®®è‰æ–™æˆ–ç©€é¡å¼•èµ·çš„,å¦‚ä¸‰è‘‰è‰,è‹œè“¿æˆ–è±†ç§‘ç‰§è‰ã€‚é€™äº›æ½®æ¿•æ–°é®®è‰æ–™åœ¨ç˜¤èƒƒä¸­å½¢æˆå¾®å°çš„æ°£æ³¡,å°è‡´å±±ç¾Šå™¯æ°£ç„¡æ³•æ’å‡ºã€‚ç˜¤èƒƒéš¨è‘—æ³¡æ²«è†¨è„¹å£“è¿«æ©«éš”è†œ,é€ æˆå±±ç¾Šå¯èƒ½å› å‘¼å¸æˆ–å¾ªç’°è¡°ç«­è€Œå¾ˆ (source=read.txt) score=0.8322\n"
     ]
    }
   ],
   "source": [
    "# âš¡ QA-onlyï¼šåªå»ºç«‹æª¢ç´¢èˆ‡QAï¼Œä¸é‡æ–°å¯«å…¥æˆ–æŠ½å–\n",
    "import os\n",
    "import time\n",
    "import ollama\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "\n",
    "# èˆ‡å®Œæ•´æµç¨‹ä¿æŒä¸€è‡´çš„è¨­å®š\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "INDEX_NAME = \"chunk_embeddings\"\n",
    "TOP_K = 5\n",
    "\n",
    "# é€£ç·š Neo4j\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "\n",
    "# è¼‰å…¥æœ¬åœ° Embedding æ¨¡å‹ï¼ˆä¸ä¸‹è¼‰ã€ä¸å¯«å…¥ï¼‰\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# å»ºç«‹æª¢ç´¢èˆ‡ LLMï¼ˆä¸é‡å»ºç´¢å¼•ï¼‰\n",
    "_vector_retriever = VectorRetriever(\n",
    "    driver=_driver,\n",
    "    index_name=INDEX_NAME,\n",
    "    embedder=_embedder,\n",
    "    return_properties=[\"text\", \"source\"],\n",
    ")\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag = GraphRAG(llm=_llm, retriever=_vector_retriever)\n",
    "\n",
    "# å‘é‡æŸ¥è©¢ fallbackï¼ˆè‹¥ retriever_result ç‚ºç©ºï¼‰\n",
    "def _vector_query_fallback(query_text: str, top_k: int = TOP_K):\n",
    "    q_emb = _embedder.embed_query(query_text)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    with _driver.session() as session:\n",
    "        rows = session.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes($index, $k, $qemb)\n",
    "            YIELD node, score\n",
    "            RETURN node.text AS text, node.source AS source, score\n",
    "            LIMIT $k\n",
    "            \"\"\",\n",
    "            index=INDEX_NAME,\n",
    "            k=top_k,\n",
    "            qemb=q_emb,\n",
    "        ).data()\n",
    "    return rows\n",
    "\n",
    "def _measure_tok_s(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        # 1) ç›´æ¥å¾ eval æ‹¿ tokens_per_second\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or eval_info.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        # 2) ä»¥ eval_count / eval_duration æ¨ä¼°ï¼ˆeval_duration é€šå¸¸ç‚ºå¥ˆç§’ï¼‰\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "# å°è£æˆå¯é‡ç”¨çš„ QA å‡½å¼\n",
    "def qa_only(query: str, top_k: int = TOP_K, temperature: float | None = None):\n",
    "    # æ ¹æ“š temperature å‹•æ…‹å»ºç«‹ LLM èˆ‡ RAGï¼ˆä¸å½±éŸ¿é è¨­ _ragï¼‰\n",
    "    rag_to_use = _rag\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_temp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(llm=_llm_temp, retriever=_vector_retriever)\n",
    "        except Exception as e:\n",
    "            print(f\"[QA-only] å»ºç«‹å¸¶ temperature çš„ LLM å¤±æ•—ï¼Œæ”¹ç”¨é è¨­ï¼š{e}\")\n",
    "            rag_to_use = _rag\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config={\"top_k\": top_k})\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "    print(\"\\nå•é¡Œ:\", query)\n",
    "    print(\"å›ç­”:\", getattr(resp, \"answer\", None))\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    nodes = getattr(resp, \"retriever_result\", None)\n",
    "    # å˜—è©¦ç”¨æœ€ç°¡ prompt ä¼°è¨ˆ tok/sï¼ˆä¸å½±éŸ¿å¯¦éš›å›ç­”ï¼‰\n",
    "    ctx_text = \"\\n\\n\".join([n.get(\"text\", \"\") for n in (nodes or [])[:min(len(nodes or []), top_k)]])\n",
    "    probe_prompt = f\"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç°¡çŸ­å›ç­”ï¼š\\n\\n{ctx_text}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "    tps = _measure_tok_s(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (ç„¡ eval çµ±è¨ˆ)\")\n",
    "    if nodes:\n",
    "        print(\"ç›¸é—œå…§å®¹ (å‰å¹¾ç­†):\")\n",
    "        for node in nodes[:min(len(nodes), top_k)]:\n",
    "            print(\"-\", node.get(\"text\", \"\"), f\"(source={node.get('source')})\")\n",
    "    else:\n",
    "        print(\"æª¢ç´¢çµæœç‚ºç©ºï¼Œæ”¹ç”¨å‘é‡ç´¢å¼• fallbackï¼š\")\n",
    "        rows = _vector_query_fallback(query, top_k)\n",
    "        if rows:\n",
    "            for r in rows:\n",
    "                try:\n",
    "                    score = r.get(\"score\")\n",
    "                    score_s = f\" score={score:.4f}\" if isinstance(score, (int, float)) else \"\"\n",
    "                except Exception:\n",
    "                    score_s = \"\"\n",
    "                print(\"-\", r.get(\"text\", \"\"), f\"(source={r.get('source')})\" + score_s)\n",
    "        else:\n",
    "            print(\"ï¼ˆæŸ¥ç„¡ç›¸é—œå…§å®¹ï¼‰\")\n",
    "\n",
    "# ç¯„ä¾‹ï¼ˆé è¨­ä¸è‡ªå‹•åŸ·è¡Œï¼‰\n",
    "RUN_QA_ONLY_DEMO = True\n",
    "if RUN_QA_ONLY_DEMO:\n",
    "    qa_only(\"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\", top_k=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b9ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\n",
      "å›ç­”: According to the provided context, mountain goats deficient in vitamin A may lead to:\n",
      "\n",
      "* å¤œç›²ç—‡ (night blindness) due to a lack of vision\n",
      "* è§’è³ªåŒ– (keratinization) of epithelial cells in the respiratory tract, intestines, reproductive organs, and skin\n",
      "* å¼±æŠµæŠ—åŠ› (weak immune system), making them more susceptible to diseases and parasites\n",
      "\n",
      "Additionally, severe vitamin A deficiency can cause å±±ç¾Šå¤œç›²ç—‡ (mountain goat night blindness), which is characterized by a lack of vision.\n",
      "Inference latency: 7035.8 ms\n",
      "LLM throughput: 88.86 tok/s (Ollama eval)\n",
      "æª¢ç´¢çµæœç‚ºç©ºï¼Œæ”¹ç”¨å‘é‡ç´¢å¼• fallbackï¼š\n",
      "- ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„NRC,å»ºè­°ç¶­ç”Ÿç´ Aåœ¨å±±ç¾Šæ¯å…¬æ–¤æ—¥ç³§ä¸­è‡³å°‘å«3500åœ‹éš›å–®ä½(IU)ä»¥ä¸Šã€‚è€Œ2007å¹´ç‰ˆçš„NRCå‰‡å°‡å±±ç¾Šç¶­ç”Ÿç´ Aéœ€è¦é‡æ”¹ä»¥è¦–ç¶²é†‡ç•¶é‡(retinol equivalent,RE)ç‚ºä¼°ç®—å–®ä½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ D:ç¶­ç”Ÿç´ Dåœ¨å±±ç¾Šçš„éˆ£ã€ç£·å¸æ”¶ä¸Šæ‰®æ¼”è‘—éå¸¸é‡è¦çš„è§’è‰²,ç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´å±±ç¾Šéˆ£ç£·å¸æ”¶ä¸å¹³è¡¡ã€å½±éŸ¿å±±ç¾Šéª¨éª¼ç™¼è‚²ã€‚æ”¾ç‰§ç¾Šéš»æœƒå› å¤ªé™½ç…§å°„å…¶çš®è†šè€Œè‡ªè¡Œåˆæˆç¶­ç”Ÿç´ D,æ•…ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dã€‚ç„¶è€Œåœˆé£¼ç¾Šéš»å‰‡å› ç„¡æ³•é•·æ™‚é–“æ¥è§¸å¤ªé™½å…‰èˆ‡ (source=read.txt) score=0.8935\n",
      "- ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹”æ­£å¸¸ï¼šç•¶ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå‰‡èº«é«”ä¹‹å‘¼å¸é“ã€è…¸é“ã€ç”Ÿæ®–é“ã€å°¿é“èˆ‡çœ¼ç›è¡¨çš®ç´°èƒæœƒè§’è³ªåŒ–(epithelial keratinization)ã€‚3.å¼·æŠµæŠ—åŠ›ï¼šå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå®¹æ˜“ä½¿å±±ç¾Šæ„ŸæŸ“ç–¾ç—…åŠå¯„ç”ŸèŸ²ã€ç¹æ®–æ€§èƒ½ä¸ä½³ã€‚\n",
      "\n",
      "é€ æˆå±±ç¾Šç¶­ç”Ÿç´ Aç¼ºä¹ä¹‹åŸå› æœ‰äºŒï¼Œä¸»è¦ç‚ºé£¼ç³§ç¶­ç”Ÿç´ Aå«é‡ä¸è¶³ï¼Œå°¤å…¶åœ¨ä¹¾æ—±å­£ç¯€æˆ–å†¬å­£ï¼Œå› é’è‰ä¾›æ‡‰å—é™ï¼Œæˆ–é¤µé£¼å±±ç¾Šä¹‹ä¹¾è‰å“è³ªä¸è‰¯ï¼Œä¾‹å¦‚ä¹¾è‰è²¯å­˜æœŸé–“å¤ªé•·åŠä¹¾è‰è²¯å­˜ä¸ç•¶ï¼Œå› æ—¥æ›¬èˆ‡é›¨æ·‹ã€‚è£½é€ ä¹¾è‰æ™‚ï¼Œå› å¤©å€™ä¸è‰¯ï¼Œä¹¾è‰ (source=read.txt) score=0.8598\n",
      "- è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Šè€Œå½¢æˆå¤œç›²ç—‡ã€‚å› æ­¤åœ¨è‚‰ç¾Šæ—¥ç³§ä¸­ï¼Œæ¯å…¬æ–¤ç²¾æ–™è‡³å°‘è¦å«3500åœ‹éš›å–®ä½(IU)ä¹‹ç¶­ç”Ÿç´ Aã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Dï¼šåŒæ¨£æ˜¯è„‚æº¶æ€§ç¶­ç”Ÿç´ ï¼Œå…¶ä¸»è¦æ˜¯å¹«åŠ©å‹•ç‰©æœ‰æ•ˆç‡çš„å¸æ”¶åŠåˆ©ç”¨éˆ£èˆ‡ç£·ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´è‚‰ç¾Šéˆ£èˆ‡ç£·çš„å¸æ”¶ä¸å¹³è¡¡ï¼Œå½±éŸ¿è‚‰ç¾Šéª¨éª¼ç™¼è‚²ï¼Œä»”ç¾ŠåŠæˆç¾Šå‡æœƒç”¢ç”Ÿè»Ÿéª¨ç—‡åŠéª¨è³ªç–é¬†ç—‡ã€‚ç”±æ–¼é™½å…‰ä¸­çš„ç´«å¤–ç·šç…§å°„å‹•ç‰©çš®è†šï¼Œæœƒä½¿å‹•ç‰©çš®è†šä¸­ä¹‹7dehydrocholesterolè½‰åŒ–æˆç¶­ç”Ÿç´ D3ã€‚å› æ­¤æ”¾ç‰§ç¾Šéš»ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dï¼Œç„¶è€Œåœˆé£¼çš„ç¾Š (source=read.txt) score=0.8475\n",
      "- ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ä¹‹ä¸‹ç—¢ã€åŠ‡æ¸´(polydipsia)ã€å¤šå°¿ã€å‘¼å¸å›°é›£èˆ‡ç›´è…¸æº«åº¦ä½ä¸‹ã€‚ç¡’éé‡ä¹Ÿæœƒå¼•èµ·è¹„ä¹‹ç–¼ç—›åŠè„«è½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aï¼šå±±ç¾Šä¹‹èŠ»æ–™ä¸å«æœ‰ç¶­ç”Ÿç´ Aï¼Œä¸éå«æœ‰ç¶­ç”Ÿç´ Aä¹‹å…ˆé©…ç‰©ï¼Œå³èƒ¡è˜¿è””ç´ ã€‚å¤§ç´„1gçš„B-èƒ¡è˜¿è””ç´ ç›¸ç•¶æ–¼400IUçš„ç¶­ç”Ÿç´ Aã€‚å‹•ç‰©å¸æ”¶ç¶­ç”Ÿç´ Aä¹‹ä¸»è¦éƒ¨ä½åœ¨å°è…¸å‰æ®µï¼Œèº«é«”è²¯è—ç¶­ç”Ÿç´ Aä¹‹å™¨å®˜ç‚ºè‚è‡Ÿï¼Œç´„ä½”75-90%ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsi (source=read.txt) score=0.8347\n",
      "- ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´ Aç¼ºä¹ã€‚2.ç„¡æ³•æ”¾ç‰§é£¼é¤Šå±±ç¾Šè€…éœ€æä¾›é’è‰çµ¦é£¼é¤Šçš„å±±ç¾Šã€‚3.æ³¨æ„åœ¨é£¼æ–™å…§æ·»åŠ ç¶­ç”Ÿç´ Aã€‚4.æ¯ç¾Šæ‡·å­•æœŸè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚5.å‡ºç”Ÿä»”ç¾Šè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚6.å¥åº·çš„æ¯ç¾Šæ‰èƒ½ç”Ÿç”¢å„ªè³ªçš„åˆä¹³ã€‚\n",
      "\n",
      "å±±ç¾Šé¼“è„¹(bloat)ï¼šå±±ç¾Šé¼“è„¹é€šå¸¸æ˜¯ç”±å¤§é‡æ¡é£Ÿè±å¯Œçš„æ½®æ¿•æ–°é®®è‰æ–™æˆ–ç©€é¡å¼•èµ·çš„,å¦‚ä¸‰è‘‰è‰,è‹œè“¿æˆ–è±†ç§‘ç‰§è‰ã€‚é€™äº›æ½®æ¿•æ–°é®®è‰æ–™åœ¨ç˜¤èƒƒä¸­å½¢æˆå¾®å°çš„æ°£æ³¡,å°è‡´å±±ç¾Šå™¯æ°£ç„¡æ³•æ’å‡ºã€‚ç˜¤èƒƒéš¨è‘—æ³¡æ²«è†¨è„¹å£“è¿«æ©«éš”è†œ,é€ æˆå±±ç¾Šå¯èƒ½å› å‘¼å¸æˆ–å¾ªç’°è¡°ç«­è€Œå¾ˆ (source=read.txt) score=0.8322\n",
      "LLM throughput: 88.86 tok/s (Ollama eval)\n",
      "æª¢ç´¢çµæœç‚ºç©ºï¼Œæ”¹ç”¨å‘é‡ç´¢å¼• fallbackï¼š\n",
      "- ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„NRC,å»ºè­°ç¶­ç”Ÿç´ Aåœ¨å±±ç¾Šæ¯å…¬æ–¤æ—¥ç³§ä¸­è‡³å°‘å«3500åœ‹éš›å–®ä½(IU)ä»¥ä¸Šã€‚è€Œ2007å¹´ç‰ˆçš„NRCå‰‡å°‡å±±ç¾Šç¶­ç”Ÿç´ Aéœ€è¦é‡æ”¹ä»¥è¦–ç¶²é†‡ç•¶é‡(retinol equivalent,RE)ç‚ºä¼°ç®—å–®ä½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ D:ç¶­ç”Ÿç´ Dåœ¨å±±ç¾Šçš„éˆ£ã€ç£·å¸æ”¶ä¸Šæ‰®æ¼”è‘—éå¸¸é‡è¦çš„è§’è‰²,ç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´å±±ç¾Šéˆ£ç£·å¸æ”¶ä¸å¹³è¡¡ã€å½±éŸ¿å±±ç¾Šéª¨éª¼ç™¼è‚²ã€‚æ”¾ç‰§ç¾Šéš»æœƒå› å¤ªé™½ç…§å°„å…¶çš®è†šè€Œè‡ªè¡Œåˆæˆç¶­ç”Ÿç´ D,æ•…ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dã€‚ç„¶è€Œåœˆé£¼ç¾Šéš»å‰‡å› ç„¡æ³•é•·æ™‚é–“æ¥è§¸å¤ªé™½å…‰èˆ‡ (source=read.txt) score=0.8935\n",
      "- ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹”æ­£å¸¸ï¼šç•¶ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå‰‡èº«é«”ä¹‹å‘¼å¸é“ã€è…¸é“ã€ç”Ÿæ®–é“ã€å°¿é“èˆ‡çœ¼ç›è¡¨çš®ç´°èƒæœƒè§’è³ªåŒ–(epithelial keratinization)ã€‚3.å¼·æŠµæŠ—åŠ›ï¼šå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aï¼Œå®¹æ˜“ä½¿å±±ç¾Šæ„ŸæŸ“ç–¾ç—…åŠå¯„ç”ŸèŸ²ã€ç¹æ®–æ€§èƒ½ä¸ä½³ã€‚\n",
      "\n",
      "é€ æˆå±±ç¾Šç¶­ç”Ÿç´ Aç¼ºä¹ä¹‹åŸå› æœ‰äºŒï¼Œä¸»è¦ç‚ºé£¼ç³§ç¶­ç”Ÿç´ Aå«é‡ä¸è¶³ï¼Œå°¤å…¶åœ¨ä¹¾æ—±å­£ç¯€æˆ–å†¬å­£ï¼Œå› é’è‰ä¾›æ‡‰å—é™ï¼Œæˆ–é¤µé£¼å±±ç¾Šä¹‹ä¹¾è‰å“è³ªä¸è‰¯ï¼Œä¾‹å¦‚ä¹¾è‰è²¯å­˜æœŸé–“å¤ªé•·åŠä¹¾è‰è²¯å­˜ä¸ç•¶ï¼Œå› æ—¥æ›¬èˆ‡é›¨æ·‹ã€‚è£½é€ ä¹¾è‰æ™‚ï¼Œå› å¤©å€™ä¸è‰¯ï¼Œä¹¾è‰ (source=read.txt) score=0.8598\n",
      "- è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Šè€Œå½¢æˆå¤œç›²ç—‡ã€‚å› æ­¤åœ¨è‚‰ç¾Šæ—¥ç³§ä¸­ï¼Œæ¯å…¬æ–¤ç²¾æ–™è‡³å°‘è¦å«3500åœ‹éš›å–®ä½(IU)ä¹‹ç¶­ç”Ÿç´ Aã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Dï¼šåŒæ¨£æ˜¯è„‚æº¶æ€§ç¶­ç”Ÿç´ ï¼Œå…¶ä¸»è¦æ˜¯å¹«åŠ©å‹•ç‰©æœ‰æ•ˆç‡çš„å¸æ”¶åŠåˆ©ç”¨éˆ£èˆ‡ç£·ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Dæœƒå°è‡´è‚‰ç¾Šéˆ£èˆ‡ç£·çš„å¸æ”¶ä¸å¹³è¡¡ï¼Œå½±éŸ¿è‚‰ç¾Šéª¨éª¼ç™¼è‚²ï¼Œä»”ç¾ŠåŠæˆç¾Šå‡æœƒç”¢ç”Ÿè»Ÿéª¨ç—‡åŠéª¨è³ªç–é¬†ç—‡ã€‚ç”±æ–¼é™½å…‰ä¸­çš„ç´«å¤–ç·šç…§å°„å‹•ç‰©çš®è†šï¼Œæœƒä½¿å‹•ç‰©çš®è†šä¸­ä¹‹7dehydrocholesterolè½‰åŒ–æˆç¶­ç”Ÿç´ D3ã€‚å› æ­¤æ”¾ç‰§ç¾Šéš»ä¸æœƒç¼ºä¹ç¶­ç”Ÿç´ Dï¼Œç„¶è€Œåœˆé£¼çš„ç¾Š (source=read.txt) score=0.8475\n",
      "- ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ä¹‹ä¸‹ç—¢ã€åŠ‡æ¸´(polydipsia)ã€å¤šå°¿ã€å‘¼å¸å›°é›£èˆ‡ç›´è…¸æº«åº¦ä½ä¸‹ã€‚ç¡’éé‡ä¹Ÿæœƒå¼•èµ·è¹„ä¹‹ç–¼ç—›åŠè„«è½ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aï¼šå±±ç¾Šä¹‹èŠ»æ–™ä¸å«æœ‰ç¶­ç”Ÿç´ Aï¼Œä¸éå«æœ‰ç¶­ç”Ÿç´ Aä¹‹å…ˆé©…ç‰©ï¼Œå³èƒ¡è˜¿è””ç´ ã€‚å¤§ç´„1gçš„B-èƒ¡è˜¿è””ç´ ç›¸ç•¶æ–¼400IUçš„ç¶­ç”Ÿç´ Aã€‚å‹•ç‰©å¸æ”¶ç¶­ç”Ÿç´ Aä¹‹ä¸»è¦éƒ¨ä½åœ¨å°è…¸å‰æ®µï¼Œèº«é«”è²¯è—ç¶­ç”Ÿç´ Aä¹‹å™¨å®˜ç‚ºè‚è‡Ÿï¼Œç´„ä½”75-90%ä¹‹ç¸½è²¯å­˜é‡ã€‚\n",
      "\n",
      "ç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsi (source=read.txt) score=0.8347\n",
      "- ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "\n",
      "å±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´ Aç¼ºä¹ã€‚2.ç„¡æ³•æ”¾ç‰§é£¼é¤Šå±±ç¾Šè€…éœ€æä¾›é’è‰çµ¦é£¼é¤Šçš„å±±ç¾Šã€‚3.æ³¨æ„åœ¨é£¼æ–™å…§æ·»åŠ ç¶­ç”Ÿç´ Aã€‚4.æ¯ç¾Šæ‡·å­•æœŸè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚5.å‡ºç”Ÿä»”ç¾Šè¦æ³¨æ„ç¶­ç”Ÿç´ Aè£œå……ã€‚6.å¥åº·çš„æ¯ç¾Šæ‰èƒ½ç”Ÿç”¢å„ªè³ªçš„åˆä¹³ã€‚\n",
      "\n",
      "å±±ç¾Šé¼“è„¹(bloat)ï¼šå±±ç¾Šé¼“è„¹é€šå¸¸æ˜¯ç”±å¤§é‡æ¡é£Ÿè±å¯Œçš„æ½®æ¿•æ–°é®®è‰æ–™æˆ–ç©€é¡å¼•èµ·çš„,å¦‚ä¸‰è‘‰è‰,è‹œè“¿æˆ–è±†ç§‘ç‰§è‰ã€‚é€™äº›æ½®æ¿•æ–°é®®è‰æ–™åœ¨ç˜¤èƒƒä¸­å½¢æˆå¾®å°çš„æ°£æ³¡,å°è‡´å±±ç¾Šå™¯æ°£ç„¡æ³•æ’å‡ºã€‚ç˜¤èƒƒéš¨è‘—æ³¡æ²«è†¨è„¹å£“è¿«æ©«éš”è†œ,é€ æˆå±±ç¾Šå¯èƒ½å› å‘¼å¸æˆ–å¾ªç’°è¡°ç«­è€Œå¾ˆ (source=read.txt) score=0.8322\n"
     ]
    }
   ],
   "source": [
    "RUN_QA_ONLY_DEMO = True\n",
    "if RUN_QA_ONLY_DEMO:\n",
    "    qa_only(\"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\", top_k=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a24a7",
   "metadata": {},
   "source": [
    "### å¦‚ä½•ä½¿ç”¨ QA-only æ¨¡å¼\n",
    "\n",
    "- ä¸æƒ³é‡è·‘æ•´å€‹æµç¨‹æ™‚ï¼Œç›´æ¥åŸ·è¡Œä¸Šä¸€å€‹ã€ŒQA-onlyã€ç¨‹å¼ç¢¼ Cellã€‚\n",
    "- è©² Cell åªé€£ç·š Neo4jã€è¼‰å…¥æœ¬åœ°å‘é‡æ¨¡å‹ã€å»ºç«‹æª¢ç´¢èˆ‡ LLMï¼Œä¸æœƒé‡å»ºç´¢å¼•æˆ–é‡å¯«è³‡æ–™ã€‚\n",
    "- å‘¼å«æ–¹å¼ï¼š\n",
    "  - `qa_only(\"ä½ çš„å•é¡Œ\", top_k=5)`\n",
    "- å¦‚éœ€å¿«é€Ÿç¤ºç¯„ï¼ŒæŠŠ `RUN_QA_ONLY_DEMO = True` å¾Œå†æ¬¡åŸ·è¡Œè©² Cellã€‚\n",
    "\n",
    "#### é¡å¤–è¼¸å‡ºèªªæ˜ï¼ˆLatency èˆ‡ tok/sï¼‰\n",
    "- â±ï¸ Inference latency: åŒ…å«æª¢ç´¢èˆ‡ LLM ç”Ÿæˆçš„ç«¯åˆ°ç«¯æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰ã€‚\n",
    "- ğŸš€ LLM throughput (tok/s): é€éä¸€æ¬¡æ¥µç°¡ `ollama.chat` å–å¾— Ollama çš„ eval çµ±è¨ˆï¼ˆè‹¥ç‰ˆæœ¬/æ¨¡å‹ä¸æ”¯æ´å‰‡é¡¯ç¤º N/Aï¼‰ã€‚é€™ä¸æœƒå½±éŸ¿å¯¦éš›å›ç­”ï¼Œåªä½œç‚ºåƒè€ƒæ•ˆèƒ½æŒ‡æ¨™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a082f",
   "metadata": {},
   "source": [
    "## âœ… Schema hardening and indexes (optional)\n",
    "\n",
    "é€™å€‹å€å¡Šæœƒï¼š\n",
    "- æ–°å¢å”¯ä¸€æ€§é™åˆ¶ï¼š`Chunk.id`ã€`Entity.name`\n",
    "- å»ºç«‹ Fulltext ç´¢å¼•ï¼š`chunk_text_fts` æ–¼ `Chunk(text)`ï¼Œæ”¯æ´æ··åˆæª¢ç´¢\n",
    "\n",
    "åŸ·è¡Œä¸€æ¬¡å³å¯ï¼ˆä¹‹å¾Œæœƒè‡ªå‹•è·³éå·²å­˜åœ¨çš„ç´¢å¼•/é™åˆ¶ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dab9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema and fulltext index ensured âœ…\n"
     ]
    }
   ],
   "source": [
    "# å»ºç«‹å”¯ä¸€æ€§é™åˆ¶èˆ‡ Fulltext ç´¢å¼•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "_uri = \"bolt://localhost:7687\"\n",
    "_user = \"neo4j\"\n",
    "_pw = \"neo4jgoat\"\n",
    "\n",
    "_driver = GraphDatabase.driver(_uri, auth=(_user, _pw))\n",
    "with _driver.session() as session:\n",
    "    # constraints\n",
    "    session.run(\"CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\n",
    "    session.run(\"CREATE CONSTRAINT entity_name_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE\")\n",
    "    # fulltext\n",
    "    session.run(\n",
    "        \"\"\"\n",
    "        CREATE FULLTEXT INDEX chunk_text_fts IF NOT EXISTS\n",
    "        FOR (c:Chunk) ON EACH [c.text]\n",
    "        \"\"\"\n",
    "    )\n",
    "print(\"Schema and fulltext index ensured âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b63fa",
   "metadata": {},
   "source": [
    "## ğŸ” Hybrid retrieval and graph expansion (optional)\n",
    "\n",
    "é€™å€‹å€å¡Šç¤ºç¯„ï¼š\n",
    "- ä½¿ç”¨ `HybridRetriever` åŒæ™‚çµåˆå‘é‡ç´¢å¼•èˆ‡ Fulltext ç´¢å¼•ï¼ˆå¯é¸ç·šæ€§åŠ æ¬Šï¼‰\n",
    "- ä»¥æª¢ç´¢åˆ°çš„ Chunk æ“´å±•åˆ°å…¶ `MENTIONS` çš„ `Entity` èˆ‡ `RELATION` ä¸‰å…ƒçµ„ï¼Œçµ„åˆæ›´è±å¯Œçš„ä¸Šä¸‹æ–‡\n",
    "\n",
    "ä½ å¯ä»¥èˆ‡ç¾æœ‰ `VectorRetriever + GraphRAG` ä½µå­˜ï¼Œäº’ä¸å½±éŸ¿ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd2b2f",
   "metadata": {},
   "source": [
    "# HybridRetriever + Graph expansion QA\n",
    "from neo4j_graphrag.retrievers.hybrid import HybridRetriever, HybridSearchRanker\n",
    "from neo4j_graphrag.retrievers import VectorRetriever as _VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_VECTOR = \"chunk_embeddings\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# Ensure the fulltext index exists and is online (idempotent, with procedure fallback)\n",
    "def _ensure_fulltext_index(driver, index_name: str, label: str, prop: str = \"text\", strict: bool = False) -> bool:\n",
    "    \"\"\"Try to ensure a fulltext index exists. Returns True if exists/created, else False.\n",
    "    If strict=True, raises when cannot be ensured.\n",
    "    \"\"\"\n",
    "    def _exists(sess) -> bool:\n",
    "        try:\n",
    "            rows = sess.run(\"SHOW INDEXES\").data()\n",
    "            if any(str(r.get(\"name\")) == index_name and (r.get(\"type\") == \"FULLTEXT\" or r.get(\"indexType\") == \"FULLTEXT\") for r in rows):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback check via procedure\n",
    "        try:\n",
    "            r = sess.run(\"CALL db.index.fulltext.list() YIELD name WHERE name = $name RETURN name\", name=index_name).single()\n",
    "            if r:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    with driver.session() as session:\n",
    "        if _exists(session):\n",
    "            return True\n",
    "        created = False\n",
    "        # Try schema DDL (Neo4j 5.x)\n",
    "        try:\n",
    "            cypher = f\"\"\"\n",
    "            CREATE FULLTEXT INDEX {index_name} IF NOT EXISTS\n",
    "            FOR (n:{label}) ON EACH [n.{prop}]\n",
    "            \"\"\"\n",
    "            session.run(cypher)\n",
    "            created = True\n",
    "        except Exception:\n",
    "            created = False\n",
    "        # Fallback to procedure (Neo4j 4.x/compat)\n",
    "        if not created:\n",
    "            try:\n",
    "                session.run(\n",
    "                    \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                    name=index_name,\n",
    "                    labels=[label],\n",
    "                    props=[prop],\n",
    "                )\n",
    "                created = True\n",
    "            except Exception:\n",
    "                created = False\n",
    "        # Wait until online (best-effort)\n",
    "        try:\n",
    "            session.run(\"CALL db.awaitIndexes()\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        ok = _exists(session)\n",
    "        if not ok and strict:\n",
    "            raise RuntimeError(f\"ç„¡æ³•å»ºç«‹ Fulltext ç´¢å¼•: {index_name}\")\n",
    "        return ok\n",
    "\n",
    "_USING_VECTOR_ONLY = False\n",
    "_has_fts = _ensure_fulltext_index(_driver, INDEX_FTS, label=\"Chunk\", prop=\"text\", strict=False)\n",
    "\n",
    "# 1) æº–å‚™æª¢ç´¢å™¨ï¼šè‹¥ Fulltext ä¸å¯ç”¨ï¼Œå‰‡é€€åŒ–æˆå‘é‡-only æª¢ç´¢\n",
    "if _has_fts:\n",
    "    _hybrid = HybridRetriever(\n",
    "        driver=_driver,\n",
    "        vector_index_name=INDEX_VECTOR,\n",
    "        fulltext_index_name=INDEX_FTS,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Fulltext ç´¢å¼•ä¸å¯ç”¨æˆ–ç„¡æ³•å»ºç«‹ï¼Œå°‡ä½¿ç”¨å‘é‡-only æª¢ç´¢åšç‚ºæ›¿ä»£ã€‚\")\n",
    "    _USING_VECTOR_ONLY = True\n",
    "    _hybrid = _VectorRetriever(\n",
    "        driver=_driver,\n",
    "        index_name=INDEX_VECTOR,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag_hybrid = GraphRAG(retriever=_hybrid, llm=_llm)\n",
    "\n",
    "\n",
    "def _measure_tok_s_hybrid(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"Estimate tok/s from Ollama eval stats; return -1.0 if unavailable.\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or r.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "def qa_hybrid(query: str, top_k: int = 5, ranker: str = \"naive\", alpha: float | None = None, temperature: float | None = None):\n",
    "    \"\"\"Hybrid QA with optional temperature to control LLM variability.\"\"\"\n",
    "    # choose LLM per call if temperature is provided\n",
    "    rag_to_use = _rag_hybrid\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_tmp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(retriever=_hybrid, llm=_llm_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"[Hybrid] å»ºç«‹å¸¶ temperature çš„ LLM å¤±æ•—ï¼Œæ”¹ç”¨é è¨­ï¼š{e}\")\n",
    "            rag_to_use = _rag_hybrid\n",
    "\n",
    "    if _USING_VECTOR_ONLY:\n",
    "        print(\"[Hybrid] æç¤ºï¼šFulltext ä¸å¯ç”¨ï¼Œç¾ä»¥å‘é‡-only æ¨¡å¼åŸ·è¡Œã€‚\")\n",
    "    cfg = {\"top_k\": top_k}\n",
    "    if not _USING_VECTOR_ONLY and ranker == \"linear\":\n",
    "        cfg[\"ranker\"] = HybridSearchRanker.LINEAR\n",
    "        cfg[\"alpha\"] = alpha if alpha is not None else 0.6\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config=cfg, return_context=True)\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[Hybrid] å•é¡Œ:\", query)\n",
    "    print(\"[Hybrid] å›ç­”:\", resp.answer)\n",
    "\n",
    "    # Print latency and estimate tok/s via a small probe\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    items = getattr(resp, \"retriever_result\", None)\n",
    "    # Build a small context snippet for throughput probe\n",
    "    ctx_texts = []\n",
    "    if items:\n",
    "        try:\n",
    "            # Hybrid items container\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None)\n",
    "                if not content and isinstance(getattr(it, \"metadata\", None), dict):\n",
    "                    content = getattr(it, \"metadata\", {}).get(\"text\")\n",
    "                if content:\n",
    "                    ctx_texts.append(str(content))\n",
    "        except Exception:\n",
    "            # Vector-only list of dicts\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    ctx_texts.append(str(n.get(\"text\", \"\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "    ctx_blob = \"\\n\\n\".join(ctx_texts)\n",
    "    probe_prompt = f\"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç°¡çŸ­å›ç­”ï¼š\\n\\n{ctx_blob}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (ç„¡ eval çµ±è¨ˆ)\")\n",
    "\n",
    "    if items:\n",
    "        print(\"[Hybrid] æª¢ç´¢çµæœæ‘˜è¦:\")\n",
    "        # æ”¯æ´ HybridRetriever çš„ items å®¹å™¨æˆ– VectorRetriever çš„ list ç¯€é»\n",
    "        try:\n",
    "            # Hybrid path\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None) or it.get(\"text\", \"\")\n",
    "                meta = getattr(it, \"metadata\", None) or it.get(\"source\", \"\")\n",
    "                print(\"-\", str(content)[:120].replace(\"\\n\", \" \"), str(meta)[:80])\n",
    "        except Exception:\n",
    "            # Vector-only path (list of dicts)\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    print(\"-\", str(n.get(\"text\", \"\"))[:120].replace(\"\\n\", \" \"), f\"(source={n.get('source')})\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# 2) å¾æª¢ç´¢åˆ°çš„ Chunk æ“´å±•åœ–ï¼ˆMENTIONS èˆ‡ RELATIONï¼‰\n",
    "\n",
    "def expand_context_from_chunks(query: str, top_k: int = 5) -> str:\n",
    "    q_emb = _embedder.embed_query(query)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    cypher = \"\"\"\n",
    "    CALL db.index.vector.queryNodes($index, $k, $qemb)\n",
    "    YIELD node, score\n",
    "    WITH collect(node)[..$k] AS chunks\n",
    "    // æ“´å±•ç›¸é—œå¯¦é«”èˆ‡ä¸‰å…ƒçµ„\n",
    "    UNWIND chunks AS c\n",
    "    OPTIONAL MATCH (c)-[:MENTIONS]->(e:Entity)\n",
    "    OPTIONAL MATCH (e)<-[r:RELATION]->(t:Entity)\n",
    "    WITH chunks, collect(DISTINCT e.name) AS ents,\n",
    "         collect(DISTINCT {h:e.name, rel:r.type, t:t.name, sample: head(coalesce(r.chunks, []))}) AS triples\n",
    "    RETURN [c IN chunks | c.text][..$k] AS chunk_texts, ents[..50] AS entities, triples[..100] AS triples\n",
    "    \"\"\"\n",
    "    with _driver.session() as session:\n",
    "        row = session.run(cypher, index=INDEX_VECTOR, k=top_k, qemb=q_emb).single()\n",
    "    if not row:\n",
    "        return \"\"\n",
    "    chunks = row.get(\"chunk_texts\") or []\n",
    "    ents = row.get(\"entities\") or []\n",
    "    triples = row.get(\"triples\") or []\n",
    "    triple_lines = [f\"({t.get('h')})-[:{t.get('rel')}]->({t.get('t')})\" for t in triples if t]\n",
    "    context = (\n",
    "        \"[Chunks]\\n\" + \"\\n\\n\".join(chunks) +\n",
    "        \"\\n\\n[Entities]\\n\" + \", \".join(ents) +\n",
    "        \"\\n\\n[Triples]\\n\" + \"\\n\".join(triple_lines)\n",
    "    )\n",
    "    return context\n",
    "\n",
    "\n",
    "def qa_graph(query: str, top_k: int = 5, temperature: float | None = None):\n",
    "    \"\"\"Graph expansion QA with metrics and adjustable temperature.\n",
    "\n",
    "    Args:\n",
    "        query: å•é¡Œ\n",
    "        top_k: æª¢ç´¢æ“´å±•çš„ chunk æ•¸é‡\n",
    "        temperature: è‹¥æä¾›ï¼Œå°‡ä»¥è©²å€¼å»ºç«‹è‡¨æ™‚ LLM ä»¥æ§åˆ¶éš¨æ©Ÿæ€§ï¼ˆ0 æ›´ä¿å®ˆï¼Œ1 æ›´ç™¼æ•£ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    context = expand_context_from_chunks(query, top_k=top_k)\n",
    "    if not context:\n",
    "        print(\"[GraphExpand] ç„¡æª¢ç´¢çµæœ\")\n",
    "        return\n",
    "\n",
    "    prompt = f\"è«‹æ ¹æ“šæä¾›çš„å…§å®¹å›ç­”å•é¡Œï¼Œæ¢åˆ—ä¸”å«ä¾æ“šã€‚\\n\\n{context}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "\n",
    "    # é¸æ“‡ LLMï¼ˆæ”¯æ´ per-call èª¿æ•´ temperatureï¼‰\n",
    "    llm_obj = _llm\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            llm_obj = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[GraphExpand] å»ºç«‹å¸¶ temperature çš„ LLM å¤±æ•—ï¼Œæ”¹ç”¨é è¨­ï¼š{e}\")\n",
    "            llm_obj = _llm\n",
    "\n",
    "    ans = llm_obj.invoke(prompt).content\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[GraphExpand] å•é¡Œ:\", query)\n",
    "    print(\"[GraphExpand] å›ç­”:\\n\", ans)\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "\n",
    "    # ä»¥ç›¸åŒ context æ§‹é€ æ¥µç°¡ probe ä¼°ç®— tok/s\n",
    "    probe_prompt = f\"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç°¡çŸ­å›ç­”ï¼š\\n\\n{context}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (ç„¡ eval çµ±è¨ˆ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04cb292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aint\\Desktop\\LLM+KB\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Fulltext ç´¢å¼•ä¸å¯ç”¨æˆ–ç„¡æ³•å»ºç«‹ï¼Œå°‡ä½¿ç”¨å‘é‡-only æª¢ç´¢åšç‚ºæ›¿ä»£ã€‚\n"
     ]
    }
   ],
   "source": [
    "# HybridRetriever + Graph expansion QA\n",
    "from neo4j_graphrag.retrievers.hybrid import HybridRetriever, HybridSearchRanker\n",
    "from neo4j_graphrag.retrievers import VectorRetriever as _VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_VECTOR = \"chunk_embeddings\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# Ensure the fulltext index exists and is online (idempotent, with procedure fallback)\n",
    "def _ensure_fulltext_index(driver, index_name: str, label: str, prop: str = \"text\", strict: bool = False) -> bool:\n",
    "    \"\"\"Try to ensure a fulltext index exists. Returns True if exists/created, else False.\n",
    "    If strict=True, raises when cannot be ensured.\n",
    "    \"\"\"\n",
    "    def _exists(sess) -> bool:\n",
    "        try:\n",
    "            rows = sess.run(\"SHOW INDEXES\").data()\n",
    "            if any(str(r.get(\"name\")) == index_name and (r.get(\"type\") == \"FULLTEXT\" or r.get(\"indexType\") == \"FULLTEXT\") for r in rows):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback check via procedure\n",
    "        try:\n",
    "            r = sess.run(\"CALL db.index.fulltext.list() YIELD name WHERE name = $name RETURN name\", name=index_name).single()\n",
    "            if r:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    with driver.session() as session:\n",
    "        if _exists(session):\n",
    "            return True\n",
    "        created = False\n",
    "        # Try schema DDL (Neo4j 5.x)\n",
    "        try:\n",
    "            cypher = f\"\"\"\n",
    "            CREATE FULLTEXT INDEX {index_name} IF NOT EXISTS\n",
    "            FOR (n:{label}) ON EACH [n.{prop}]\n",
    "            \"\"\"\n",
    "            session.run(cypher)\n",
    "            created = True\n",
    "        except Exception:\n",
    "            created = False\n",
    "        # Fallback to procedure (Neo4j 4.x/compat)\n",
    "        if not created:\n",
    "            try:\n",
    "                session.run(\n",
    "                    \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                    name=index_name,\n",
    "                    labels=[label],\n",
    "                    props=[prop],\n",
    "                )\n",
    "                created = True\n",
    "            except Exception:\n",
    "                created = False\n",
    "        # Wait until online (best-effort)\n",
    "        try:\n",
    "            session.run(\"CALL db.awaitIndexes()\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        ok = _exists(session)\n",
    "        if not ok and strict:\n",
    "            raise RuntimeError(f\"ç„¡æ³•å»ºç«‹ Fulltext ç´¢å¼•: {index_name}\")\n",
    "        return ok\n",
    "\n",
    "_USING_VECTOR_ONLY = False\n",
    "_has_fts = _ensure_fulltext_index(_driver, INDEX_FTS, label=\"Chunk\", prop=\"text\", strict=False)\n",
    "\n",
    "# 1) æº–å‚™æª¢ç´¢å™¨ï¼šè‹¥ Fulltext ä¸å¯ç”¨ï¼Œå‰‡é€€åŒ–æˆå‘é‡-only æª¢ç´¢\n",
    "if _has_fts:\n",
    "    _hybrid = HybridRetriever(\n",
    "        driver=_driver,\n",
    "        vector_index_name=INDEX_VECTOR,\n",
    "        fulltext_index_name=INDEX_FTS,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Fulltext ç´¢å¼•ä¸å¯ç”¨æˆ–ç„¡æ³•å»ºç«‹ï¼Œå°‡ä½¿ç”¨å‘é‡-only æª¢ç´¢åšç‚ºæ›¿ä»£ã€‚\")\n",
    "    _USING_VECTOR_ONLY = True\n",
    "    _hybrid = _VectorRetriever(\n",
    "        driver=_driver,\n",
    "        index_name=INDEX_VECTOR,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag_hybrid = GraphRAG(retriever=_hybrid, llm=_llm)\n",
    "\n",
    "\n",
    "def _measure_tok_s_hybrid(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"Estimate tok/s from Ollama eval stats; return -1.0 if unavailable.\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or r.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "def qa_hybrid(query: str, top_k: int = 5, ranker: str = \"naive\", alpha: float | None = None, temperature: float | None = None):\n",
    "    \"\"\"Hybrid QA with optional temperature to control LLM variability.\"\"\"\n",
    "    # choose LLM per call if temperature is provided\n",
    "    rag_to_use = _rag_hybrid\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_tmp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(retriever=_hybrid, llm=_llm_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"[Hybrid] å»ºç«‹å¸¶ temperature çš„ LLM å¤±æ•—ï¼Œæ”¹ç”¨é è¨­ï¼š{e}\")\n",
    "            rag_to_use = _rag_hybrid\n",
    "\n",
    "    if _USING_VECTOR_ONLY:\n",
    "        print(\"[Hybrid] æç¤ºï¼šFulltext ä¸å¯ç”¨ï¼Œç¾ä»¥å‘é‡-only æ¨¡å¼åŸ·è¡Œã€‚\")\n",
    "    cfg = {\"top_k\": top_k}\n",
    "    if not _USING_VECTOR_ONLY and ranker == \"linear\":\n",
    "        cfg[\"ranker\"] = HybridSearchRanker.LINEAR\n",
    "        cfg[\"alpha\"] = alpha if alpha is not None else 0.6\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config=cfg, return_context=True)\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[Hybrid] å•é¡Œ:\", query)\n",
    "    print(\"[Hybrid] å›ç­”:\", resp.answer)\n",
    "\n",
    "    # Print latency and estimate tok/s via a small probe\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    items = getattr(resp, \"retriever_result\", None)\n",
    "    # Build a small context snippet for throughput probe\n",
    "    ctx_texts = []\n",
    "    if items:\n",
    "        try:\n",
    "            # Hybrid items container\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None)\n",
    "                if not content and isinstance(getattr(it, \"metadata\", None), dict):\n",
    "                    content = getattr(it, \"metadata\", {}).get(\"text\")\n",
    "                if content:\n",
    "                    ctx_texts.append(str(content))\n",
    "        except Exception:\n",
    "            # Vector-only list of dicts\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    ctx_texts.append(str(n.get(\"text\", \"\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "    ctx_blob = \"\\n\\n\".join(ctx_texts)\n",
    "    probe_prompt = f\"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç°¡çŸ­å›ç­”ï¼š\\n\\n{ctx_blob}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (ç„¡ eval çµ±è¨ˆ)\")\n",
    "\n",
    "    if items:\n",
    "        print(\"[Hybrid] æª¢ç´¢çµæœæ‘˜è¦:\")\n",
    "        # æ”¯æ´ HybridRetriever çš„ items å®¹å™¨æˆ– VectorRetriever çš„ list ç¯€é»\n",
    "        try:\n",
    "            # Hybrid path\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None) or it.get(\"text\", \"\")\n",
    "                meta = getattr(it, \"metadata\", None) or it.get(\"source\", \"\")\n",
    "                print(\"-\", str(content)[:120].replace(\"\\n\", \" \"), str(meta)[:80])\n",
    "        except Exception:\n",
    "            # Vector-only path (list of dicts)\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    print(\"-\", str(n.get(\"text\", \"\"))[:120].replace(\"\\n\", \" \"), f\"(source={n.get('source')})\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# 2) å¾æª¢ç´¢åˆ°çš„ Chunk æ“´å±•åœ–ï¼ˆMENTIONS èˆ‡ RELATIONï¼‰\n",
    "\n",
    "def expand_context_from_chunks(query: str, top_k: int = 5) -> str:\n",
    "    q_emb = _embedder.embed_query(query)\n",
    "    if hasattr(q_emb, \"tolist\"):\n",
    "        q_emb = q_emb.tolist()\n",
    "    cypher = \"\"\"\n",
    "    CALL db.index.vector.queryNodes($index, $k, $qemb)\n",
    "    YIELD node, score\n",
    "    WITH collect(node)[..$k] AS chunks\n",
    "    // æ“´å±•ç›¸é—œå¯¦é«”èˆ‡ä¸‰å…ƒçµ„\n",
    "    UNWIND chunks AS c\n",
    "    OPTIONAL MATCH (c)-[:MENTIONS]->(e:Entity)\n",
    "    OPTIONAL MATCH (e)<-[r:RELATION]->(t:Entity)\n",
    "    WITH chunks, collect(DISTINCT e.name) AS ents,\n",
    "         collect(DISTINCT {h:e.name, rel:r.type, t:t.name, sample: head(coalesce(r.chunks, []))}) AS triples\n",
    "    RETURN [c IN chunks | c.text][..$k] AS chunk_texts, ents[..50] AS entities, triples[..100] AS triples\n",
    "    \"\"\"\n",
    "    with _driver.session() as session:\n",
    "        row = session.run(cypher, index=INDEX_VECTOR, k=top_k, qemb=q_emb).single()\n",
    "    if not row:\n",
    "        return \"\"\n",
    "    chunks = row.get(\"chunk_texts\") or []\n",
    "    ents = row.get(\"entities\") or []\n",
    "    triples = row.get(\"triples\") or []\n",
    "    triple_lines = [f\"({t.get('h')})-[:{t.get('rel')}]->({t.get('t')})\" for t in triples if t]\n",
    "    context = (\n",
    "        \"[Chunks]\\n\" + \"\\n\\n\".join(chunks) +\n",
    "        \"\\n\\n[Entities]\\n\" + \", \".join(ents) +\n",
    "        \"\\n\\n[Triples]\\n\" + \"\\n\".join(triple_lines)\n",
    "    )\n",
    "    return context\n",
    "\n",
    "\n",
    "def qa_graph(query: str, top_k: int = 5, temperature: float | None = None):\n",
    "    \"\"\"Graph expansion QA with metrics and adjustable temperature.\n",
    "\n",
    "    Args:\n",
    "        query: å•é¡Œ\n",
    "        top_k: æª¢ç´¢æ“´å±•çš„ chunk æ•¸é‡\n",
    "        temperature: è‹¥æä¾›ï¼Œå°‡ä»¥è©²å€¼å»ºç«‹è‡¨æ™‚ LLM ä»¥æ§åˆ¶éš¨æ©Ÿæ€§ï¼ˆ0 æ›´ä¿å®ˆï¼Œ1 æ›´ç™¼æ•£ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    context = expand_context_from_chunks(query, top_k=top_k)\n",
    "    if not context:\n",
    "        print(\"[GraphExpand] ç„¡æª¢ç´¢çµæœ\")\n",
    "        return\n",
    "\n",
    "    prompt = f\"è«‹æ ¹æ“šæä¾›çš„å…§å®¹å›ç­”å•é¡Œï¼Œæ¢åˆ—ä¸”å«ä¾æ“šã€‚\\n\\n{context}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "\n",
    "    # é¸æ“‡ LLMï¼ˆæ”¯æ´ per-call èª¿æ•´ temperatureï¼‰\n",
    "    llm_obj = _llm\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            llm_obj = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[GraphExpand] å»ºç«‹å¸¶ temperature çš„ LLM å¤±æ•—ï¼Œæ”¹ç”¨é è¨­ï¼š{e}\")\n",
    "            llm_obj = _llm\n",
    "\n",
    "    ans = llm_obj.invoke(prompt).content\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[GraphExpand] å•é¡Œ:\", query)\n",
    "    print(\"[GraphExpand] å›ç­”:\\n\", ans)\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "\n",
    "    # ä»¥ç›¸åŒ context æ§‹é€ æ¥µç°¡ probe ä¼°ç®— tok/s\n",
    "    probe_prompt = f\"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç°¡çŸ­å›ç­”ï¼š\\n\\n{context}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (ç„¡ eval çµ±è¨ˆ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdc0dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Fulltext ç´¢å¼•ä¸å¯ç”¨æˆ–ç„¡æ³•å»ºç«‹ï¼Œå°‡ä½¿ç”¨å‘é‡-only æª¢ç´¢åšç‚ºæ›¿ä»£ã€‚\n"
     ]
    }
   ],
   "source": [
    "# HybridRetriever + Graph expansion QA\n",
    "from neo4j_graphrag.retrievers.hybrid import HybridRetriever, HybridSearchRanker\n",
    "from neo4j_graphrag.retrievers import VectorRetriever as _VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.embeddings.sentence_transformers import SentenceTransformerEmbeddings\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_VECTOR = \"chunk_embeddings\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "EMBED_MODEL_DIR = \"./models/text2vec-large-chinese\"\n",
    "OLLAMA_MODEL = \"llama3:8b-instruct-q4_K_M\"\n",
    "\n",
    "_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "_embedder = SentenceTransformerEmbeddings(model=EMBED_MODEL_DIR)\n",
    "\n",
    "# Ensure the fulltext index exists and is online (idempotent, with procedure fallback)\n",
    "def _ensure_fulltext_index(driver, index_name: str, label: str, prop: str = \"text\", strict: bool = False) -> bool:\n",
    "    \"\"\"Try to ensure a fulltext index exists. Returns True if exists/created, else False.\n",
    "    If strict=True, raises when cannot be ensured.\n",
    "    \"\"\"\n",
    "    def _exists(sess) -> bool:\n",
    "        try:\n",
    "            rows = sess.run(\"SHOW INDEXES\").data()\n",
    "            if any(str(r.get(\"name\")) == index_name and (r.get(\"type\") == \"FULLTEXT\" or r.get(\"indexType\") == \"FULLTEXT\") for r in rows):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback check via procedure\n",
    "        try:\n",
    "            r = sess.run(\"CALL db.index.fulltext.list() YIELD name WHERE name = $name RETURN name\", name=index_name).single()\n",
    "            if r:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    with driver.session() as session:\n",
    "        if _exists(session):\n",
    "            return True\n",
    "        created = False\n",
    "        # Try schema DDL (Neo4j 5.x)\n",
    "        try:\n",
    "            cypher = f\"\"\"\n",
    "            CREATE FULLTEXT INDEX {index_name} IF NOT EXISTS\n",
    "            FOR (n:{label}) ON EACH [n.{prop}]\n",
    "            \"\"\"\n",
    "            session.run(cypher)\n",
    "            created = True\n",
    "        except Exception:\n",
    "            created = False\n",
    "        # Fallback to procedure (Neo4j 4.x/compat)\n",
    "        if not created:\n",
    "            try:\n",
    "                session.run(\n",
    "                    \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                    name=index_name,\n",
    "                    labels=[label],\n",
    "                    props=[prop],\n",
    "                )\n",
    "                created = True\n",
    "            except Exception:\n",
    "                created = False\n",
    "        # Wait until online (best-effort)\n",
    "        try:\n",
    "            session.run(\"CALL db.awaitIndexes()\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        ok = _exists(session)\n",
    "        if not ok and strict:\n",
    "            raise RuntimeError(f\"ç„¡æ³•å»ºç«‹ Fulltext ç´¢å¼•: {index_name}\")\n",
    "        return ok\n",
    "\n",
    "_USING_VECTOR_ONLY = False\n",
    "_has_fts = _ensure_fulltext_index(_driver, INDEX_FTS, label=\"Chunk\", prop=\"text\", strict=False)\n",
    "\n",
    "# 1) æº–å‚™æª¢ç´¢å™¨ï¼šè‹¥ Fulltext ä¸å¯ç”¨ï¼Œå‰‡é€€åŒ–æˆå‘é‡-only æª¢ç´¢\n",
    "if _has_fts:\n",
    "    _hybrid = HybridRetriever(\n",
    "        driver=_driver,\n",
    "        vector_index_name=INDEX_VECTOR,\n",
    "        fulltext_index_name=INDEX_FTS,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Fulltext ç´¢å¼•ä¸å¯ç”¨æˆ–ç„¡æ³•å»ºç«‹ï¼Œå°‡ä½¿ç”¨å‘é‡-only æª¢ç´¢åšç‚ºæ›¿ä»£ã€‚\")\n",
    "    _USING_VECTOR_ONLY = True\n",
    "    _hybrid = _VectorRetriever(\n",
    "        driver=_driver,\n",
    "        index_name=INDEX_VECTOR,\n",
    "        embedder=_embedder,\n",
    "        return_properties=[\"text\", \"source\", \"id\"],\n",
    "    )\n",
    "\n",
    "_llm = OllamaLLM(model_name=OLLAMA_MODEL)\n",
    "_rag_hybrid = GraphRAG(retriever=_hybrid, llm=_llm)\n",
    "\n",
    "\n",
    "def _measure_tok_s_hybrid(prompt: str, model: str = OLLAMA_MODEL) -> float:\n",
    "    \"\"\"Estimate tok/s from Ollama eval stats; return -1.0 if unavailable.\"\"\"\n",
    "    try:\n",
    "        r = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], options={\"temperature\": 0})\n",
    "        eval_info = r.get(\"eval\", {}) or r.get(\"eval_info\", {})\n",
    "        tps = eval_info.get(\"tokens_per_second\") or r.get(\"tps\")\n",
    "        if isinstance(tps, (int, float)) and tps > 0:\n",
    "            return float(tps)\n",
    "        eval_count = r.get(\"eval_count\") or r.get(\"eval_token_count\") or r.get(\"eval_tokens\")\n",
    "        eval_duration = r.get(\"eval_duration\")  # nanoseconds\n",
    "        if isinstance(eval_count, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "            seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "            if seconds > 0:\n",
    "                return float(eval_count) / seconds\n",
    "    except Exception:\n",
    "        pass\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "def qa_hybrid(query: str, top_k: int = 5, ranker: str = \"naive\", alpha: float | None = None, temperature: float | None = None):\n",
    "    \"\"\"Hybrid QA with optional temperature to control LLM variability.\"\"\"\n",
    "    # choose LLM per call if temperature is provided\n",
    "    rag_to_use = _rag_hybrid\n",
    "    if temperature is not None:\n",
    "        try:\n",
    "            _llm_tmp = OllamaLLM(\n",
    "                model_name=OLLAMA_MODEL,\n",
    "                model_params={\"options\": {\"temperature\": float(temperature)}},\n",
    "            )\n",
    "            rag_to_use = GraphRAG(retriever=_hybrid, llm=_llm_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"[Hybrid] å»ºç«‹å¸¶ temperature çš„ LLM å¤±æ•—ï¼Œæ”¹ç”¨é è¨­ï¼š{e}\")\n",
    "            rag_to_use = _rag_hybrid\n",
    "\n",
    "    if _USING_VECTOR_ONLY:\n",
    "        print(\"[Hybrid] æç¤ºï¼šFulltext ä¸å¯ç”¨ï¼Œç¾ä»¥å‘é‡-only æ¨¡å¼åŸ·è¡Œã€‚\")\n",
    "    cfg = {\"top_k\": top_k}\n",
    "    if not _USING_VECTOR_ONLY and ranker == \"linear\":\n",
    "        cfg[\"ranker\"] = HybridSearchRanker.LINEAR\n",
    "        cfg[\"alpha\"] = alpha if alpha is not None else 0.6\n",
    "    t0 = time.perf_counter()\n",
    "    resp = rag_to_use.search(query_text=query, retriever_config=cfg, return_context=True)\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    print(\"\\n[Hybrid] å•é¡Œ:\", query)\n",
    "    print(\"[Hybrid] å›ç­”:\", resp.answer)\n",
    "\n",
    "    # Print latency and estimate tok/s via a small probe\n",
    "    print(f\"Inference latency: {latency_ms:.1f} ms\")\n",
    "    items = getattr(resp, \"retriever_result\", None)\n",
    "    # Build a small context snippet for throughput probe\n",
    "    ctx_texts = []\n",
    "    if items:\n",
    "        try:\n",
    "            # Hybrid items container\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None)\n",
    "                if not content and isinstance(getattr(it, \"metadata\", None), dict):\n",
    "                    content = getattr(it, \"metadata\", {}).get(\"text\")\n",
    "                if content:\n",
    "                    ctx_texts.append(str(content))\n",
    "        except Exception:\n",
    "            # Vector-only list of dicts\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    ctx_texts.append(str(n.get(\"text\", \"\")))\n",
    "            except Exception:\n",
    "                pass\n",
    "    ctx_blob = \"\\n\\n\".join(ctx_texts)\n",
    "    probe_prompt = f\"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç°¡çŸ­å›ç­”ï¼š\\n\\n{ctx_blob}\\n\\nå•é¡Œï¼š{query}\\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\"\n",
    "    tps = _measure_tok_s_hybrid(probe_prompt, model=OLLAMA_MODEL)\n",
    "    if tps > 0:\n",
    "        print(f\"LLM throughput: {tps:.2f} tok/s (Ollama eval)\")\n",
    "    else:\n",
    "        print(\"LLM throughput: N/A (ç„¡ eval çµ±è¨ˆ)\")\n",
    "\n",
    "    if items:\n",
    "        print(\"[Hybrid] æª¢ç´¢çµæœæ‘˜è¦:\")\n",
    "        # æ”¯æ´ HybridRetriever çš„ items å®¹å™¨æˆ– VectorRetriever çš„ list ç¯€é»\n",
    "        try:\n",
    "            # Hybrid path\n",
    "            for it in items.items[:min(len(items.items), top_k)]:\n",
    "                content = getattr(it, \"content\", None) or it.get(\"text\", \"\")\n",
    "                meta = getattr(it, \"metadata\", None) or it.get(\"source\", \"\")\n",
    "                print(\"-\", str(content)[:120].replace(\"\\n\", \" \"), str(meta)[:80])\n",
    "        except Exception:\n",
    "            # Vector-only path (list of dicts)\n",
    "            try:\n",
    "                for n in items[:min(len(items), top_k)]:\n",
    "                    print(\"-\", str(n.get(\"text\", \"\"))[:120].replace(\"\\n\", \" \"), f\"(source={n.get('source')})\")\n",
    "            except Exception:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d166b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SHOW INDEXES ==\n",
      "{'id': 3, 'name': 'chunk_embeddings', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'VECTOR', 'entityType': 'NODE', 'labelsOrTypes': ['Chunk'], 'properties': ['embedding'], 'indexProvider': 'vector-2.0', 'owningConstraint': None, 'lastRead': neo4j.time.DateTime(2025, 9, 22, 11, 32, 17, 818000000, tzinfo=<UTC>), 'readCount': 48}\n",
      "{'id': 4, 'name': 'chunk_fulltext', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'FULLTEXT', 'entityType': 'NODE', 'labelsOrTypes': ['Chunk'], 'properties': ['text'], 'indexProvider': 'fulltext-1.0', 'owningConstraint': None, 'lastRead': neo4j.time.DateTime(2025, 9, 11, 9, 9, 19, 80000000, tzinfo=<UTC>), 'readCount': 10}\n",
      "{'id': 5, 'name': 'chunk_id_unique', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'RANGE', 'entityType': 'NODE', 'labelsOrTypes': ['Chunk'], 'properties': ['id'], 'indexProvider': 'range-1.0', 'owningConstraint': 'chunk_id_unique', 'lastRead': None, 'readCount': 0}\n",
      "{'id': 7, 'name': 'entity_name_unique', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'RANGE', 'entityType': 'NODE', 'labelsOrTypes': ['Entity'], 'properties': ['name'], 'indexProvider': 'range-1.0', 'owningConstraint': 'entity_name_unique', 'lastRead': None, 'readCount': 0}\n",
      "{'id': 1, 'name': 'index_343aff4e', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'LOOKUP', 'entityType': 'NODE', 'labelsOrTypes': None, 'properties': None, 'indexProvider': 'token-lookup-1.0', 'owningConstraint': None, 'lastRead': neo4j.time.DateTime(2025, 9, 17, 7, 20, 33, 517000000, tzinfo=<UTC>), 'readCount': 216167}\n",
      "{'id': 2, 'name': 'index_f7700477', 'state': 'ONLINE', 'populationPercent': 100.0, 'type': 'LOOKUP', 'entityType': 'RELATIONSHIP', 'labelsOrTypes': None, 'properties': None, 'indexProvider': 'token-lookup-1.0', 'owningConstraint': None, 'lastRead': None, 'readCount': 0}\n",
      "\n",
      "== Fulltext list ==\n",
      "fulltext list error: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `db.index.fulltext.list` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n",
      "DDL å»ºç«‹/ç¢ºä¿ Fulltext ç´¢å¼•å®Œæˆ\n",
      "Fulltext ä»ä¸å¯ç”¨: chunk_text_fts\n"
     ]
    }
   ],
   "source": [
    "# è¨ºæ–·èˆ‡ï¼ˆé‡æ–°ï¼‰å»ºç«‹ Fulltext ç´¢å¼•ï¼ˆå¯é¸ï¼‰\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PW = \"neo4jgoat\"\n",
    "INDEX_FTS = \"chunk_text_fts\"\n",
    "\n",
    "_driver_diag = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "with _driver_diag.session() as s:\n",
    "    print(\"== SHOW INDEXES ==\")\n",
    "    try:\n",
    "        for r in s.run(\"SHOW INDEXES\").data():\n",
    "            print(r)\n",
    "    except Exception as e:\n",
    "        print(\"show indexes error:\", e)\n",
    "\n",
    "    print(\"\\n== Fulltext list ==\")\n",
    "    try:\n",
    "        for r in s.run(\"CALL db.index.fulltext.list()\").data():\n",
    "            print(r)\n",
    "    except Exception as e:\n",
    "        print(\"fulltext list error:\", e)\n",
    "\n",
    "    # å˜—è©¦ä»¥ DDL å»ºç«‹ï¼ˆNeo4j 5.xï¼‰ï¼Œå¤±æ•—å‰‡ä»¥ procedure å»ºç«‹\n",
    "    try:\n",
    "        s.run(f\"\"\"\n",
    "        CREATE FULLTEXT INDEX {INDEX_FTS} IF NOT EXISTS\n",
    "        FOR (c:Chunk) ON EACH [c.text]\n",
    "        \"\"\")\n",
    "        print(\"DDL å»ºç«‹/ç¢ºä¿ Fulltext ç´¢å¼•å®Œæˆ\")\n",
    "    except Exception as e:\n",
    "        print(\"DDL å»ºç«‹å¤±æ•—ï¼Œå˜—è©¦ procedureï¼š\", e)\n",
    "        try:\n",
    "            s.run(\n",
    "                \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                name=INDEX_FTS,\n",
    "                labels=[\"Chunk\"],\n",
    "                props=[\"text\"],\n",
    "            )\n",
    "            print(\"procedure å»ºç«‹ Fulltext ç´¢å¼•å®Œæˆ\")\n",
    "        except Exception as e2:\n",
    "            print(\"procedure äº¦å¤±æ•—ï¼š\", e2)\n",
    "\n",
    "    # ç­‰å¾…ç´¢å¼• onlineï¼ˆæœ€ä½³åŠªåŠ›ï¼‰\n",
    "    try:\n",
    "        s.run(\"CALL db.awaitIndexes()\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    ok = False\n",
    "    try:\n",
    "        ok = s.run(\n",
    "            \"CALL db.index.fulltext.list() YIELD name WHERE name = $n RETURN count(*) AS c\",\n",
    "            n=INDEX_FTS,\n",
    "        ).single().get(\"c\", 0) > 0\n",
    "    except Exception:\n",
    "        ok = False\n",
    "\n",
    "print(\"Fulltext å¯ç”¨:\" if ok else \"Fulltext ä»ä¸å¯ç”¨:\", INDEX_FTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7234054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hybrid] æç¤ºï¼šFulltext ä¸å¯ç”¨ï¼Œç¾ä»¥å‘é‡-only æ¨¡å¼åŸ·è¡Œã€‚\n",
      "\n",
      "[Hybrid] å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\n",
      "[Hybrid] å›ç­”: æ ¹æ“šæä¾›çš„æ–‡æœ¬ï¼Œå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚\n",
      "Inference latency: 1560.6 ms\n",
      "\n",
      "[Hybrid] å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\n",
      "[Hybrid] å›ç­”: æ ¹æ“šæä¾›çš„æ–‡æœ¬ï¼Œå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚\n",
      "Inference latency: 1560.6 ms\n",
      "LLM throughput: 78.18 tok/s (Ollama eval)\n",
      "[Hybrid] æª¢ç´¢çµæœæ‘˜è¦:\n",
      "- {'id': '0_17', 'text': 'ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\\n\\nç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„N {'score': 0.8934812545776367, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': 'ä¹‹ç¸½è²¯å­˜é‡ã€‚\\n\\nç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹” {'score': 0.8598113059997559, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_434', 'text': 'è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Š {'score': 0.8474764823913574, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': 'ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ {'score': 0.8346552848815918, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': 'ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´  {'score': 0.832240104675293, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n",
      "LLM throughput: 78.18 tok/s (Ollama eval)\n",
      "[Hybrid] æª¢ç´¢çµæœæ‘˜è¦:\n",
      "- {'id': '0_17', 'text': 'ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\\n\\nç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„N {'score': 0.8934812545776367, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': 'ä¹‹ç¸½è²¯å­˜é‡ã€‚\\n\\nç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹” {'score': 0.8598113059997559, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_434', 'text': 'è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Š {'score': 0.8474764823913574, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': 'ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ {'score': 0.8346552848815918, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': 'ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´  {'score': 0.832240104675293, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n"
     ]
    }
   ],
   "source": [
    "RUN_QA_hybrid = True\n",
    "if RUN_QA_hybrid:\n",
    "    qa_hybrid(\"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\", top_k=5, ranker=\"linear\", alpha=0.6, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed67d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GraphExpand] å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\n",
      "[GraphExpand] å›ç­”:\n",
      " æ ¹æ“šæä¾›çš„å…§å®¹ï¼Œå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»¥ä¸‹å•é¡Œï¼š\n",
      "\n",
      "* å¤œç›²ç—‡ï¼šå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œå±±ç¾Šåœ¨é»‘æš—ä¸­ç„¡æ³•æ­£å¸¸åœ°ç™¼ç¾ç‰©é«”ã€‚\n",
      "* è§†åŠ›ä¸‹é™ï¼šç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿçš„å¿…è¦ç‰©è³ªï¼Œå› æ­¤å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›ä¸‹é™ã€‚\n",
      "\n",
      "é€™å…©å€‹å•é¡Œéƒ½æ˜¯ç”±æ–¼å±±ç¾Šçš„ç¶­ç”Ÿç´ Aæ°´å¹³ä¸è¶³æ‰€å¼•èµ·çš„ï¼Œä¸¦ä¸”å¦‚æœç¼ºä¹ç¶­ç”Ÿç´ Aå¤ªä¹…ï¼Œå¯èƒ½æœƒå°è‡´ççœ¼ã€‚\n",
      "Inference latency: 4473.8 ms\n",
      "LLM throughput: 76.81 tok/s (Ollama eval)\n"
     ]
    }
   ],
   "source": [
    "RUN_QA_graph_DEMO = True\n",
    "if RUN_QA_graph_DEMO:\n",
    "    qa_graph(\"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿç¹é«”ä¸­æ–‡å›ç­”\", top_k=5,temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b077f8",
   "metadata": {},
   "source": [
    "## ğŸ” åˆ©ç”¨åœ–è­œé—œä¿‚çš„é€²éšæª¢ç´¢æ¨¡å¼\n",
    "\n",
    "ç›¸è¼ƒæ–¼ QA-only çš„ç´”å‘é‡æª¢ç´¢ï¼Œä»¥ä¸‹å…©ç¨®æ¨¡å¼èƒ½å……åˆ†åˆ©ç”¨å·²å»ºç«‹çš„çŸ¥è­˜åœ–è­œé—œä¿‚ï¼š\n",
    "\n",
    "### 1ï¸âƒ£ Graph Expansion æ¨¡å¼ (æ¨è–¦)\n",
    "**æœ€ä½³é¸æ“‡**ï¼šç›´æ¥åˆ©ç”¨å¯¦é«”é—œä¿‚å’Œä¸‰å…ƒçµ„é€²è¡Œæ¨ç†\n",
    "\n",
    "**å„ªå‹¢**ï¼š\n",
    "- ğŸ§  **æ™ºèƒ½æ¨ç†**ï¼šé€šé `MENTIONS` é—œä¿‚æ‰¾åˆ°ç›¸é—œå¯¦é«”\n",
    "- ğŸ”— **é—œä¿‚æ“´å±•**ï¼šåˆ©ç”¨ `RELATION` ä¸‰å…ƒçµ„è±å¯Œä¸Šä¸‹æ–‡\n",
    "- ğŸ“Š **çµæ§‹åŒ–è³‡è¨Š**ï¼šæä¾› [Chunks] + [Entities] + [Triples] å®Œæ•´è„ˆçµ¡\n",
    "- ğŸ¯ **ç²¾æº–å›ç­”**ï¼šåŸºæ–¼çŸ¥è­˜åœ–è­œçš„é‚è¼¯æ¨ç†\n",
    "\n",
    "**ä½¿ç”¨æ–¹å¼**ï¼š\n",
    "```python\n",
    "# åŸºç¤ç”¨æ³•\n",
    "qa_graph(\"ä½ çš„å•é¡Œ\", top_k=5)\n",
    "\n",
    "# æ§åˆ¶å‰µæ„åº¦ï¼ˆtemperatureï¼‰\n",
    "qa_graph(\"ä½ çš„å•é¡Œ\", top_k=5, temperature=0.0)  # ä¿å®ˆå›ç­”\n",
    "qa_graph(\"ä½ çš„å•é¡Œ\", top_k=5, temperature=0.8)  # å‰µæ„å›ç­”\n",
    "```\n",
    "\n",
    "### 2ï¸âƒ£ Hybrid æ¨¡å¼\n",
    "**å¹³è¡¡é¸æ“‡**ï¼šçµåˆå‘é‡æª¢ç´¢å’Œå…¨æ–‡æª¢ç´¢\n",
    "\n",
    "**å„ªå‹¢**ï¼š\n",
    "- ğŸ” **é›™é‡æª¢ç´¢**ï¼šå‘é‡èªæ„ + é—œéµå­—åŒ¹é…\n",
    "- âš¡ **é«˜æ•ˆèƒ½**ï¼šæ¯”ç´”åœ–éæ­·æ›´å¿«\n",
    "- ğŸ›ï¸ **å¯èª¿æ¬Šé‡**ï¼šæ§åˆ¶å‘é‡vså…¨æ–‡çš„æ¯”é‡\n",
    "\n",
    "**ä½¿ç”¨æ–¹å¼**ï¼š\n",
    "```python\n",
    "# åŸºç¤æ··åˆæª¢ç´¢\n",
    "qa_hybrid(\"ä½ çš„å•é¡Œ\", top_k=5)\n",
    "\n",
    "# ç·šæ€§åŠ æ¬Šæª¢ç´¢ï¼ˆæ¨è–¦ï¼‰\n",
    "qa_hybrid(\"ä½ çš„å•é¡Œ\", top_k=5, ranker=\"linear\", alpha=0.6)\n",
    "\n",
    "# æ§åˆ¶å‰µæ„åº¦\n",
    "qa_hybrid(\"ä½ çš„å•é¡Œ\", top_k=5, temperature=0.7)\n",
    "```\n",
    "\n",
    "### ğŸ†š ä¸‰ç¨®æ¨¡å¼æ¯”è¼ƒ\n",
    "\n",
    "| ç‰¹æ€§ | QA-only | Hybrid | Graph Expansion |\n",
    "|------|---------|--------|-----------------|\n",
    "| **æª¢ç´¢æ–¹å¼** | ç´”å‘é‡ | å‘é‡+å…¨æ–‡ | å‘é‡+åœ–è­œé—œä¿‚ |\n",
    "| **ä½¿ç”¨åœ–è­œ** | âŒ | âŒ | âœ… |\n",
    "| **æ¨ç†èƒ½åŠ›** | åŸºç¤ | ä¸­ç­‰ | é«˜ç´š |\n",
    "| **å›ç­”å“è³ª** | è‰¯å¥½ | å¾ˆå¥½ | å„ªç§€ |\n",
    "| **åŸ·è¡Œé€Ÿåº¦** | æœ€å¿« | å¿« | ä¸­ç­‰ |\n",
    "| **é©ç”¨å ´æ™¯** | ç°¡å–®å•ç­” | é—œéµå­—+èªæ„ | è¤‡é›œæ¨ç† |\n",
    "\n",
    "### ğŸ’¡ å»ºè­°ä½¿ç”¨ç­–ç•¥\n",
    "\n",
    "1. **è¤‡é›œæ¨ç†å•é¡Œ** â†’ ä½¿ç”¨ `qa_graph()`\n",
    "2. **éœ€è¦ç²¾ç¢ºåŒ¹é…** â†’ ä½¿ç”¨ `qa_hybrid()` \n",
    "3. **å¿«é€Ÿç°¡å–®æŸ¥è©¢** â†’ ä½¿ç”¨ `qa_only()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a91c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ” é€²éšæª¢ç´¢æ¨¡å¼æ¯”è¼ƒç¤ºç¯„ ===\n",
      "\n",
      "ğŸš€ 1. Graph Expansion æ¨¡å¼ (åˆ©ç”¨åœ–è­œé—œä¿‚)\n",
      "==================================================\n",
      "\n",
      "[GraphExpand] å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿ\n",
      "[GraphExpand] å›ç­”:\n",
      " æ ¹æ“šæä¾›çš„å…§å®¹ï¼Œå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼ˆå¤œç›²ç—‡ï¼‰ã€‚ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿçš„å¿…è¦ç‰©è³ªï¼Œå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "Inference latency: 6731.4 ms\n",
      "\n",
      "[GraphExpand] å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿ\n",
      "[GraphExpand] å›ç­”:\n",
      " æ ¹æ“šæä¾›çš„å…§å®¹ï¼Œå±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼ˆå¤œç›²ç—‡ï¼‰ã€‚ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿçš„å¿…è¦ç‰©è³ªï¼Œå› æ­¤ç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\n",
      "Inference latency: 6731.4 ms\n",
      "LLM throughput: 79.82 tok/s (Ollama eval)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ” 2. Hybrid æ¨¡å¼ (å‘é‡ + å…¨æ–‡æª¢ç´¢)\n",
      "==================================================\n",
      "LLM throughput: 79.82 tok/s (Ollama eval)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ” 2. Hybrid æ¨¡å¼ (å‘é‡ + å…¨æ–‡æª¢ç´¢)\n",
      "==================================================\n",
      "[Hybrid] æç¤ºï¼šFulltext ä¸å¯ç”¨ï¼Œç¾ä»¥å‘é‡-only æ¨¡å¼åŸ·è¡Œã€‚\n",
      "[Hybrid] æç¤ºï¼šFulltext ä¸å¯ç”¨ï¼Œç¾ä»¥å‘é‡-only æ¨¡å¼åŸ·è¡Œã€‚\n",
      "\n",
      "[Hybrid] å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿ\n",
      "[Hybrid] å›ç­”: According to the provided context, mountain goats lacking vitamin A will experience night blindness, which can lead to severe consequences such as complete loss of vision.\n",
      "Inference latency: 3100.1 ms\n",
      "\n",
      "[Hybrid] å•é¡Œ: å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿ\n",
      "[Hybrid] å›ç­”: According to the provided context, mountain goats lacking vitamin A will experience night blindness, which can lead to severe consequences such as complete loss of vision.\n",
      "Inference latency: 3100.1 ms\n",
      "LLM throughput: 84.10 tok/s (Ollama eval)\n",
      "[Hybrid] æª¢ç´¢çµæœæ‘˜è¦:\n",
      "- {'id': '0_17', 'text': 'ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\\n\\nç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„N {'score': 0.8930339813232422, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': 'ä¹‹ç¸½è²¯å­˜é‡ã€‚\\n\\nç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹” {'score': 0.858832836151123, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n",
      "- {'id': '0_434', 'text': 'è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Š {'score': 0.8501629829406738, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': 'ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ {'score': 0.8311724662780762, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': 'ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´  {'score': 0.8266477584838867, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ æ¯”è¼ƒçµæœï¼š\n",
      "- Graph Expansion æä¾›æ›´è±å¯Œçš„å¯¦é«”é—œä¿‚å’Œä¸‰å…ƒçµ„è³‡è¨Š\n",
      "- Hybrid æ¨¡å¼çµåˆèªæ„å’Œé—œéµå­—åŒ¹é…ï¼Œæª¢ç´¢æ›´å…¨é¢\n",
      "- å…©ç¨®æ¨¡å¼éƒ½æ¯” QA-only æä¾›æ›´æ·±å…¥çš„ä¸Šä¸‹æ–‡ç†è§£\n",
      "LLM throughput: 84.10 tok/s (Ollama eval)\n",
      "[Hybrid] æª¢ç´¢çµæœæ‘˜è¦:\n",
      "- {'id': '0_17', 'text': 'ç”Ÿç´ ç¾¤,ä»¥æ»¿è¶³å±±ç¾Šç”Ÿç†éœ€è¦ã€‚\\n\\nç¶­ç”Ÿç´ A:ç¶­ç”Ÿç´ Açš„ç¼ºä¹æœƒå°è‡´å±±ç¾Šé£Ÿæ…¾æ¸›å°‘,ç¨®å…¬ç¾Šç²¾æ¶²å“è³ªé™ä½ã€ç¼ºä¹æ€§æ…¾,æ¯ç¾Šå‰‡ç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡é™ä½,åš´é‡ç¼ºä¹äº¦æœƒé€ æˆå±±ç¾Šå¤œç›²ä¹‹ç—…è®Šã€‚å› æ­¤1981å¹´ç‰ˆçš„N {'score': 0.8930339813232422, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_406', 'text': 'ä¹‹ç¸½è²¯å­˜é‡ã€‚\\n\\nç¶­ç”Ÿç´ Aä¹‹åŠŸèƒ½åŒ…æ‹¬å¦‚ä¸‹ï¼š1.ç¶­æŒæ­£å¸¸ä¹‹è¦–åŠ›ï¼šå› ç¶­ç”Ÿç´ Aæ˜¯çœ¼ç›è¦–ç´«è³ª(rhodopsin)è‰²ç´ å½¢æˆèˆ‡å†ç”Ÿæ‰€å¿…éœ€çš„ç‰©è³ªã€‚æ‰€ä»¥å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å¤œç›²ç—‡ï¼Œåš´é‡ç”šè‡³ççœ¼ã€‚2.ç¹” {'score': 0.858832836151123, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40d\n",
      "- {'id': '0_434', 'text': 'è½‰æ›æˆè¦–è¦ºé†‡ã€‚ç¶­ç”Ÿç´ Aåœ¨ç´°èƒèˆ‡çµ„ç¹”çš„ç”Ÿé•·ç™¼è‚²ä¸Šæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚è‚‰ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´å…¶é£Ÿæ…¾æ¸›å°‘ï¼Œç¨®å…¬ç¾Šæœƒç¼ºä¹æ€§æ…¾ä¸”ç²¾æ¶²å“è³ªä½è½ã€‚ç¨®æ¯ç¾Šå‰‡æœƒç™¼æƒ…ä¸æ˜“ã€ç”Ÿæ®–æ•ˆç‡å·®ï¼Œæ›´åš´é‡çš„ç¼ºä¹æœƒé€ æˆè‚‰ç¾Šçœ¼ç›ç—…è®Š {'score': 0.8501629829406738, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_405', 'text': 'ç—ºï¼Œå°¤å…¶å¾Œè‚¢åƒµç¡¬è€Œä¸èƒ½æèµ·ã€‚ç¡’ç”±ä¹³æ±æ’å‡ºè€Œè²¯å­˜åœ¨çš®æ¯›ã€‚å±±ç¾Šèˆ‡å…¶ä»–å®¶ç•œä¸€æ¨£å¾ˆå®¹æ˜“ç™¼ç”Ÿä¸­æ¯’ï¼Œä¸€èˆ¬èªç‚ºå±±ç¾Šç¡’ä¸­æ¯’ä¹‹åŠ‘é‡èˆ‡ç¶¿ç¾Šè€…ç›¸åŒç‚º3ppmã€‚å±±ç¾Šä¸­æ¯’ä¹‹ç—‡ç‹€ç‚ºå–ªå¤±é£Ÿæ…¾ã€å¤±é‡ã€ç²¾ç¥æ²®å–ªã€å…ˆä¾¿ç§˜è€Œå¾Œéš¨ {'score': 0.8311724662780762, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "- {'id': '0_34', 'text': 'ä¾‹ã€‚3.å¢åŠ æ—¥ç³§ä¸­ç£·é…¸æ°«éˆ£çš„æ·»åŠ æ¯”ä¾‹ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡ï¼šå±±ç¾Šå¤œç›²ç—‡æºè‡ªæ–¼ç¶­ç”Ÿç´ Aç¼ºä¹,æ—¥ç³§ä¸­ç¶­ç”Ÿç´ Aç¼ºä¹æœƒå°è‡´å±±ç¾Šå¤±å»è¦–åŠ›ã€‚\\n\\nå±±ç¾Šå¤œç›²ç—‡çš„é é˜²æ–¹å¼æœ‰é€™äº›ï¼š1.æ³¨æ„ä»¥æ”¾ç‰§é£¼é¤Šå±±ç¾Šçš„ç¶­ç”Ÿç´  {'score': 0.8266477584838867, 'nodeLabels': ['Chunk'], 'id': '4:d0f31978-c03f-40\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ æ¯”è¼ƒçµæœï¼š\n",
      "- Graph Expansion æä¾›æ›´è±å¯Œçš„å¯¦é«”é—œä¿‚å’Œä¸‰å…ƒçµ„è³‡è¨Š\n",
      "- Hybrid æ¨¡å¼çµåˆèªæ„å’Œé—œéµå­—åŒ¹é…ï¼Œæª¢ç´¢æ›´å…¨é¢\n",
      "- å…©ç¨®æ¨¡å¼éƒ½æ¯” QA-only æä¾›æ›´æ·±å…¥çš„ä¸Šä¸‹æ–‡ç†è§£\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª é€²éšæª¢ç´¢æ¨¡å¼ç¤ºç¯„èˆ‡æ¯”è¼ƒ\n",
    "print(\"=== ğŸ” é€²éšæª¢ç´¢æ¨¡å¼æ¯”è¼ƒç¤ºç¯„ ===\\n\")\n",
    "\n",
    "# æ¸¬è©¦å•é¡Œ\n",
    "test_query = \"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´è¦–åŠ›å‡ºç¾ä»€éº¼å•é¡Œï¼Ÿ\"\n",
    "\n",
    "print(\"ğŸš€ 1. Graph Expansion æ¨¡å¼ (åˆ©ç”¨åœ–è­œé—œä¿‚)\")\n",
    "print(\"=\" * 50)\n",
    "qa_graph(test_query, top_k=5, temperature=0.3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"ğŸ” 2. Hybrid æ¨¡å¼ (å‘é‡ + å…¨æ–‡æª¢ç´¢)\")\n",
    "print(\"=\" * 50)\n",
    "qa_hybrid(test_query, top_k=5, ranker=\"linear\", alpha=0.6, temperature=0.3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "print(\"ğŸ’¡ æ¯”è¼ƒçµæœï¼š\")\n",
    "print(\"- Graph Expansion æä¾›æ›´è±å¯Œçš„å¯¦é«”é—œä¿‚å’Œä¸‰å…ƒçµ„è³‡è¨Š\")\n",
    "print(\"- Hybrid æ¨¡å¼çµåˆèªæ„å’Œé—œéµå­—åŒ¹é…ï¼Œæª¢ç´¢æ›´å…¨é¢\")\n",
    "print(\"- å…©ç¨®æ¨¡å¼éƒ½æ¯” QA-only æä¾›æ›´æ·±å…¥çš„ä¸Šä¸‹æ–‡ç†è§£\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcbee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ åƒæ•¸èª¿å„ªç¤ºç¯„\n",
    "print(\"=== ğŸ›ï¸ ä¸åŒåƒæ•¸è¨­å®šçš„æ•ˆæœæ¯”è¼ƒ ===\\n\")\n",
    "\n",
    "test_query = \"å±±ç¾Šç¼ºä¹ç¶­ç”Ÿç´ Aæœƒå°è‡´ä»€éº¼å¥åº·å•é¡Œï¼Ÿ\"\n",
    "\n",
    "print(\"ğŸ§Š ä½æº«åº¦ (temperature=0.0) - ä¿å®ˆå›ç­”\")\n",
    "print(\"-\" * 40)\n",
    "qa_graph(test_query, top_k=3, temperature=0.0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"ğŸ”¥ é«˜æº«åº¦ (temperature=0.9) - å‰µæ„å›ç­”\")\n",
    "print(\"-\" * 40)\n",
    "qa_graph(test_query, top_k=3, temperature=0.9)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"âš–ï¸ Hybrid æ¨¡å¼æ¬Šé‡èª¿æ•´ç¤ºç¯„\")\n",
    "print(\"-\" * 40)\n",
    "print(\"ğŸ“Š Alpha=0.3 (åé‡å…¨æ–‡æª¢ç´¢)\")\n",
    "qa_hybrid(test_query, top_k=3, ranker=\"linear\", alpha=0.3, temperature=0.5)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "print(\"ğŸ“Š Alpha=0.8 (åé‡å‘é‡æª¢ç´¢)\")\n",
    "qa_hybrid(test_query, top_k=3, ranker=\"linear\", alpha=0.8, temperature=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM+KB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
